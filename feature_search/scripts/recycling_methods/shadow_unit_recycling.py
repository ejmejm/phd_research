import logging
import os
import sys
from typing import Iterator, Tuple
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import numpy as np
import torch
import torch.nn as nn
from torch.optim import Optimizer
from tqdm import tqdm

import wandb
import hydra
from omegaconf import DictConfig

from idbd import IDBD
from models import MLP, ACTIVATION_MAP
from tasks import NonlinearGEOFFTask
from experiment_helpers import *
from scripts.feature_maturity_experiment import *


logger = logging.getLogger(__name__)


# class ShadowUnitsMLP(nn.Module):
#     def __init__(
#         self, 
#         input_dim: int,
#         n_shadow_units: int,
#         output_dim: int,
#         n_layers: int,
#         hidden_dim: int,
#         weight_init_method: str,
#         activation: str = 'tanh',
#         n_frozen_layers: int = 0,
#         device: str = 'cuda'
#     ):
#         """
#         Args:
#             input_dim: Number of input features
#             n_shadow_units: Number of shadow units
#             output_dim: Number of output classes
#             n_layers: Number of layers (including output)
#             hidden_dim: Size of hidden layers
#             weight_init_method: How to initialize weights ('zeros', 'kaiming', or 'binary')
#             activation: Activation function ('relu', 'tanh', or 'sigmoid')
#             n_frozen_layers: Number of frozen layers
#             device: Device to put model on
#         """
#         assert n_layers >= 2, "Shadow units MLP must have at least 2 layers!"
#         super().__init__()

#         self.input_dim = input_dim
#         self.output_dim = output_dim
#         self.n_shadow_units = n_shadow_units
#         self.n_frozen_layers = n_frozen_layers
#         self.device = device

#         full_hidden_dim = hidden_dim + n_shadow_units
#         activation_cls = ACTIVATION_MAP[activation]
        

#         # TODO:
#         #  1. Make sure this makes the expected network and shadow masks
#         #  2. Change the forward method to use the mask, first compute the value normally,
#         #     then subtract the detached value generated by just the shadow units
#         #  3. Return the shadow masks in addition to the param inputs
#         #  4. Make sure Autostep with the shadow masks works as intended
#         #     (check about half reduction in effective learning rate when 50% shadow units)

#         # Build layers
#         self.layers = nn.ModuleList()
#         self._shadow_unit_masks = []  # Keep reference to masks in a list
    
#         layer_sizes = [input_dim] + [full_hidden_dim] * (n_layers - 1) + [output_dim]

#         for i in range(1, len(layer_sizes)):
#             # Add layer and activation
#             self.layers.append(nn.Linear(layer_sizes[i - 1], layer_sizes[i], bias=False))
#             if i < len(layer_sizes) - 1:
#                 self.layers.append(activation_cls())

#             # Add shadow unit mask
#             if i > 1:
#                 shadow_unit_mask = torch.zeros(layer_sizes[i - 1], dtype=torch.bool)
#                 shadow_unit_mask[-n_shadow_units:] = True
#                 self.register_buffer(f'shadow_unit_mask_{i - 1}', shadow_unit_mask)
#                 self._shadow_unit_masks.append(getattr(self, f'shadow_unit_mask_{i - 1}'))
        
#         # Freeze layers
#         for i in range(n_frozen_layers):
#             layer = self.layers[int(i*2)]
#             layer.weight.requires_grad = False
#             if layer.bias is not None:
#                 layer.bias.requires_grad = False
        
#         # Initialize weights
#         self._initialize_weights(self.layers[0], weight_init_method)
    
#     def _initialize_weights(self, layer: nn.Module, method: str):
#         """Initialize weights according to specified method."""
#         if method == 'zeros':
#             nn.init.zeros_(layer.weight)
#             if layer.bias is not None:
#                 nn.init.zeros_(layer.bias)
#         elif method == 'kaiming_uniform':
#             nn.init.kaiming_uniform_(layer.weight)
#             if layer.bias is not None:
#                 nn.init.zeros_(layer.bias)
#         elif method == 'binary':
#             layer.weight.data = torch.randint(0, 2, layer.weight.shape, device=layer.weight.device).float() * 2 - 1
#             if layer.bias is not None:
#                 nn.init.zeros_(layer.bias)
#         else:
#             raise ValueError(f'Invalid weight initialization method: {method}')
    
#     def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, Dict[nn.Module, torch.Tensor]]:
#         param_inputs = {}
#         shadow_masks = {}
        
#         z = x
#         for i in range(0, len(self.layers), 2):
#             param_inputs[self.layers[i].weight] = z

#             # Apply linear layer
#             out = self.layers[i](z)

#             # Subtract effects of shadow units if not first layer
#             if i > 0:
#                 shadow_unit_mask = self._shadow_unit_masks[i // 2 - 1]
#                 shadow_masks[self.layers[i - 1]] = shadow_unit_mask
#                 shadow_unit_inputs = z * shadow_unit_mask.unsqueeze(0)
#                 shadow_unit_outputs = self.layers[i](shadow_unit_inputs)
#                 assert shadow_unit_outputs.shape == out.shape # TODO: Sanity check, remove later
#                 out -= shadow_unit_outputs.detach()

#             z = out

#             # Apply activation to all but last layer
#             if i < len(self.layers) - 1:
#                 z = self.layers[i + 1](z) # Activation

#         return z, param_inputs, shadow_masks


class ShadowUnitsMLP(MLP):
    def __init__(
        self, 
        input_dim: int,
        n_shadow_units: int,
        output_dim: int,
        n_layers: int,
        hidden_dim: int,
        weight_init_method: str,
        activation: str = 'tanh',
        n_frozen_layers: int = 0,
        device: str = 'cuda'
    ):
        """
        Args:
            input_dim: Number of input features
            n_shadow_units: Number of shadow units
            output_dim: Number of output classes
            n_layers: Number of layers (including output)
            hidden_dim: Size of hidden layers
            weight_init_method: How to initialize weights ('zeros', 'kaiming', or 'binary')
            activation: Activation function ('relu', 'tanh', or 'sigmoid')
            n_frozen_layers: Number of frozen layers
            device: Device to put model on
        """
        super().__init__(
            input_dim = input_dim,
            output_dim = output_dim,
            n_layers = n_layers,
            hidden_dim = hidden_dim,
            weight_init_method = weight_init_method,
            activation = activation,
            n_frozen_layers = n_frozen_layers,
            device = device
        )
        assert n_layers == 2, "Shadow units MLP must have exactly 2 layers!"

        self.n_shadow_units = n_shadow_units
        activation_cls = ACTIVATION_MAP[activation]
        
        # Build layers
        self.shadow_layers = nn.ModuleList()
        self.shadow_layers.append(nn.Linear(input_dim, hidden_dim, bias=False))
        self.shadow_layers.append(activation_cls())
        for _ in range(n_layers - 2):
            self.shadow_layers.append(nn.Linear(hidden_dim, hidden_dim, bias=False))
            self.shadow_layers.append(activation_cls())
        self.shadow_layers.append(nn.Linear(hidden_dim, output_dim, bias=False))
        
        # Freeze layers
        for i in range(n_frozen_layers):
            layer = self.shadow_layers[int(i*2)]
            layer.weight.requires_grad = False
            if layer.bias is not None:
                layer.bias.requires_grad = False
        
        # Initialize shadow weights
        self._initialize_weights(self.shadow_layers[0], weight_init_method)
    
    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, Dict[nn.Module, torch.Tensor]]:
        param_inputs = {}

        param_inputs[self.layers[0].weight] = x
        features = self.layers[0](x)
        features = self.layers[1](features)
        param_inputs[self.layers[2].weight] = features
        
        param_inputs[self.shadow_layers[0].weight] = x
        shadow_features = self.shadow_layers[0](x)
        shadow_features = self.shadow_layers[1](shadow_features)
        param_inputs[self.shadow_layers[2].weight] = shadow_features

        real_value = self.layers[2](features)
        shadow_value = self.shadow_layers[2](shadow_features)

        value = real_value + shadow_value - shadow_value.detach()

        return value, param_inputs


def run_experiment(
        cfg: DictConfig,
        task: NonlinearGEOFFTask,
        task_iterator: Iterator[Tuple[torch.Tensor, torch.Tensor]],
        model: MLP,
        criterion: nn.Module,
        optimizer: Optimizer,
        cbp_tracker: CBPTracker,
        shadow_cbp_tracker: CBPTracker,
    ):

    # Training loop
    step = 0
    prev_pruned_idx = np.nan
    prune_layer = model.layers[-2]
    pbar = tqdm(total=cfg.train.total_steps, desc='Training')

    # Initialize accumulators
    cumulative_loss = np.float128(0.0)
    loss_accum = 0.0
    pruned_accum = 0
    pruned_newest_feature_accum = 0
    n_steps_since_log = 0
    total_pruned = 0
    target_buffer = []

    while step < cfg.train.total_steps:

        # Generate batch of data
        inputs, targets = next(task_iterator)
        target_buffer.extend(targets.view(-1).tolist())
        features, targets = inputs.to(cfg.device), targets.to(cfg.device)
        
        # Forward pass
        outputs, param_inputs = model(features)
        loss = criterion(outputs, targets)

        # Reset weights and optimizer states for recycled features
        # TODO: Use two different CBP trackers, one for the shadow units and one for the backbone?
        if cbp_tracker is not None:
            pruned_idxs = cbp_tracker.prune_features()
            n_pruned = sum([len(idxs) for idxs in pruned_idxs.values()])
            total_pruned += n_pruned

            if pruned_idxs:
                pruned_idxs = pruned_idxs[model.layers[-2]]
                assert len(pruned_idxs) > 0, "No features were pruned!"
                assert len(pruned_idxs) <= model.shadow_layers[-1].in_features, \
                    "Not enough shadow units to replace all pruned features!"
                
                # Replace each of the pruned features with the highest utility shadow features

                shadow_feature_utilities = list(shadow_cbp_tracker._feature_stats.values())[0]['utility']
                shadow_feature_rankings = torch.argsort(shadow_feature_utilities, descending=True)

                for i, real_idx in enumerate(pruned_idxs):
                    with torch.no_grad():
                        shadow_feature_weights = model.shadow_layers[0].weight[shadow_feature_rankings[i], :]
                        model.layers[0].weight[real_idx, :] = shadow_feature_weights
                

                # Reset all shadow feature weights and CBP tracker stats

                model._initialize_weights(model.shadow_layers[0], 'binary')
                model._initialize_weights(model.shadow_layers[-1], 'zeros')
                for layer in shadow_cbp_tracker._feature_stats:
                    stat_keys = list(shadow_cbp_tracker._feature_stats[layer].keys())
                    for key in stat_keys:
                        del shadow_cbp_tracker._feature_stats[layer][key]

        # Backward pass
        optimizer.zero_grad()
        if isinstance(optimizer, IDBD):
            # Mean over batch dimension
            param_inputs = {k: v.mean(dim=0) for k, v in param_inputs.items()}
            optimizer.step(loss, outputs, param_inputs)
        else:
            loss.backward()
            optimizer.step()
        
        # Accumulate metrics
        loss_accum += loss.item()
        cumulative_loss += loss.item()
        n_steps_since_log += 1
        
        # Log metrics
        if step % cfg.train.log_freq == 0:
            feature_utilities = list(cbp_tracker._feature_stats.values())[0]['utility']
            shadow_feature_utilities = list(shadow_cbp_tracker._feature_stats.values())[0]['utility']
            metrics = {
                'step': step,
                'samples': step * cfg.train.batch_size,
                'loss': loss_accum / n_steps_since_log,
                'cumulative_loss': cumulative_loss,
                'squared_targets': torch.tensor(target_buffer).square().mean().item(),
                'units_pruned': total_pruned,
                'utility_mean': feature_utilities.mean(),
                'utility_std': feature_utilities.std(),
                'shadow_utility_mean': shadow_feature_utilities.mean(),
                'shadow_utility_std': shadow_feature_utilities.std(),
            }

            if pruned_accum > 0:
                metrics['fraction_pruned_were_new'] = pruned_newest_feature_accum / pruned_accum
                pruned_newest_feature_accum = 0
                pruned_accum = 0

            # Add model statistics
            metrics.update(get_model_statistics(model, features, param_inputs))
            wandb.log(metrics)
            
            pbar.set_postfix(loss=metrics['loss'])
            pbar.update(cfg.train.log_freq)
            
            # Reset accumulators
            loss_accum = 0.0
            n_steps_since_log = 0
            target_buffer = []

        step += 1

    pbar.close()
    wandb.finish()


def prepare_experiment(cfg: DictConfig):
    full_hidden_dim = cfg.model.hidden_dim
    n_shadow_units = int(cfg.model.fraction_shadow_units * full_hidden_dim)
    n_real_units = full_hidden_dim - n_shadow_units
    model = ShadowUnitsMLP(
        input_dim = cfg.task.n_features,
        n_shadow_units = n_shadow_units,
        output_dim = cfg.model.output_dim,
        n_layers = cfg.model.n_layers,
        hidden_dim = n_real_units,
        weight_init_method = cfg.model.weight_init_method,
        activation = cfg.model.activation,
        n_frozen_layers = cfg.model.n_frozen_layers,
        device = cfg.device,
    )

    task, task_iterator, model, criterion, optimizer, recycler, cbp_tracker = \
        prepare_components(cfg, model=model)

    assert isinstance(task, NonlinearGEOFFTask)
    
    assert cfg.model.weight_init_method == 'binary', \
        "Binary weight initialization is required for reproducing Mahmood and Sutton (2013)"
    assert cfg.task.weight_init == 'binary', \
        "Binary weight initialization is required for reproducing Mahmood and Sutton (2013)"
    assert cfg.model.activation == 'ltu', \
        "LTU activations are required for reproducing Mahmood and Sutton (2013)"
    assert cfg.task.activation == 'ltu', \
        "LTU activations are required for reproducing Mahmood and Sutton (2013)"

    # Additionaly initialize the CBP tracker for the shadow units
    if cfg.feature_recycling.use_cbp_utility:
        shadow_cbp_tracker = CBPTracker(
            optimizer = optimizer,
            replace_rate = cfg.feature_recycling.recycle_rate,
            decay_rate = cfg.feature_recycling.utility_decay,
            maturity_threshold = cfg.feature_recycling.feature_protection_steps,
            seed = cfg.seed + hash('shadow_cbp_tracker'),
        )
        shadow_cbp_tracker.track_sequential(model.shadow_layers)
    
    if cbp_tracker is not None:
        cbp_tracker.incoming_weight_init = 'binary'
    if shadow_cbp_tracker is not None:
        shadow_cbp_tracker.incoming_weight_init = 'binary'

    # Init target output weights to kaiming uniform and predictor output weights to zero
    seed = cfg.seed + hash('target_output_weights')
    torch.nn.init.kaiming_uniform_(
        task.weights[-1],
        mode = 'fan_in',
        nonlinearity = 'linear',
    )
    torch.nn.init.zeros_(model.layers[-1].weight)
    
    # Change LTU threshold for target and predictors
    ltu_threshold = 0.1 * cfg.task.n_features
    for layer in model.layers:
        if isinstance(layer, LTU):
            layer.threshold = ltu_threshold
    task.activation_fn.threshold = ltu_threshold

    return task, task_iterator, model, criterion, optimizer, recycler, cbp_tracker, shadow_cbp_tracker


@hydra.main(config_path='../../conf', config_name='rupam_task_shadow_weights')
def main(cfg: DictConfig) -> None:
    """Run the feature recycling experiment."""
    task, task_iterator, model, criterion, optimizer, recycler, cbp_tracker, shadow_cbp_tracker = \
        prepare_experiment(cfg)

    run_experiment(cfg, task, task_iterator, model, criterion, optimizer, cbp_tracker, shadow_cbp_tracker)


if __name__ == '__main__':
    main()
