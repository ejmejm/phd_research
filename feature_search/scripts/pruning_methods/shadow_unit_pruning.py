import logging
import os
import sys
from typing import Iterator, Tuple
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import numpy as np
import torch
import torch.nn as nn
from torch.optim import Optimizer
from tqdm import tqdm

import wandb
import hydra
from omegaconf import DictConfig

from idbd import IDBD
from models import MLP, ACTIVATION_MAP
from tasks import NonlinearGEOFFTask
from run_experiment import *
from scripts.feature_maturity_experiment import *


logger = logging.getLogger(__name__)


class ShadowUnitsMLP(MLP):
    def __init__(
        self, 
        input_dim: int,
        n_shadow_units: int,
        output_dim: int,
        n_layers: int,
        hidden_dim: int,
        weight_init_method: str,
        activation: str = 'tanh',
        n_frozen_layers: int = 0,
        device: str = 'cuda'
    ):
        """
        Args:
            input_dim: Number of input features
            n_shadow_units: Number of shadow units
            output_dim: Number of output classes
            n_layers: Number of layers (including output)
            hidden_dim: Size of hidden layers
            weight_init_method: How to initialize weights ('zeros', 'kaiming', or 'binary')
            activation: Activation function ('relu', 'tanh', or 'sigmoid')
            n_frozen_layers: Number of frozen layers
            device: Device to put model on
        """
        assert n_layers >= 2, "Shadow units MLP must have at least 2 layers!"

        super().__init__()

        self.input_dim = input_dim
        self.output_dim = output_dim
        self.n_shadow_units = n_shadow_units
        self.n_frozen_layers = n_frozen_layers
        self.device = device

        full_hidden_dim = hidden_dim + n_shadow_units
        activation_cls = ACTIVATION_MAP[activation]
        

        # TODO:
        #  1. Make sure this makes the expected network and shadow masks
        #  2. Change the forward method to use the mask, first compute the value normally,
        #     then subtract the detached value generated by just the shadow units
        #  3. Return the shadow masks in addition to the param inputs
        #  4. Make sure Autostep with the shadow masks works as intended
        #     (check about half reduction in effective learning rate when 50% shadow units)

        # Build layers
        self.layers = nn.ModuleList()
        self._shadow_unit_masks = []  # Keep reference to masks in a list
    
        layer_sizes = [input_dim] + [full_hidden_dim] * (n_layers - 1) + [output_dim]

        for i in range(1, len(layer_sizes)):
            # Add layer and activation
            self.layers.append(nn.Linear(layer_sizes[i - 1], layer_sizes[i], bias=False))
            if i < len(layer_sizes) - 1:
                self.layers.append(activation_cls())

            # Add shadow unit mask
            if i > 1:
                shadow_unit_mask = torch.zeros(layer_sizes[i - 1], dtype=torch.bool)
                shadow_unit_mask[-n_shadow_units:] = True
                self.register_buffer(f'shadow_unit_mask_{i - 1}', shadow_unit_mask)
                self._shadow_unit_masks.append(getattr(self, f'shadow_unit_mask_{i - 1}'))

        # # Add input layer
        # self.layers.append(nn.Linear(input_dim, full_hidden_dim, bias=False))
        # self.layers.append(activation_cls())

        # for i in range(n_layers - 2):
        #     # Add layers
        #     self.layers.append(nn.Linear(full_hidden_dim, full_hidden_dim, bias=False))
        #     self.layers.append(activation_cls())

        #     # Add layer shadow unit mask as buffer
        #     shadow_unit_mask = torch.zeros(full_hidden_dim, dtype=torch.bool)
        #     shadow_unit_mask[-n_shadow_units:] = True
        #     self.register_buffer(f'shadow_unit_mask_{i}', shadow_unit_mask)
        #     self._shadow_unit_masks.append(getattr(self, f'shadow_unit_mask_{i}'))

        # # Add output layer
        # self.layers.append(nn.Linear(full_hidden_dim, output_dim, bias=False))

        # # Add output layer shadow unit mask
        # shadow_unit_mask = torch.zeros(full_hidden_dim, dtype=torch.bool)
        # shadow_unit_mask[-n_shadow_units:] = True
        # self.register_buffer(f'shadow_unit_mask_{n_layers - 2}', shadow_unit_mask)
        # self._shadow_unit_masks.append(getattr(self, f'shadow_unit_mask_{n_layers - 2}'))
        
        # Freeze layers
        for i in range(n_frozen_layers):
            layer = self.layers[int(i*2)]
            layer.weight.requires_grad = False
            if layer.bias is not None:
                layer.bias.requires_grad = False
        
        # Initialize weights
        self._initialize_weights(self.layers[0], weight_init_method)
    
    # def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, Dict[nn.Module, torch.Tensor]]:
    #     param_inputs = {}
    #     for i in range(0, len(self.layers) - 2, 2):
    #         param_inputs[self.layers[i].weight] = x
    #         x = self.layers[i](x) # Linear layer
    #         x = self.layers[i + 1](x) # Activation

    #     param_inputs[self.layers[-1].weight] = x
    #     return self.layers[-1](x), param_inputs
    
    # def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, Dict[nn.Module, torch.Tensor]]:
    #     param_inputs = {}

    #     param_inputs[self.layers[0].weight] = x
    #     features = self.layers[0](x)
    #     features = self.layers[1](features)
    #     param_inputs[self.layers[2].weight] = features
        
    #     param_inputs[self.shadow_layers[0].weight] = x
    #     shadow_features = self.shadow_layers[0](x)
    #     shadow_features = self.shadow_layers[1](shadow_features)
    #     param_inputs[self.shadow_layers[2].weight] = shadow_features

    #     real_value = self.layers[2](features)
    #     shadow_value = self.shadow_layers[2](shadow_features)

    #     value = real_value + shadow_value - shadow_value.detach()

    #     return value, param_inputs


def run_experiment(
        cfg: DictConfig,
        task: NonlinearGEOFFTask,
        task_iterator: Iterator[Tuple[torch.Tensor, torch.Tensor]],
        model: MLP,
        criterion: nn.Module,
        optimizer: Optimizer,
        cbp_tracker: CBPTracker,
    ):

    # Training loop
    step = 0
    prev_pruned_idx = np.nan
    prune_layer = model.layers[-2]
    pbar = tqdm(total=cfg.train.total_steps, desc='Training')

    # Initialize accumulators
    cumulative_loss = np.float128(0.0)
    loss_accum = 0.0
    pruned_accum = 0
    pruned_newest_feature_accum = 0
    n_steps_since_log = 0
    total_pruned = 0
    target_buffer = []

    while step < cfg.train.total_steps:

        # Generate batch of data
        inputs, targets = next(task_iterator)
        target_buffer.extend(targets.view(-1).tolist())
        features, targets = inputs.to(cfg.device), targets.to(cfg.device)

        # Reset weights and optimizer states for recycled features
        if cbp_tracker is not None:
            pruned_idxs = cbp_tracker.prune_features()
            n_pruned = sum([len(idxs) for idxs in pruned_idxs.values()])
            total_pruned += n_pruned

            if n_pruned > 1:
                raise Exception("More than one feature was pruned! This would make the logging invalid.")

            if prune_layer in pruned_idxs and len(pruned_idxs[prune_layer]) > 0:
                new_pruned_idx = pruned_idxs[prune_layer][0]
                pruned_accum += 1
                pruned_newest_feature_accum += int(new_pruned_idx == prev_pruned_idx)
                prev_pruned_idx = new_pruned_idx
        
        # Forward pass
        outputs, param_inputs = model(features)
        loss = criterion(outputs, targets)

        # Backward pass
        optimizer.zero_grad()
        if isinstance(optimizer, IDBD):
            # Mean over batch dimension
            param_inputs = {k: v.mean(dim=0) for k, v in param_inputs.items()}
            optimizer.step(loss, outputs, param_inputs)
        else:
            loss.backward()
            optimizer.step()
        
        # Accumulate metrics
        loss_accum += loss.item()
        cumulative_loss += loss.item()
        n_steps_since_log += 1
        
        # Log metrics
        if step % cfg.train.log_freq == 0:
            metrics = {
                'step': step,
                'samples': step * cfg.train.batch_size,
                'loss': loss_accum / n_steps_since_log,
                'cumulative_loss': cumulative_loss,
                'squared_targets': torch.tensor(target_buffer).square().mean().item(),
                'units_pruned': total_pruned,
            }

            if pruned_accum > 0:
                metrics['fraction_pruned_were_new'] = pruned_newest_feature_accum / pruned_accum
                pruned_newest_feature_accum = 0
                pruned_accum = 0

            # Add model statistics
            metrics.update(get_model_statistics(model, features, param_inputs))
            wandb.log(metrics)
            
            pbar.set_postfix(loss=metrics['loss'])
            pbar.update(cfg.train.log_freq)
            
            # Reset accumulators
            loss_accum = 0.0
            n_steps_since_log = 0
            target_buffer = []

        step += 1

    pbar.close()
    wandb.finish()


@hydra.main(config_path='../../conf', config_name='rupam_task')
def main(cfg: DictConfig) -> None:
    """Run the feature recycling experiment."""
    task, task_iterator, model, criterion, optimizer, recycler, cbp_tracker = \
        prepare_experiment(cfg)

    run_experiment(cfg, task, task_iterator, model, criterion, optimizer, cbp_tracker)


if __name__ == '__main__':
    main()
