defaults:
- _self_

seed: null
wandb: false
comet_ml: false
comet_ml_workspace: phd-research
project: feature-search # Will appear in logging under this project
device: cpu # {cpu, cuda}
jax_jit_cache_dir: /tmp/jax_cache # Directory to cache JIT compiled functions (JAX ONLY)

# Task parameters
task:
  name: static_linear_geoff # {dummy, static_linear_geoff, linear_geoff, nonlinear_geoff}
  type: regression # {classification, regression}
  n_features: 20 # Total number of input features the model takes as input
  n_real_features: 10 # Number of input features out of the above that have an effect on the output

# Input recycling parameters
# The input recycler is used if you want to insert distractor features in the input
# and the change the inputs as training progresses
# It is not used in any of the newer experiments
input_recycling:
  use_cbp_utility: True # Input features will be replaced based on utility when true, otherwise replaced at random
  distractor_chance: 0.0 # Chance newly introduced features will be distractors
  recycle_rate: 0.0 # Fraction of input features that are replaced each step
  utility_decay: 0.99 # Parameter used in utility computation
  feature_protection_steps: 100 # Number of steps a new feature is immune to pruning
  n_start_real_features: -1 # Force the model to start with this many real features

# Feature recycling parameters
# This is used to replace hidden features during training
feature_recycling:
  use_cbp_utility: True # Features will be replaced based on utility when true, otherwise replaced at random
  use_signed_utility: False # Use a signed version of utility, different from the original CBP utility
  recycle_rate: 0.0 # Fraction of features that are replaced each step
  utility_decay: 0.99 # Parameter used in utility computation
  feature_protection_steps: 100 # Number of steps a new feature is immune to pruning
  prune_frequency: 1 # Features are only pruned every nth step, though stats are updated every step (JAX ONLY)
  utility_reset_mode: median # {median, zero}
  initial_step_size_method: constant # {constant, mean, median} method used to set the step size of new features
                                     # constant will be the value set by the optimizer, and mean/median will take
                                     # the mean/median of the step sizes of other features in the same layer

# Train parameters
train:
  batch_size: 1
  total_steps: 10000 # Number of step to train
  log_freq: 200 # Frequency of logging stats
  standardize_cumulants: false # Normalizes targets to mean of 0 and std dev of 1 when true

# Optimizer parameters
optimizer:
  name: adam # {sgd, sgd_momentum, rmsprop, adam, idbd}
  weight_decay: 0
  learning_rate: 0.001

  # Below are only used when optimizer is idbd
  meta_learning_rate: 0.005
  version: squared_grads # {squared_inputs, squared_grads, hvp, hessian_diagonal}, recommend not changing this
  autostep: true
  step_size_decay: 0.0 # Only applicable when autostep is true, uses Swift-TD inspired step-size decay

# Model parameters
model:
  weight_init_method: zeros # {zeros, kaiming_uniform}, only affects input layer weights
  n_layers: 1 # 1 layer = linear model
  hidden_dim: 128
  output_dim: 1
  activation: relu # Only used when there is more than 1 layer
  n_frozen_layers: 0 # Number of layers to freeze starting from the input layer