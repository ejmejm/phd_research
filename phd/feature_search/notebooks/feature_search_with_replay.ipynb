{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847d0bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ejmejm/local_projects/phd_research/phd/feature_search/scripts/full_feature_search.py:513: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path='../conf', config_name='full_feature_search')\n",
      "/tmp/ipykernel_19414/3524028346.py:6: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  hydra.initialize(config_path='../conf')\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "from phd.feature_search.scripts.full_feature_search import *\n",
    "\n",
    "if not hydra.core.global_hydra.GlobalHydra().is_initialized():\n",
    "    hydra.initialize(config_path='../conf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c555b29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comet_ml: false\n",
      "comet_ml_workspace: phd-research\n",
      "device: cpu\n",
      "feature_recycling:\n",
      "  feature_protection_steps: 0\n",
      "  initial_step_size_method: constant\n",
      "  recycle_rate: 0.005\n",
      "  use_cbp_utility: true\n",
      "  use_signed_utility: false\n",
      "  utility_decay: 0.99\n",
      "input_recycling:\n",
      "  distractor_chance: 0.0\n",
      "  feature_protection_steps: 100\n",
      "  n_start_real_features: -1\n",
      "  recycle_rate: 0.0\n",
      "  use_cbp_utility: false\n",
      "  utility_decay: 0.99\n",
      "model:\n",
      "  activation: ltu\n",
      "  hidden_dim: 5120\n",
      "  n_frozen_layers: 1\n",
      "  n_layers: 2\n",
      "  output_dim: 1\n",
      "  use_bias: true\n",
      "  weight_init_method: binary\n",
      "optimizer:\n",
      "  autostep: true\n",
      "  learning_rate: 4.956427797377644e-06\n",
      "  meta_learning_rate: 0.005\n",
      "  name: idbd\n",
      "  step_size_decay: 0.0\n",
      "  version: squared_grads\n",
      "  weight_decay: 0\n",
      "project: feature-search\n",
      "representation_optimizer:\n",
      "  learning_rate: 0.001\n",
      "  name: null\n",
      "  weight_decay: 0\n",
      "seed: 20250902\n",
      "task:\n",
      "  activation: ltu\n",
      "  distractor_chance: 0.0\n",
      "  distractor_mean_range:\n",
      "  - -0.5\n",
      "  - 0.5\n",
      "  distractor_std_range:\n",
      "  - 0.1\n",
      "  - 1.0\n",
      "  flip_rate: 1.52587890625e-05\n",
      "  hidden_dim: 20\n",
      "  n_features: 20\n",
      "  n_layers: 2\n",
      "  n_real_features: 20\n",
      "  n_stationary_layers: 0\n",
      "  name: nonlinear_geoff\n",
      "  noise_std: 0.0\n",
      "  sparsity: 0.0\n",
      "  type: regression\n",
      "  weight_init: binary\n",
      "  weight_scale: 1.0\n",
      "train:\n",
      "  batch_size: 1\n",
      "  log_freq: 200\n",
      "  log_model_stats: false\n",
      "  log_optimizer_stats: false\n",
      "  log_pruning_stats: true\n",
      "  log_utility_stats: false\n",
      "  standardize_cumulants: true\n",
      "  total_steps: 1000000\n",
      "wandb: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load hydra config\n",
    "cfg = hydra.compose(\n",
    "    config_name = \"comet_sweeps/nonlinear_geoff_ablation_v5/base_config\",\n",
    "    overrides = [\n",
    "        \"wandb=true\",\n",
    "        \"comet_ml=false\",\n",
    "        \"model.hidden_dim=5120\",\n",
    "        \"optimizer.learning_rate=$\\{eval:0.003 / ${model.hidden_dim} ** 0.75\\}\",\n",
    "        \"train.log_freq=200\",\n",
    "        \"task.flip_rate=$\\{eval:2**-16\\}\",\n",
    "        \"task.n_stationary_layers=0\",\n",
    "        \"seed=20250902\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "yaml_cfg = omegaconf.OmegaConf.to_container(cfg, resolve=True)\n",
    "print(yaml.dump(yaml_cfg, indent=2, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3370fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mejmejm\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ejmejm/local_projects/phd_research/phd/feature_search/notebooks/wandb/run-20250909_115435-p64b0gnm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ejmejm/feature-search/runs/p64b0gnm' target=\"_blank\">balmy-dream-138</a></strong> to <a href='https://wandb.ai/ejmejm/feature-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ejmejm/feature-search' target=\"_blank\">https://wandb.ai/ejmejm/feature-search</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ejmejm/feature-search/runs/p64b0gnm' target=\"_blank\">https://wandb.ai/ejmejm/feature-search/runs/p64b0gnm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Run the feature recycling experiment.\"\"\"\n",
    "assert cfg.model.n_layers == 2, \"Only 2-layer models are supported!\"\n",
    "\n",
    "cfg = init_experiment(cfg.project, cfg)\n",
    "\n",
    "task, task_iterator, model, criterion, optimizer, repr_optimizer, recycler, cbp_tracker = \\\n",
    "    prepare_ltu_geoff_experiment(cfg)\n",
    "model.forward = model_distractor_forward_pass.__get__(model)\n",
    "\n",
    "distractor_tracker = DistractorTracker(\n",
    "    model,\n",
    "    cfg.task.distractor_chance,\n",
    "    tuple(cfg.task.distractor_mean_range),\n",
    "    tuple(cfg.task.distractor_std_range),\n",
    "    seed = seed_from_string(cfg.seed, 'distractor_tracker'),\n",
    ")\n",
    "\n",
    "# run_experiment(\n",
    "#     cfg, task, task_iterator, model, criterion, optimizer,\n",
    "#     repr_optimizer, cbp_tracker, distractor_tracker,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32585138",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RingBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.buffer = [None] * size\n",
    "        self.index = 0\n",
    "\n",
    "    def append(self, item):\n",
    "        self.buffer[self.index] = item\n",
    "        self.index = (self.index + 1) % self.size\n",
    "\n",
    "    def get_buffer(self):\n",
    "        return self.buffer\n",
    "        \n",
    "    def sample(self, n):\n",
    "        # Get only non-None values\n",
    "        valid_items = [x for x in self.buffer if x is not None]\n",
    "        \n",
    "        # Sample min of n and number of valid items\n",
    "        n = min(n, len(valid_items))\n",
    "        if n == 0:\n",
    "            return []\n",
    "            \n",
    "        return random.sample(valid_items, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "105d76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    model, criterion, optimizer, repr_optimizer, use_bias,\n",
    "    cumulant_stats, distractor_tracker, inputs, targets,\n",
    "    effective_lr_accum,\n",
    "):\n",
    "    # Forward pass\n",
    "    outputs, param_inputs = model(\n",
    "        inputs, distractor_tracker.replace_features, use_bias)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if cfg.train.standardize_cumulants:\n",
    "            baseline_pred = torch.zeros_like(targets)\n",
    "        else:\n",
    "            baseline_pred = cumulant_stats.running_mean.cpu().view(1, 1)\n",
    "        mean_pred_loss = criterion(baseline_pred, targets)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    if repr_optimizer is not None:\n",
    "        repr_optimizer.zero_grad()\n",
    "    \n",
    "    if isinstance(optimizer, IDBD):\n",
    "        # Mean over batch dimension\n",
    "        param_inputs = {k: v.mean(dim=0) for k, v in param_inputs.items()}\n",
    "        retain_graph = optimizer.version == 'squared_grads'\n",
    "        loss.backward(retain_graph=retain_graph)\n",
    "        stats = optimizer.step(outputs, param_inputs)\n",
    "        effective_lr_accum += list(stats.values())[0]['effective_step_size'].mean().item()\n",
    "    else:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if repr_optimizer is not None:\n",
    "        repr_optimizer.step()\n",
    "        \n",
    "    return loss.item(), mean_pred_loss.item(), effective_lr_accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d5380d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_bias = cfg.model.get('use_bias', True)\n",
    "    \n",
    "# Distractor setup\n",
    "n_hidden_units = model.layers[-1].in_features\n",
    "first_feature_idx = 1 if use_bias else 0 # First feature is bias if enabled\n",
    "distractor_tracker.process_new_features(list(range(first_feature_idx, n_hidden_units)))\n",
    "\n",
    "# Training loop\n",
    "step = 0\n",
    "prev_pruned_idxs = set()\n",
    "prune_layer = model.layers[-2]\n",
    "# pbar = tqdm(total=cfg.train.total_steps, desc='Training')\n",
    "\n",
    "# Flags\n",
    "log_utility_stats = cfg.train.get('log_utility_stats', False)\n",
    "log_pruning_stats = cfg.train.get('log_pruning_stats', False)\n",
    "log_model_stats = cfg.train.get('log_model_stats', False)\n",
    "log_optimizer_stats = cfg.train.get('log_optimizer_stats', False)\n",
    "\n",
    "# Initialize accumulators\n",
    "cumulant_stats = StandardizationStats(gamma=0.99)\n",
    "cumulative_loss = np.float128(0.0)\n",
    "loss_accum = 0.0\n",
    "mean_pred_loss_accum = 0.0\n",
    "effective_lr_accum = 0.0\n",
    "pruned_accum = 0\n",
    "pruned_newest_feature_accum = 0\n",
    "n_steps_since_log = 0\n",
    "total_pruned = 0\n",
    "prune_thresholds = []\n",
    "target_buffer = []\n",
    "\n",
    "# Replay stuff\n",
    "replay_buffer = RingBuffer(size=164)\n",
    "n_replay_steps = 0\n",
    "use_replay_buffer = True\n",
    "\n",
    "# future_flip_rate = task.flip_rate\n",
    "# task.flip_rate = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "780e4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_iterator = task.get_iterator(cfg.train.batch_size)\n",
    "for _ in range(5000):\n",
    "    inputs, targets = next(task_iterator)\n",
    "    standardized_targets, cumulant_stats = standardize_targets(targets, cumulant_stats)\n",
    "\n",
    "with torch.no_grad():\n",
    "    task_iw = task.weights[0]\n",
    "    model.layers[0].weight[:] = 0.0\n",
    "    model.layers[0].weight[1:task_iw.shape[1]+1, :task_iw.shape[0]] = task_iw.T\n",
    "\n",
    "    task_ow = task.weights[1]\n",
    "    std = torch.sqrt(cumulant_stats.running_var)\n",
    "    model.layers[2].weight[:, 0] = -cumulant_stats.running_mean / std\n",
    "    model.layers[2].weight[:task_ow.shape[1], 1:task_ow.shape[0]+1] = task_ow.T / std\n",
    "\n",
    "# x, y = next(task_iterator)\n",
    "# y, cumulant_stats = standardize_targets(y, cumulant_stats)\n",
    "# print((y - model.forward(x, use_bias=True)[0]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d4029b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 | loss: 0.0000\n",
      "step: 200 | loss: 0.0115\n",
      "step: 400 | loss: 0.0261\n",
      "step: 600 | loss: 0.0447\n",
      "step: 800 | loss: 0.0814\n",
      "step: 1000 | loss: 0.0648\n",
      "step: 1200 | loss: 0.1105\n",
      "step: 1400 | loss: 0.1820\n",
      "step: 1600 | loss: 0.3576\n",
      "step: 1800 | loss: 0.5253\n",
      "step: 2000 | loss: 0.5471\n",
      "step: 2200 | loss: 0.4837\n",
      "step: 2400 | loss: 0.4698\n",
      "step: 2600 | loss: 0.6435\n",
      "step: 2800 | loss: 0.6089\n",
      "step: 3000 | loss: 0.5738\n",
      "step: 3200 | loss: 0.6668\n",
      "step: 3400 | loss: 0.6089\n",
      "step: 3600 | loss: 0.5836\n",
      "step: 3800 | loss: 0.5694\n",
      "step: 4000 | loss: 0.7593\n",
      "step: 4200 | loss: 0.7402\n",
      "step: 4400 | loss: 0.5652\n",
      "step: 4600 | loss: 0.5516\n",
      "step: 4800 | loss: 0.5039\n",
      "step: 5000 | loss: 0.4650\n",
      "step: 5200 | loss: 0.5349\n",
      "step: 5400 | loss: 0.5101\n",
      "step: 5600 | loss: 0.6836\n",
      "step: 5800 | loss: 0.5154\n",
      "step: 6000 | loss: 0.5606\n",
      "step: 6200 | loss: 0.5216\n",
      "step: 6400 | loss: 0.5194\n",
      "step: 6600 | loss: 0.5992\n",
      "step: 6800 | loss: 0.6828\n",
      "step: 7000 | loss: 0.6937\n",
      "step: 7200 | loss: 0.7137\n",
      "step: 7400 | loss: 0.7106\n",
      "step: 7600 | loss: 0.7338\n",
      "step: 7800 | loss: 0.5581\n",
      "step: 8000 | loss: 0.6144\n",
      "step: 8200 | loss: 0.7709\n",
      "step: 8400 | loss: 0.8839\n",
      "step: 8600 | loss: 0.6117\n",
      "step: 8800 | loss: 0.8698\n",
      "step: 9000 | loss: 0.8326\n",
      "step: 9200 | loss: 0.7006\n",
      "step: 9400 | loss: 0.6345\n",
      "step: 9600 | loss: 0.7298\n",
      "step: 9800 | loss: 0.6880\n",
      "step: 10000 | loss: 0.6640\n",
      "step: 10200 | loss: 0.6610\n",
      "step: 10400 | loss: 0.6238\n",
      "step: 10600 | loss: 0.7432\n",
      "step: 10800 | loss: 0.7060\n",
      "step: 11000 | loss: 0.6862\n",
      "step: 11200 | loss: 0.6649\n",
      "step: 11400 | loss: 0.7205\n",
      "step: 11600 | loss: 0.8018\n",
      "step: 11800 | loss: 0.7450\n",
      "step: 12000 | loss: 0.8206\n",
      "step: 12200 | loss: 0.6878\n",
      "step: 12400 | loss: 0.6743\n",
      "step: 12600 | loss: 0.6272\n",
      "step: 12800 | loss: 0.6642\n",
      "step: 13000 | loss: 0.5844\n",
      "step: 13200 | loss: 0.5608\n",
      "step: 13400 | loss: 0.5751\n",
      "step: 13600 | loss: 0.5463\n",
      "step: 13800 | loss: 0.4644\n",
      "step: 14000 | loss: 0.6493\n",
      "step: 14200 | loss: 0.5954\n",
      "step: 14400 | loss: 0.5242\n",
      "step: 14600 | loss: 0.5847\n",
      "step: 14800 | loss: 0.6364\n",
      "step: 15000 | loss: 0.6343\n",
      "step: 15200 | loss: 0.6890\n",
      "step: 15400 | loss: 0.5802\n",
      "step: 15600 | loss: 0.6121\n",
      "step: 15800 | loss: 0.5488\n",
      "step: 16000 | loss: 0.5953\n",
      "step: 16200 | loss: 0.6439\n",
      "step: 16400 | loss: 0.5951\n",
      "step: 16600 | loss: 0.5927\n",
      "step: 16800 | loss: 0.5250\n",
      "step: 17000 | loss: 0.5462\n",
      "step: 17200 | loss: 0.6238\n",
      "step: 17400 | loss: 0.4778\n",
      "step: 17600 | loss: 0.6331\n",
      "step: 17800 | loss: 0.5425\n",
      "step: 18000 | loss: 0.5303\n",
      "step: 18200 | loss: 0.6740\n",
      "step: 18400 | loss: 0.6867\n",
      "step: 18600 | loss: 0.5726\n",
      "step: 18800 | loss: 0.4787\n",
      "step: 19000 | loss: 0.6075\n",
      "step: 19200 | loss: 0.5448\n",
      "step: 19400 | loss: 0.5309\n",
      "step: 19600 | loss: 0.5588\n",
      "step: 19800 | loss: 0.5100\n",
      "step: 20000 | loss: 0.5582\n",
      "step: 20200 | loss: 0.5689\n",
      "step: 20400 | loss: 0.6054\n",
      "step: 20600 | loss: 0.4932\n",
      "step: 20800 | loss: 0.5816\n",
      "step: 21000 | loss: 0.5955\n",
      "step: 21200 | loss: 0.4704\n",
      "step: 21400 | loss: 0.6154\n",
      "step: 21600 | loss: 0.4731\n",
      "step: 21800 | loss: 0.5491\n",
      "step: 22000 | loss: 0.4174\n",
      "step: 22200 | loss: 0.4702\n",
      "step: 22400 | loss: 0.4995\n",
      "step: 22600 | loss: 0.5158\n",
      "step: 22800 | loss: 0.5162\n",
      "step: 23000 | loss: 0.4375\n",
      "step: 23200 | loss: 0.4703\n",
      "step: 23400 | loss: 0.4831\n",
      "step: 23600 | loss: 0.4641\n",
      "step: 23800 | loss: 0.4757\n",
      "step: 24000 | loss: 0.4817\n",
      "step: 24200 | loss: 0.5075\n",
      "step: 24400 | loss: 0.4817\n",
      "step: 24600 | loss: 0.5643\n",
      "step: 24800 | loss: 0.5303\n",
      "step: 25000 | loss: 0.5354\n",
      "step: 25200 | loss: 0.5257\n",
      "step: 25400 | loss: 0.3996\n",
      "step: 25600 | loss: 0.4274\n",
      "step: 25800 | loss: 0.4235\n",
      "step: 26000 | loss: 0.4039\n",
      "step: 26200 | loss: 0.4345\n",
      "step: 26400 | loss: 0.3437\n",
      "step: 26600 | loss: 0.3575\n",
      "step: 26800 | loss: 0.3967\n",
      "step: 27000 | loss: 0.4818\n",
      "step: 27200 | loss: 0.3874\n",
      "step: 27400 | loss: 0.4171\n",
      "step: 27600 | loss: 0.4641\n",
      "step: 27800 | loss: 0.4310\n",
      "step: 28000 | loss: 0.4341\n",
      "step: 28200 | loss: 0.3664\n",
      "step: 28400 | loss: 0.3606\n",
      "step: 28600 | loss: 0.3734\n",
      "step: 28800 | loss: 0.3701\n",
      "step: 29000 | loss: 0.3958\n",
      "step: 29200 | loss: 0.4200\n",
      "step: 29400 | loss: 0.4526\n",
      "step: 29600 | loss: 0.3195\n",
      "step: 29800 | loss: 0.3512\n",
      "step: 30000 | loss: 0.3562\n",
      "step: 30200 | loss: 0.3605\n",
      "step: 30400 | loss: 0.3284\n",
      "step: 30600 | loss: 0.4050\n",
      "step: 30800 | loss: 0.5189\n",
      "step: 31000 | loss: 0.3372\n",
      "step: 31200 | loss: 0.4501\n",
      "step: 31400 | loss: 0.5642\n",
      "step: 31600 | loss: 0.4817\n",
      "step: 31800 | loss: 0.4920\n",
      "step: 32000 | loss: 0.4377\n",
      "step: 32200 | loss: 0.4617\n",
      "step: 32400 | loss: 0.4237\n",
      "step: 32600 | loss: 0.4090\n",
      "step: 32800 | loss: 0.4279\n",
      "step: 33000 | loss: 0.3611\n",
      "step: 33200 | loss: 0.4240\n",
      "step: 33400 | loss: 0.4265\n",
      "step: 33600 | loss: 0.3103\n",
      "step: 33800 | loss: 0.3227\n",
      "step: 34000 | loss: 0.3488\n",
      "step: 34200 | loss: 0.4339\n",
      "step: 34400 | loss: 0.4847\n",
      "step: 34600 | loss: 0.4628\n",
      "step: 34800 | loss: 0.5130\n",
      "step: 35000 | loss: 0.4754\n",
      "step: 35200 | loss: 0.4384\n",
      "step: 35400 | loss: 0.4528\n",
      "step: 35600 | loss: 0.3444\n",
      "step: 35800 | loss: 0.4545\n",
      "step: 36000 | loss: 0.3946\n",
      "step: 36200 | loss: 0.3978\n",
      "step: 36400 | loss: 0.3624\n",
      "step: 36600 | loss: 0.3627\n",
      "step: 36800 | loss: 0.3848\n",
      "step: 37000 | loss: 0.3374\n",
      "step: 37200 | loss: 0.4646\n",
      "step: 37400 | loss: 0.4301\n",
      "step: 37600 | loss: 0.3858\n",
      "step: 37800 | loss: 0.5487\n",
      "step: 38000 | loss: 0.4917\n",
      "step: 38200 | loss: 0.4101\n",
      "step: 38400 | loss: 0.4080\n",
      "step: 38600 | loss: 0.4802\n",
      "step: 38800 | loss: 0.5051\n",
      "step: 39000 | loss: 0.4686\n",
      "step: 39200 | loss: 0.4901\n",
      "step: 39400 | loss: 0.4615\n",
      "step: 39600 | loss: 0.3991\n",
      "step: 39800 | loss: 0.6218\n",
      "step: 40000 | loss: 0.5788\n",
      "step: 40200 | loss: 0.4351\n",
      "step: 40400 | loss: 0.4285\n",
      "step: 40600 | loss: 0.5261\n",
      "step: 40800 | loss: 0.4194\n",
      "step: 41000 | loss: 0.4658\n",
      "step: 41200 | loss: 0.3643\n",
      "step: 41400 | loss: 0.4251\n",
      "step: 41600 | loss: 0.4269\n",
      "step: 41800 | loss: 0.4138\n",
      "step: 42000 | loss: 0.5012\n",
      "step: 42200 | loss: 0.4139\n",
      "step: 42400 | loss: 0.3348\n",
      "step: 42600 | loss: 0.3624\n",
      "step: 42800 | loss: 0.4002\n",
      "step: 43000 | loss: 0.4739\n",
      "step: 43200 | loss: 0.4981\n",
      "step: 43400 | loss: 0.4292\n",
      "step: 43600 | loss: 0.4699\n",
      "step: 43800 | loss: 0.4381\n",
      "step: 44000 | loss: 0.4030\n",
      "step: 44200 | loss: 0.5808\n",
      "step: 44400 | loss: 0.5366\n",
      "step: 44600 | loss: 0.5026\n",
      "step: 44800 | loss: 0.4936\n",
      "step: 45000 | loss: 0.5114\n",
      "step: 45200 | loss: 0.4079\n",
      "step: 45400 | loss: 0.3963\n",
      "step: 45600 | loss: 0.4565\n",
      "step: 45800 | loss: 0.4382\n",
      "step: 46000 | loss: 0.4282\n",
      "step: 46200 | loss: 0.4461\n",
      "step: 46400 | loss: 0.4278\n",
      "step: 46600 | loss: 0.4032\n",
      "step: 46800 | loss: 0.3536\n",
      "step: 47000 | loss: 0.3876\n",
      "step: 47200 | loss: 0.3575\n",
      "step: 47400 | loss: 0.3874\n",
      "step: 47600 | loss: 0.3988\n",
      "step: 47800 | loss: 0.3322\n",
      "step: 48000 | loss: 0.3808\n",
      "step: 48200 | loss: 0.3278\n",
      "step: 48400 | loss: 0.3015\n",
      "step: 48600 | loss: 0.4123\n",
      "step: 48800 | loss: 0.3671\n",
      "step: 49000 | loss: 0.4128\n",
      "step: 49200 | loss: 0.3999\n",
      "step: 49400 | loss: 0.4341\n",
      "step: 49600 | loss: 0.3600\n",
      "step: 49800 | loss: 0.3290\n",
      "step: 50000 | loss: 0.4253\n",
      "step: 50200 | loss: 0.4314\n",
      "step: 50400 | loss: 0.3608\n",
      "step: 50600 | loss: 0.4575\n",
      "step: 50800 | loss: 0.4399\n",
      "step: 51000 | loss: 0.4722\n",
      "step: 51200 | loss: 0.4999\n",
      "step: 51400 | loss: 0.3717\n",
      "step: 51600 | loss: 0.4093\n",
      "step: 51800 | loss: 0.4154\n",
      "step: 52000 | loss: 0.3966\n",
      "step: 52200 | loss: 0.3872\n",
      "step: 52400 | loss: 0.4052\n",
      "step: 52600 | loss: 0.5001\n",
      "step: 52800 | loss: 0.4865\n",
      "step: 53000 | loss: 0.4688\n",
      "step: 53200 | loss: 0.4859\n",
      "step: 53400 | loss: 0.3993\n",
      "step: 53600 | loss: 0.5396\n",
      "step: 53800 | loss: 0.5217\n",
      "step: 54000 | loss: 0.4926\n",
      "step: 54200 | loss: 0.6078\n",
      "step: 54400 | loss: 0.4711\n",
      "step: 54600 | loss: 0.4697\n",
      "step: 54800 | loss: 0.3783\n",
      "step: 55000 | loss: 0.4572\n",
      "step: 55200 | loss: 0.4093\n",
      "step: 55400 | loss: 0.4635\n",
      "step: 55600 | loss: 0.4753\n",
      "step: 55800 | loss: 0.5715\n",
      "step: 56000 | loss: 0.4833\n",
      "step: 56200 | loss: 0.4399\n",
      "step: 56400 | loss: 0.4398\n",
      "step: 56600 | loss: 0.3662\n",
      "step: 56800 | loss: 0.4842\n",
      "step: 57000 | loss: 0.3942\n",
      "step: 57200 | loss: 0.5097\n",
      "step: 57400 | loss: 0.4108\n",
      "step: 57600 | loss: 0.4026\n",
      "step: 57800 | loss: 0.4278\n",
      "step: 58000 | loss: 0.4392\n",
      "step: 58200 | loss: 0.4187\n",
      "step: 58400 | loss: 0.4349\n",
      "step: 58600 | loss: 0.4273\n",
      "step: 58800 | loss: 0.4208\n",
      "step: 59000 | loss: 0.3097\n",
      "step: 59200 | loss: 0.3714\n",
      "step: 59400 | loss: 0.4544\n",
      "step: 59600 | loss: 0.3973\n",
      "step: 59800 | loss: 0.4362\n",
      "step: 60000 | loss: 0.4505\n",
      "step: 60200 | loss: 0.4578\n",
      "step: 60400 | loss: 0.4392\n",
      "step: 60600 | loss: 0.5495\n",
      "step: 60800 | loss: 0.5323\n",
      "step: 61000 | loss: 0.5051\n",
      "step: 61200 | loss: 0.4579\n",
      "step: 61400 | loss: 0.4377\n",
      "step: 61600 | loss: 0.5289\n",
      "step: 61800 | loss: 0.4332\n",
      "step: 62000 | loss: 0.3624\n",
      "step: 62200 | loss: 0.4221\n",
      "step: 62400 | loss: 0.4193\n",
      "step: 62600 | loss: 0.3924\n",
      "step: 62800 | loss: 0.3301\n",
      "step: 63000 | loss: 0.3610\n",
      "step: 63200 | loss: 0.4184\n",
      "step: 63400 | loss: 0.4390\n",
      "step: 63600 | loss: 0.3316\n",
      "step: 63800 | loss: 0.3422\n",
      "step: 64000 | loss: 0.6721\n",
      "step: 64200 | loss: 0.5420\n",
      "step: 64400 | loss: 0.6271\n",
      "step: 64600 | loss: 0.5401\n",
      "step: 64800 | loss: 0.4465\n",
      "step: 65000 | loss: 0.4720\n",
      "step: 65200 | loss: 0.4715\n",
      "step: 65400 | loss: 0.4286\n",
      "step: 65600 | loss: 0.4762\n",
      "step: 65800 | loss: 0.5468\n",
      "step: 66000 | loss: 0.4839\n",
      "step: 66200 | loss: 0.4460\n",
      "step: 66400 | loss: 0.4346\n",
      "step: 66600 | loss: 0.4515\n",
      "step: 66800 | loss: 0.4554\n",
      "step: 67000 | loss: 0.4941\n",
      "step: 67200 | loss: 0.4772\n",
      "step: 67400 | loss: 0.6086\n",
      "step: 67600 | loss: 0.5993\n",
      "step: 67800 | loss: 0.5631\n",
      "step: 68000 | loss: 0.5374\n",
      "step: 68200 | loss: 0.5492\n",
      "step: 68400 | loss: 0.5888\n",
      "step: 68600 | loss: 0.5602\n",
      "step: 68800 | loss: 0.5202\n",
      "step: 69000 | loss: 0.4587\n",
      "step: 69200 | loss: 0.4709\n",
      "step: 69400 | loss: 0.5020\n",
      "step: 69600 | loss: 0.4094\n",
      "step: 69800 | loss: 0.4486\n",
      "step: 70000 | loss: 0.4447\n",
      "step: 70200 | loss: 0.4624\n",
      "step: 70400 | loss: 0.4176\n",
      "step: 70600 | loss: 0.4487\n",
      "step: 70800 | loss: 0.4850\n",
      "step: 71000 | loss: 0.5262\n",
      "step: 71200 | loss: 0.4515\n",
      "step: 71400 | loss: 0.3874\n",
      "step: 71600 | loss: 0.5102\n",
      "step: 71800 | loss: 0.4573\n",
      "step: 72000 | loss: 0.4617\n",
      "step: 72200 | loss: 0.4475\n",
      "step: 72400 | loss: 0.5098\n",
      "step: 72600 | loss: 0.4481\n",
      "step: 72800 | loss: 0.4163\n",
      "step: 73000 | loss: 0.4280\n",
      "step: 73200 | loss: 0.4432\n",
      "step: 73400 | loss: 0.5406\n",
      "step: 73600 | loss: 0.5209\n",
      "step: 73800 | loss: 0.5162\n",
      "step: 74000 | loss: 0.6459\n",
      "step: 74200 | loss: 0.5079\n",
      "step: 74400 | loss: 0.5365\n",
      "step: 74600 | loss: 0.4227\n",
      "step: 74800 | loss: 0.4466\n",
      "step: 75000 | loss: 0.5117\n",
      "step: 75200 | loss: 0.3786\n",
      "step: 75400 | loss: 0.3680\n",
      "step: 75600 | loss: 0.5469\n",
      "step: 75800 | loss: 0.5166\n",
      "step: 76000 | loss: 0.4804\n",
      "step: 76200 | loss: 0.4156\n",
      "step: 76400 | loss: 0.4566\n",
      "step: 76600 | loss: 0.4345\n",
      "step: 76800 | loss: 0.4299\n",
      "step: 77000 | loss: 0.4452\n",
      "step: 77200 | loss: 0.5048\n",
      "step: 77400 | loss: 0.3710\n",
      "step: 77600 | loss: 0.3763\n",
      "step: 77800 | loss: 0.4597\n",
      "step: 78000 | loss: 0.3780\n",
      "step: 78200 | loss: 0.3421\n",
      "step: 78400 | loss: 0.3772\n",
      "step: 78600 | loss: 0.3746\n",
      "step: 78800 | loss: 0.3798\n",
      "step: 79000 | loss: 0.3147\n",
      "step: 79200 | loss: 0.3654\n",
      "step: 79400 | loss: 0.3487\n",
      "step: 79600 | loss: 0.3625\n",
      "step: 79800 | loss: 0.4062\n",
      "step: 80000 | loss: 0.3543\n",
      "step: 80200 | loss: 0.4108\n",
      "step: 80400 | loss: 0.4572\n",
      "step: 80600 | loss: 0.4399\n",
      "step: 80800 | loss: 0.3833\n",
      "step: 81000 | loss: 0.4590\n",
      "step: 81200 | loss: 0.4703\n",
      "step: 81400 | loss: 0.4376\n",
      "step: 81600 | loss: 0.4280\n",
      "step: 81800 | loss: 0.3697\n",
      "step: 82000 | loss: 0.4074\n",
      "step: 82200 | loss: 0.3509\n",
      "step: 82400 | loss: 0.4337\n",
      "step: 82600 | loss: 0.4149\n",
      "step: 82800 | loss: 0.3997\n",
      "step: 83000 | loss: 0.3439\n",
      "step: 83200 | loss: 0.3471\n",
      "step: 83400 | loss: 0.3502\n",
      "step: 83600 | loss: 0.4045\n",
      "step: 83800 | loss: 0.4074\n",
      "step: 84000 | loss: 0.4527\n",
      "step: 84200 | loss: 0.4542\n",
      "step: 84400 | loss: 0.3800\n",
      "step: 84600 | loss: 0.5052\n",
      "step: 84800 | loss: 0.5101\n",
      "step: 85000 | loss: 0.4102\n",
      "step: 85200 | loss: 0.4650\n",
      "step: 85400 | loss: 0.4952\n",
      "step: 85600 | loss: 0.4399\n",
      "step: 85800 | loss: 0.3856\n",
      "step: 86000 | loss: 0.4897\n",
      "step: 86200 | loss: 0.4418\n",
      "step: 86400 | loss: 0.5100\n",
      "step: 86600 | loss: 0.4494\n",
      "step: 86800 | loss: 0.4398\n",
      "step: 87000 | loss: 0.3466\n",
      "step: 87200 | loss: 0.4469\n",
      "step: 87400 | loss: 0.4389\n",
      "step: 87600 | loss: 0.3611\n",
      "step: 87800 | loss: 0.4398\n",
      "step: 88000 | loss: 0.4479\n",
      "step: 88200 | loss: 0.3945\n",
      "step: 88400 | loss: 0.4128\n",
      "step: 88600 | loss: 0.4496\n",
      "step: 88800 | loss: 0.4376\n",
      "step: 89000 | loss: 0.5140\n",
      "step: 89200 | loss: 0.4848\n",
      "step: 89400 | loss: 0.4640\n",
      "step: 89600 | loss: 0.4791\n",
      "step: 89800 | loss: 0.5189\n",
      "step: 90000 | loss: 0.4999\n",
      "step: 90200 | loss: 0.7220\n",
      "step: 90400 | loss: 0.6515\n",
      "step: 90600 | loss: 0.5591\n",
      "step: 90800 | loss: 0.6345\n",
      "step: 91000 | loss: 0.4808\n",
      "step: 91200 | loss: 0.5788\n",
      "step: 91400 | loss: 0.5751\n",
      "step: 91600 | loss: 0.6014\n",
      "step: 91800 | loss: 0.6387\n",
      "step: 92000 | loss: 0.5888\n",
      "step: 92200 | loss: 0.5812\n",
      "step: 92400 | loss: 0.4751\n",
      "step: 92600 | loss: 0.6252\n",
      "step: 92800 | loss: 0.6614\n",
      "step: 93000 | loss: 0.6526\n",
      "step: 93200 | loss: 0.5869\n",
      "step: 93400 | loss: 0.4898\n",
      "step: 93600 | loss: 0.6585\n",
      "step: 93800 | loss: 0.5679\n",
      "step: 94000 | loss: 0.6876\n",
      "step: 94200 | loss: 0.6616\n",
      "step: 94400 | loss: 0.5912\n",
      "step: 94600 | loss: 0.5977\n",
      "step: 94800 | loss: 0.5168\n",
      "step: 95000 | loss: 0.5894\n",
      "step: 95200 | loss: 0.5267\n",
      "step: 95400 | loss: 0.4957\n",
      "step: 95600 | loss: 0.5397\n",
      "step: 95800 | loss: 0.5575\n",
      "step: 96000 | loss: 0.4443\n",
      "step: 96200 | loss: 0.5305\n",
      "step: 96400 | loss: 0.5491\n",
      "step: 96600 | loss: 0.5555\n",
      "step: 96800 | loss: 0.5944\n",
      "step: 97000 | loss: 0.6522\n",
      "step: 97200 | loss: 0.5425\n",
      "step: 97400 | loss: 0.6158\n",
      "step: 97600 | loss: 0.5717\n",
      "step: 97800 | loss: 0.5690\n",
      "step: 98000 | loss: 0.5349\n",
      "step: 98200 | loss: 0.5176\n",
      "step: 98400 | loss: 0.6113\n",
      "step: 98600 | loss: 0.6272\n",
      "step: 98800 | loss: 0.6717\n",
      "step: 99000 | loss: 0.6948\n",
      "step: 99200 | loss: 0.5651\n",
      "step: 99400 | loss: 0.6456\n",
      "step: 99600 | loss: 0.5793\n",
      "step: 99800 | loss: 0.5361\n",
      "step: 100000 | loss: 0.6383\n",
      "step: 100200 | loss: 0.6279\n",
      "step: 100400 | loss: 0.6533\n",
      "step: 100600 | loss: 0.5586\n",
      "step: 100800 | loss: 0.5870\n",
      "step: 101000 | loss: 0.5716\n",
      "step: 101200 | loss: 0.6313\n",
      "step: 101400 | loss: 0.7077\n",
      "step: 101600 | loss: 0.6740\n",
      "step: 101800 | loss: 0.6478\n",
      "step: 102000 | loss: 0.6081\n",
      "step: 102200 | loss: 0.6121\n",
      "step: 102400 | loss: 0.5176\n",
      "step: 102600 | loss: 0.4858\n",
      "step: 102800 | loss: 0.6970\n",
      "step: 103000 | loss: 0.5971\n",
      "step: 103200 | loss: 0.6236\n",
      "step: 103400 | loss: 0.5640\n",
      "step: 103600 | loss: 0.5789\n",
      "step: 103800 | loss: 0.4365\n",
      "step: 104000 | loss: 0.4929\n",
      "step: 104200 | loss: 0.5123\n",
      "step: 104400 | loss: 0.4525\n",
      "step: 104600 | loss: 0.4662\n",
      "step: 104800 | loss: 0.5293\n",
      "step: 105000 | loss: 0.5087\n",
      "step: 105200 | loss: 0.4015\n",
      "step: 105400 | loss: 0.5275\n",
      "step: 105600 | loss: 0.4215\n",
      "step: 105800 | loss: 0.4500\n",
      "step: 106000 | loss: 0.4405\n",
      "step: 106200 | loss: 0.4377\n",
      "step: 106400 | loss: 0.4607\n",
      "step: 106600 | loss: 0.7871\n",
      "step: 106800 | loss: 0.6051\n",
      "step: 107000 | loss: 0.5851\n",
      "step: 107200 | loss: 0.5802\n",
      "step: 107400 | loss: 0.6140\n",
      "step: 107600 | loss: 0.4783\n",
      "step: 107800 | loss: 0.4741\n",
      "step: 108000 | loss: 0.5104\n",
      "step: 108200 | loss: 0.5131\n",
      "step: 108400 | loss: 0.5539\n",
      "step: 108600 | loss: 0.4809\n",
      "step: 108800 | loss: 0.4893\n",
      "step: 109000 | loss: 0.4460\n",
      "step: 109200 | loss: 0.5500\n",
      "step: 109400 | loss: 0.5384\n",
      "step: 109600 | loss: 0.4247\n",
      "step: 109800 | loss: 0.5103\n",
      "step: 110000 | loss: 0.5354\n",
      "step: 110200 | loss: 0.5953\n",
      "step: 110400 | loss: 0.4947\n",
      "step: 110600 | loss: 0.4082\n",
      "step: 110800 | loss: 0.4209\n",
      "step: 111000 | loss: 0.4065\n",
      "step: 111200 | loss: 0.3822\n",
      "step: 111400 | loss: 0.4465\n",
      "step: 111600 | loss: 0.3564\n",
      "step: 111800 | loss: 0.4623\n",
      "step: 112000 | loss: 0.3536\n",
      "step: 112200 | loss: 0.4476\n",
      "step: 112400 | loss: 0.4506\n",
      "step: 112600 | loss: 0.3805\n",
      "step: 112800 | loss: 0.4119\n",
      "step: 113000 | loss: 0.5150\n",
      "step: 113200 | loss: 0.6248\n",
      "step: 113400 | loss: 0.5467\n",
      "step: 113600 | loss: 0.6156\n",
      "step: 113800 | loss: 0.5450\n",
      "step: 114000 | loss: 0.4817\n",
      "step: 114200 | loss: 0.5402\n",
      "step: 114400 | loss: 0.5000\n",
      "step: 114600 | loss: 0.5135\n",
      "step: 114800 | loss: 0.4786\n",
      "step: 115000 | loss: 0.5129\n",
      "step: 115200 | loss: 0.4936\n",
      "step: 115400 | loss: 0.4291\n",
      "step: 115600 | loss: 0.5007\n",
      "step: 115800 | loss: 0.5590\n",
      "step: 116000 | loss: 0.4682\n",
      "step: 116200 | loss: 0.5055\n",
      "step: 116400 | loss: 0.5393\n",
      "step: 116600 | loss: 0.5508\n",
      "step: 116800 | loss: 0.4789\n",
      "step: 117000 | loss: 0.4438\n",
      "step: 117200 | loss: 0.4637\n",
      "step: 117400 | loss: 0.5686\n",
      "step: 117600 | loss: 0.5457\n",
      "step: 117800 | loss: 0.4231\n",
      "step: 118000 | loss: 0.3999\n",
      "step: 118200 | loss: 0.4020\n",
      "step: 118400 | loss: 0.4283\n",
      "step: 118600 | loss: 0.4337\n",
      "step: 118800 | loss: 0.4939\n",
      "step: 119000 | loss: 0.4028\n",
      "step: 119200 | loss: 0.4059\n",
      "step: 119400 | loss: 0.4536\n",
      "step: 119600 | loss: 0.5709\n",
      "step: 119800 | loss: 0.4946\n",
      "step: 120000 | loss: 0.4986\n",
      "step: 120200 | loss: 0.3292\n",
      "step: 120400 | loss: 0.3081\n",
      "step: 120600 | loss: 0.3273\n",
      "step: 120800 | loss: 0.3814\n",
      "step: 121000 | loss: 0.2699\n",
      "step: 121200 | loss: 0.3427\n",
      "step: 121400 | loss: 0.3157\n",
      "step: 121600 | loss: 0.3729\n",
      "step: 121800 | loss: 0.2922\n",
      "step: 122000 | loss: 0.3087\n",
      "step: 122200 | loss: 0.3196\n",
      "step: 122400 | loss: 0.3387\n",
      "step: 122600 | loss: 0.2960\n",
      "step: 122800 | loss: 0.3213\n",
      "step: 123000 | loss: 0.4030\n",
      "step: 123200 | loss: 0.3581\n",
      "step: 123400 | loss: 0.3461\n",
      "step: 123600 | loss: 0.4103\n",
      "step: 123800 | loss: 0.3936\n",
      "step: 124000 | loss: 0.3040\n",
      "step: 124200 | loss: 0.3365\n",
      "step: 124400 | loss: 0.3525\n",
      "step: 124600 | loss: 0.3008\n",
      "step: 124800 | loss: 0.2872\n",
      "step: 125000 | loss: 0.3116\n",
      "step: 125200 | loss: 0.2934\n",
      "step: 125400 | loss: 0.2591\n",
      "step: 125600 | loss: 0.3700\n",
      "step: 125800 | loss: 0.2964\n",
      "step: 126000 | loss: 0.3001\n",
      "step: 126200 | loss: 0.3239\n",
      "step: 126400 | loss: 0.3858\n",
      "step: 126600 | loss: 0.3861\n",
      "step: 126800 | loss: 0.4507\n",
      "step: 127000 | loss: 0.3317\n",
      "step: 127200 | loss: 0.4126\n",
      "step: 127400 | loss: 0.4008\n",
      "step: 127600 | loss: 0.3463\n",
      "step: 127800 | loss: 0.3849\n",
      "step: 128000 | loss: 0.3750\n",
      "step: 128200 | loss: 0.3782\n",
      "step: 128400 | loss: 0.3815\n",
      "step: 128600 | loss: 0.3685\n",
      "step: 128800 | loss: 0.3326\n",
      "step: 129000 | loss: 0.3133\n",
      "step: 129200 | loss: 0.2833\n",
      "step: 129400 | loss: 0.4130\n",
      "step: 129600 | loss: 0.6500\n",
      "step: 129800 | loss: 0.7015\n",
      "step: 130000 | loss: 0.5262\n",
      "step: 130200 | loss: 0.6499\n",
      "step: 130400 | loss: 0.5044\n",
      "step: 130600 | loss: 0.4797\n",
      "step: 130800 | loss: 0.3624\n",
      "step: 131000 | loss: 0.4622\n",
      "step: 131200 | loss: 0.4724\n",
      "step: 131400 | loss: 0.4599\n",
      "step: 131600 | loss: 0.4550\n",
      "step: 131800 | loss: 0.4282\n",
      "step: 132000 | loss: 0.4260\n",
      "step: 132200 | loss: 0.4432\n",
      "step: 132400 | loss: 0.4084\n",
      "step: 132600 | loss: 0.4397\n",
      "step: 132800 | loss: 0.4733\n",
      "step: 133000 | loss: 0.4549\n",
      "step: 133200 | loss: 0.4436\n",
      "step: 133400 | loss: 0.3771\n",
      "step: 133600 | loss: 0.4472\n",
      "step: 133800 | loss: 0.4382\n",
      "step: 134000 | loss: 0.4519\n",
      "step: 134200 | loss: 0.4253\n",
      "step: 134400 | loss: 0.4429\n",
      "step: 134600 | loss: 0.3513\n",
      "step: 134800 | loss: 0.3358\n",
      "step: 135000 | loss: 0.4047\n",
      "step: 135200 | loss: 0.3635\n",
      "step: 135400 | loss: 0.3380\n",
      "step: 135600 | loss: 0.3316\n",
      "step: 135800 | loss: 0.3889\n",
      "step: 136000 | loss: 0.4225\n",
      "step: 136200 | loss: 0.4124\n",
      "step: 136400 | loss: 0.3951\n",
      "step: 136600 | loss: 0.4236\n",
      "step: 136800 | loss: 0.3966\n",
      "step: 137000 | loss: 0.5268\n",
      "step: 137200 | loss: 0.5551\n",
      "step: 137400 | loss: 0.4697\n",
      "step: 137600 | loss: 0.4442\n",
      "step: 137800 | loss: 0.5524\n",
      "step: 138000 | loss: 0.4493\n",
      "step: 138200 | loss: 0.4295\n",
      "step: 138400 | loss: 0.5847\n",
      "step: 138600 | loss: 0.5042\n",
      "step: 138800 | loss: 0.5002\n",
      "step: 139000 | loss: 0.4441\n",
      "step: 139200 | loss: 0.4432\n",
      "step: 139400 | loss: 0.4566\n",
      "step: 139600 | loss: 0.5372\n",
      "step: 139800 | loss: 0.5111\n",
      "step: 140000 | loss: 0.4599\n",
      "step: 140200 | loss: 0.5353\n",
      "step: 140400 | loss: 0.4693\n",
      "step: 140600 | loss: 0.4919\n",
      "step: 140800 | loss: 0.4613\n",
      "step: 141000 | loss: 0.4457\n",
      "step: 141200 | loss: 0.4162\n",
      "step: 141400 | loss: 0.3733\n",
      "step: 141600 | loss: 0.3547\n",
      "step: 141800 | loss: 0.3653\n",
      "step: 142000 | loss: 0.4569\n",
      "step: 142200 | loss: 0.3902\n",
      "step: 142400 | loss: 0.4239\n",
      "step: 142600 | loss: 0.4197\n",
      "step: 142800 | loss: 0.4788\n",
      "step: 143000 | loss: 0.4218\n",
      "step: 143200 | loss: 0.3720\n",
      "step: 143400 | loss: 0.3816\n",
      "step: 143600 | loss: 0.3436\n",
      "step: 143800 | loss: 0.5044\n",
      "step: 144000 | loss: 0.4196\n",
      "step: 144200 | loss: 0.4196\n",
      "step: 144400 | loss: 0.3740\n",
      "step: 144600 | loss: 0.3997\n",
      "step: 144800 | loss: 0.3615\n",
      "step: 145000 | loss: 0.4050\n",
      "step: 145200 | loss: 0.3571\n",
      "step: 145400 | loss: 0.3326\n",
      "step: 145600 | loss: 0.2950\n",
      "step: 145800 | loss: 0.5198\n",
      "step: 146000 | loss: 0.5255\n",
      "step: 146200 | loss: 0.5603\n",
      "step: 146400 | loss: 0.4155\n",
      "step: 146600 | loss: 0.3828\n",
      "step: 146800 | loss: 0.3409\n",
      "step: 147000 | loss: 0.3744\n",
      "step: 147200 | loss: 0.3573\n",
      "step: 147400 | loss: 0.4101\n",
      "step: 147600 | loss: 0.3219\n",
      "step: 147800 | loss: 0.3630\n",
      "step: 148000 | loss: 0.3301\n",
      "step: 148200 | loss: 0.3853\n",
      "step: 148400 | loss: 0.4721\n",
      "step: 148600 | loss: 0.3296\n",
      "step: 148800 | loss: 0.3043\n",
      "step: 149000 | loss: 0.3970\n",
      "step: 149200 | loss: 0.4326\n",
      "step: 149400 | loss: 0.3814\n",
      "step: 149600 | loss: 0.3661\n",
      "step: 149800 | loss: 0.3713\n",
      "step: 150000 | loss: 0.3064\n",
      "step: 150200 | loss: 0.3278\n",
      "step: 150400 | loss: 0.4597\n",
      "step: 150600 | loss: 0.4210\n",
      "step: 150800 | loss: 0.3760\n",
      "step: 151000 | loss: 0.4662\n",
      "step: 151200 | loss: 0.2915\n",
      "step: 151400 | loss: 0.3796\n",
      "step: 151600 | loss: 0.4016\n",
      "step: 151800 | loss: 0.4450\n",
      "step: 152000 | loss: 0.4285\n",
      "step: 152200 | loss: 0.4167\n",
      "step: 152400 | loss: 0.3816\n",
      "step: 152600 | loss: 0.4426\n",
      "step: 152800 | loss: 0.3126\n",
      "step: 153000 | loss: 0.3445\n",
      "step: 153200 | loss: 0.3723\n",
      "step: 153400 | loss: 0.3734\n",
      "step: 153600 | loss: 0.4854\n",
      "step: 153800 | loss: 0.4513\n",
      "step: 154000 | loss: 0.4417\n",
      "step: 154200 | loss: 0.3529\n",
      "step: 154400 | loss: 0.3804\n",
      "step: 154600 | loss: 0.3783\n",
      "step: 154800 | loss: 0.4006\n",
      "step: 155000 | loss: 0.4440\n",
      "step: 155200 | loss: 0.4073\n",
      "step: 155400 | loss: 0.4131\n",
      "step: 155600 | loss: 0.5476\n",
      "step: 155800 | loss: 0.7459\n",
      "step: 156000 | loss: 0.8766\n",
      "step: 156200 | loss: 0.5868\n",
      "step: 156400 | loss: 0.7657\n",
      "step: 156600 | loss: 0.5807\n",
      "step: 156800 | loss: 0.5049\n",
      "step: 157000 | loss: 0.5316\n",
      "step: 157200 | loss: 0.4288\n",
      "step: 157400 | loss: 0.5350\n",
      "step: 157600 | loss: 0.4706\n",
      "step: 157800 | loss: 0.6194\n",
      "step: 158000 | loss: 0.4603\n",
      "step: 158200 | loss: 0.5993\n",
      "step: 158400 | loss: 0.5504\n",
      "step: 158600 | loss: 0.5102\n",
      "step: 158800 | loss: 0.5515\n",
      "step: 159000 | loss: 0.6035\n",
      "step: 159200 | loss: 0.6265\n",
      "step: 159400 | loss: 0.5374\n",
      "step: 159600 | loss: 0.6061\n",
      "step: 159800 | loss: 0.5114\n",
      "step: 160000 | loss: 0.5159\n",
      "step: 160200 | loss: 0.4354\n",
      "step: 160400 | loss: 0.4881\n",
      "step: 160600 | loss: 0.4228\n",
      "step: 160800 | loss: 0.4112\n",
      "step: 161000 | loss: 0.3332\n",
      "step: 161200 | loss: 0.4023\n",
      "step: 161400 | loss: 0.4420\n",
      "step: 161600 | loss: 0.4815\n",
      "step: 161800 | loss: 0.3555\n",
      "step: 162000 | loss: 0.4708\n",
      "step: 162200 | loss: 0.4168\n",
      "step: 162400 | loss: 0.4508\n",
      "step: 162600 | loss: 0.5274\n",
      "step: 162800 | loss: 0.4280\n",
      "step: 163000 | loss: 0.4387\n",
      "step: 163200 | loss: 0.4132\n",
      "step: 163400 | loss: 0.3819\n",
      "step: 163600 | loss: 0.4300\n",
      "step: 163800 | loss: 0.4708\n",
      "step: 164000 | loss: 0.4940\n",
      "step: 164200 | loss: 0.4429\n",
      "step: 164400 | loss: 0.5638\n",
      "step: 164600 | loss: 0.3798\n",
      "step: 164800 | loss: 0.4792\n",
      "step: 165000 | loss: 0.5355\n",
      "step: 165200 | loss: 0.4509\n",
      "step: 165400 | loss: 0.4876\n",
      "step: 165600 | loss: 0.4569\n",
      "step: 165800 | loss: 0.4895\n",
      "step: 166000 | loss: 0.4682\n",
      "step: 166200 | loss: 0.4293\n",
      "step: 166400 | loss: 0.3945\n",
      "step: 166600 | loss: 0.4080\n",
      "step: 166800 | loss: 0.3524\n",
      "step: 167000 | loss: 0.4147\n",
      "step: 167200 | loss: 0.4130\n",
      "step: 167400 | loss: 0.3499\n",
      "step: 167600 | loss: 0.4236\n",
      "step: 167800 | loss: 0.3737\n",
      "step: 168000 | loss: 0.3008\n",
      "step: 168200 | loss: 0.3329\n",
      "step: 168400 | loss: 0.3918\n",
      "step: 168600 | loss: 0.4304\n",
      "step: 168800 | loss: 0.5918\n",
      "step: 169000 | loss: 0.5920\n",
      "step: 169200 | loss: 0.6764\n",
      "step: 169400 | loss: 0.5396\n",
      "step: 169600 | loss: 0.6084\n",
      "step: 169800 | loss: 0.4743\n",
      "step: 170000 | loss: 0.4387\n",
      "step: 170200 | loss: 0.4131\n",
      "step: 170400 | loss: 0.4261\n",
      "step: 170600 | loss: 0.4274\n",
      "step: 170800 | loss: 0.3773\n",
      "step: 171000 | loss: 0.3588\n",
      "step: 171200 | loss: 0.4037\n",
      "step: 171400 | loss: 0.4787\n",
      "step: 171600 | loss: 0.3815\n",
      "step: 171800 | loss: 0.4305\n",
      "step: 172000 | loss: 0.5150\n",
      "step: 172200 | loss: 0.3945\n",
      "step: 172400 | loss: 0.4835\n",
      "step: 172600 | loss: 0.3929\n",
      "step: 172800 | loss: 0.4189\n",
      "step: 173000 | loss: 0.3965\n",
      "step: 173200 | loss: 0.3842\n",
      "step: 173400 | loss: 0.3281\n",
      "step: 173600 | loss: 0.3689\n",
      "step: 173800 | loss: 0.3324\n",
      "step: 174000 | loss: 0.3147\n",
      "step: 174200 | loss: 0.4273\n",
      "step: 174400 | loss: 0.2778\n",
      "step: 174600 | loss: 0.3582\n",
      "step: 174800 | loss: 0.3642\n",
      "step: 175000 | loss: 0.3772\n",
      "step: 175200 | loss: 0.4565\n",
      "step: 175400 | loss: 0.4461\n",
      "step: 175600 | loss: 0.4095\n",
      "step: 175800 | loss: 0.4143\n",
      "step: 176000 | loss: 0.3878\n",
      "step: 176200 | loss: 0.4818\n",
      "step: 176400 | loss: 0.3862\n",
      "step: 176600 | loss: 0.5028\n",
      "step: 176800 | loss: 0.3619\n",
      "step: 177000 | loss: 0.4055\n",
      "step: 177200 | loss: 0.4292\n",
      "step: 177400 | loss: 0.4009\n",
      "step: 177600 | loss: 0.4852\n",
      "step: 177800 | loss: 0.4058\n",
      "step: 178000 | loss: 0.3884\n",
      "step: 178200 | loss: 0.4797\n",
      "step: 178400 | loss: 0.3866\n",
      "step: 178600 | loss: 0.4444\n",
      "step: 178800 | loss: 0.4943\n",
      "step: 179000 | loss: 0.4844\n",
      "step: 179200 | loss: 0.3763\n",
      "step: 179400 | loss: 0.4177\n",
      "step: 179600 | loss: 0.4224\n",
      "step: 179800 | loss: 0.3069\n",
      "step: 180000 | loss: 0.4161\n",
      "step: 180200 | loss: 0.4171\n",
      "step: 180400 | loss: 0.3919\n",
      "step: 180600 | loss: 0.3202\n",
      "step: 180800 | loss: 0.4327\n",
      "step: 181000 | loss: 0.3650\n",
      "step: 181200 | loss: 0.4128\n",
      "step: 181400 | loss: 0.3741\n",
      "step: 181600 | loss: 0.4232\n",
      "step: 181800 | loss: 0.4328\n",
      "step: 182000 | loss: 0.4628\n",
      "step: 182200 | loss: 0.4234\n",
      "step: 182400 | loss: 0.5114\n",
      "step: 182600 | loss: 0.4046\n",
      "step: 182800 | loss: 0.4411\n",
      "step: 183000 | loss: 0.3927\n",
      "step: 183200 | loss: 0.3525\n",
      "step: 183400 | loss: 0.4049\n",
      "step: 183600 | loss: 0.4152\n",
      "step: 183800 | loss: 0.3870\n",
      "step: 184000 | loss: 0.5037\n",
      "step: 184200 | loss: 0.3887\n",
      "step: 184400 | loss: 0.4840\n",
      "step: 184600 | loss: 0.4082\n",
      "step: 184800 | loss: 0.5030\n",
      "step: 185000 | loss: 0.4958\n",
      "step: 185200 | loss: 0.4003\n",
      "step: 185400 | loss: 0.4260\n",
      "step: 185600 | loss: 0.4547\n",
      "step: 185800 | loss: 0.4609\n",
      "step: 186000 | loss: 0.5152\n",
      "step: 186200 | loss: 0.4615\n",
      "step: 186400 | loss: 0.4024\n",
      "step: 186600 | loss: 0.4175\n",
      "step: 186800 | loss: 0.4361\n",
      "step: 187000 | loss: 0.3802\n",
      "step: 187200 | loss: 0.3148\n",
      "step: 187400 | loss: 0.5463\n",
      "step: 187600 | loss: 0.3795\n",
      "step: 187800 | loss: 0.4114\n",
      "step: 188000 | loss: 0.4545\n",
      "step: 188200 | loss: 0.5537\n",
      "step: 188400 | loss: 0.4571\n",
      "step: 188600 | loss: 0.5268\n",
      "step: 188800 | loss: 0.5397\n",
      "step: 189000 | loss: 0.6150\n",
      "step: 189200 | loss: 0.4963\n",
      "step: 189400 | loss: 0.4062\n",
      "step: 189600 | loss: 0.5207\n",
      "step: 189800 | loss: 0.4280\n",
      "step: 190000 | loss: 0.4866\n",
      "step: 190200 | loss: 0.5603\n",
      "step: 190400 | loss: 0.4998\n",
      "step: 190600 | loss: 0.4980\n",
      "step: 190800 | loss: 0.4901\n",
      "step: 191000 | loss: 0.5376\n",
      "step: 191200 | loss: 0.5054\n",
      "step: 191400 | loss: 0.5392\n",
      "step: 191600 | loss: 0.5534\n",
      "step: 191800 | loss: 0.6679\n",
      "step: 192000 | loss: 0.5225\n",
      "step: 192200 | loss: 0.4880\n",
      "step: 192400 | loss: 0.5374\n",
      "step: 192600 | loss: 0.5264\n",
      "step: 192800 | loss: 0.5217\n",
      "step: 193000 | loss: 0.5295\n",
      "step: 193200 | loss: 0.6374\n",
      "step: 193400 | loss: 0.6165\n",
      "step: 193600 | loss: 0.5807\n",
      "step: 193800 | loss: 0.6844\n",
      "step: 194000 | loss: 0.6100\n",
      "step: 194200 | loss: 0.5699\n",
      "step: 194400 | loss: 0.5519\n",
      "step: 194600 | loss: 0.5709\n",
      "step: 194800 | loss: 0.6370\n",
      "step: 195000 | loss: 0.6799\n",
      "step: 195200 | loss: 0.6725\n",
      "step: 195400 | loss: 0.7415\n",
      "step: 195600 | loss: 0.7789\n",
      "step: 195800 | loss: 0.5667\n",
      "step: 196000 | loss: 0.6237\n",
      "step: 196200 | loss: 0.6217\n",
      "step: 196400 | loss: 0.5888\n",
      "step: 196600 | loss: 0.5730\n",
      "step: 196800 | loss: 0.4836\n",
      "step: 197000 | loss: 0.4586\n",
      "step: 197200 | loss: 0.3911\n",
      "step: 197400 | loss: 0.4571\n",
      "step: 197600 | loss: 0.4006\n",
      "step: 197800 | loss: 0.4789\n",
      "step: 198000 | loss: 0.4679\n",
      "step: 198200 | loss: 0.5178\n",
      "step: 198400 | loss: 0.3997\n",
      "step: 198600 | loss: 0.4556\n",
      "step: 198800 | loss: 0.4988\n",
      "step: 199000 | loss: 0.4330\n",
      "step: 199200 | loss: 0.4425\n",
      "step: 199400 | loss: 0.4474\n",
      "step: 199600 | loss: 0.3673\n",
      "step: 199800 | loss: 0.4729\n",
      "step: 200000 | loss: 0.3524\n",
      "step: 200200 | loss: 0.3310\n",
      "step: 200400 | loss: 0.4148\n",
      "step: 200600 | loss: 0.3515\n",
      "step: 200800 | loss: 0.4299\n",
      "step: 201000 | loss: 0.3742\n",
      "step: 201200 | loss: 0.3666\n",
      "step: 201400 | loss: 0.3372\n",
      "step: 201600 | loss: 0.3980\n",
      "step: 201800 | loss: 0.3587\n",
      "step: 202000 | loss: 0.3360\n",
      "step: 202200 | loss: 0.3930\n",
      "step: 202400 | loss: 0.3366\n",
      "step: 202600 | loss: 0.3177\n",
      "step: 202800 | loss: 0.3134\n",
      "step: 203000 | loss: 0.3392\n",
      "step: 203200 | loss: 0.3105\n",
      "step: 203400 | loss: 0.2975\n",
      "step: 203600 | loss: 0.3132\n",
      "step: 203800 | loss: 0.2840\n",
      "step: 204000 | loss: 0.3038\n",
      "step: 204200 | loss: 0.2982\n",
      "step: 204400 | loss: 0.3087\n",
      "step: 204600 | loss: 0.3373\n",
      "step: 204800 | loss: 0.4310\n",
      "step: 205000 | loss: 0.5241\n",
      "step: 205200 | loss: 0.4421\n",
      "step: 205400 | loss: 0.3758\n",
      "step: 205600 | loss: 0.3803\n",
      "step: 205800 | loss: 0.3547\n",
      "step: 206000 | loss: 0.3840\n",
      "step: 206200 | loss: 0.4009\n",
      "step: 206400 | loss: 0.3813\n",
      "step: 206600 | loss: 0.3564\n",
      "step: 206800 | loss: 0.3132\n",
      "step: 207000 | loss: 0.3630\n",
      "step: 207200 | loss: 0.3530\n",
      "step: 207400 | loss: 0.3572\n",
      "step: 207600 | loss: 0.3631\n",
      "step: 207800 | loss: 0.4803\n",
      "step: 208000 | loss: 0.4025\n",
      "step: 208200 | loss: 0.5269\n",
      "step: 208400 | loss: 0.4825\n",
      "step: 208600 | loss: 0.4589\n",
      "step: 208800 | loss: 0.4765\n",
      "step: 209000 | loss: 0.4489\n",
      "step: 209200 | loss: 0.4315\n",
      "step: 209400 | loss: 0.3874\n",
      "step: 209600 | loss: 0.5091\n",
      "step: 209800 | loss: 0.3891\n",
      "step: 210000 | loss: 0.4141\n",
      "step: 210200 | loss: 0.3651\n",
      "step: 210400 | loss: 0.3852\n",
      "step: 210600 | loss: 0.4144\n",
      "step: 210800 | loss: 0.3879\n",
      "step: 211000 | loss: 0.4098\n",
      "step: 211200 | loss: 0.4566\n",
      "step: 211400 | loss: 0.4114\n",
      "step: 211600 | loss: 0.5224\n",
      "step: 211800 | loss: 0.4681\n",
      "step: 212000 | loss: 0.4459\n",
      "step: 212200 | loss: 0.3897\n",
      "step: 212400 | loss: 0.3158\n",
      "step: 212600 | loss: 0.4661\n",
      "step: 212800 | loss: 0.3787\n",
      "step: 213000 | loss: 0.3840\n",
      "step: 213200 | loss: 0.4235\n",
      "step: 213400 | loss: 0.4492\n",
      "step: 213600 | loss: 0.3978\n",
      "step: 213800 | loss: 0.4191\n",
      "step: 214000 | loss: 0.4574\n",
      "step: 214200 | loss: 0.4890\n",
      "step: 214400 | loss: 0.4727\n",
      "step: 214600 | loss: 0.5066\n",
      "step: 214800 | loss: 0.5789\n",
      "step: 215000 | loss: 0.3911\n",
      "step: 215200 | loss: 0.4792\n",
      "step: 215400 | loss: 0.4485\n",
      "step: 215600 | loss: 0.4346\n",
      "step: 215800 | loss: 0.4498\n",
      "step: 216000 | loss: 0.5391\n",
      "step: 216200 | loss: 0.5363\n",
      "step: 216400 | loss: 0.4625\n",
      "step: 216600 | loss: 0.5439\n",
      "step: 216800 | loss: 0.5259\n",
      "step: 217000 | loss: 0.5600\n",
      "step: 217200 | loss: 0.5360\n",
      "step: 217400 | loss: 0.4343\n",
      "step: 217600 | loss: 0.4569\n",
      "step: 217800 | loss: 0.4698\n",
      "step: 218000 | loss: 0.6181\n",
      "step: 218200 | loss: 0.4578\n",
      "step: 218400 | loss: 0.4022\n",
      "step: 218600 | loss: 0.3215\n",
      "step: 218800 | loss: 0.3152\n",
      "step: 219000 | loss: 0.3214\n",
      "step: 219200 | loss: 0.3259\n",
      "step: 219400 | loss: 0.3229\n",
      "step: 219600 | loss: 0.3398\n",
      "step: 219800 | loss: 0.3351\n",
      "step: 220000 | loss: 0.2778\n",
      "step: 220200 | loss: 0.3559\n",
      "step: 220400 | loss: 0.2805\n",
      "step: 220600 | loss: 0.2749\n",
      "step: 220800 | loss: 0.3366\n",
      "step: 221000 | loss: 0.3272\n",
      "step: 221200 | loss: 0.3451\n",
      "step: 221400 | loss: 0.3745\n",
      "step: 221600 | loss: 0.3321\n",
      "step: 221800 | loss: 0.3300\n",
      "step: 222000 | loss: 0.4110\n",
      "step: 222200 | loss: 0.3174\n",
      "step: 222400 | loss: 0.3100\n",
      "step: 222600 | loss: 0.2784\n",
      "step: 222800 | loss: 0.2960\n",
      "step: 223000 | loss: 0.3346\n",
      "step: 223200 | loss: 0.2799\n",
      "step: 223400 | loss: 0.3703\n",
      "step: 223600 | loss: 0.3274\n",
      "step: 223800 | loss: 0.3954\n",
      "step: 224000 | loss: 0.2961\n",
      "step: 224200 | loss: 0.2790\n",
      "step: 224400 | loss: 0.3106\n",
      "step: 224600 | loss: 0.3138\n",
      "step: 224800 | loss: 0.3322\n",
      "step: 225000 | loss: 0.3529\n",
      "step: 225200 | loss: 0.3040\n",
      "step: 225400 | loss: 0.3231\n",
      "step: 225600 | loss: 0.3487\n",
      "step: 225800 | loss: 0.3314\n",
      "step: 226000 | loss: 0.3236\n",
      "step: 226200 | loss: 0.3257\n",
      "step: 226400 | loss: 0.2974\n",
      "step: 226600 | loss: 0.2857\n",
      "step: 226800 | loss: 0.3561\n",
      "step: 227000 | loss: 0.2628\n",
      "step: 227200 | loss: 0.3385\n",
      "step: 227400 | loss: 0.2689\n",
      "step: 227600 | loss: 0.2858\n",
      "step: 227800 | loss: 0.3030\n",
      "step: 228000 | loss: 0.3086\n",
      "step: 228200 | loss: 0.2796\n",
      "step: 228400 | loss: 0.3211\n",
      "step: 228600 | loss: 0.3047\n",
      "step: 228800 | loss: 0.3081\n",
      "step: 229000 | loss: 0.3309\n",
      "step: 229200 | loss: 0.3181\n",
      "step: 229400 | loss: 0.3220\n",
      "step: 229600 | loss: 0.3428\n",
      "step: 229800 | loss: 0.3182\n",
      "step: 230000 | loss: 0.3830\n",
      "step: 230200 | loss: 0.3491\n",
      "step: 230400 | loss: 0.3702\n",
      "step: 230600 | loss: 0.3587\n",
      "step: 230800 | loss: 0.3082\n",
      "step: 231000 | loss: 0.2921\n",
      "step: 231200 | loss: 0.3157\n",
      "step: 231400 | loss: 0.3887\n",
      "step: 231600 | loss: 0.3594\n",
      "step: 231800 | loss: 0.3544\n",
      "step: 232000 | loss: 0.3050\n",
      "step: 232200 | loss: 0.2511\n",
      "step: 232400 | loss: 0.3026\n",
      "step: 232600 | loss: 0.2830\n",
      "step: 232800 | loss: 0.2859\n",
      "step: 233000 | loss: 0.2663\n",
      "step: 233200 | loss: 0.2870\n",
      "step: 233400 | loss: 0.3225\n",
      "step: 233600 | loss: 0.3174\n",
      "step: 233800 | loss: 0.3003\n",
      "step: 234000 | loss: 0.2667\n",
      "step: 234200 | loss: 0.2663\n",
      "step: 234400 | loss: 0.3589\n",
      "step: 234600 | loss: 0.3468\n",
      "step: 234800 | loss: 0.3493\n",
      "step: 235000 | loss: 0.3114\n",
      "step: 235200 | loss: 0.2654\n",
      "step: 235400 | loss: 0.3142\n",
      "step: 235600 | loss: 0.2767\n",
      "step: 235800 | loss: 0.3263\n",
      "step: 236000 | loss: 0.3330\n",
      "step: 236200 | loss: 0.2963\n",
      "step: 236400 | loss: 0.3372\n",
      "step: 236600 | loss: 0.3074\n",
      "step: 236800 | loss: 0.3297\n",
      "step: 237000 | loss: 0.2609\n",
      "step: 237200 | loss: 0.2891\n",
      "step: 237400 | loss: 0.2884\n",
      "step: 237600 | loss: 0.4509\n",
      "step: 237800 | loss: 0.4370\n",
      "step: 238000 | loss: 0.3972\n",
      "step: 238200 | loss: 0.3413\n",
      "step: 238400 | loss: 0.3757\n",
      "step: 238600 | loss: 0.3592\n",
      "step: 238800 | loss: 0.4347\n",
      "step: 239000 | loss: 0.4325\n",
      "step: 239200 | loss: 0.3729\n",
      "step: 239400 | loss: 0.3709\n",
      "step: 239600 | loss: 0.3958\n",
      "step: 239800 | loss: 0.4050\n",
      "step: 240000 | loss: 0.3796\n",
      "step: 240200 | loss: 0.3581\n",
      "step: 240400 | loss: 0.3714\n",
      "step: 240600 | loss: 0.3020\n",
      "step: 240800 | loss: 0.3868\n",
      "step: 241000 | loss: 0.5039\n",
      "step: 241200 | loss: 0.4526\n",
      "step: 241400 | loss: 0.4306\n",
      "step: 241600 | loss: 0.3538\n",
      "step: 241800 | loss: 0.3418\n",
      "step: 242000 | loss: 0.4315\n",
      "step: 242200 | loss: 0.3836\n",
      "step: 242400 | loss: 0.3746\n",
      "step: 242600 | loss: 0.2856\n",
      "step: 242800 | loss: 0.3222\n",
      "step: 243000 | loss: 0.3382\n",
      "step: 243200 | loss: 0.3812\n",
      "step: 243400 | loss: 0.4312\n",
      "step: 243600 | loss: 0.3612\n",
      "step: 243800 | loss: 0.4331\n",
      "step: 244000 | loss: 0.4012\n",
      "step: 244200 | loss: 0.6334\n",
      "step: 244400 | loss: 0.6566\n",
      "step: 244600 | loss: 0.4720\n",
      "step: 244800 | loss: 0.5538\n",
      "step: 245000 | loss: 0.4782\n",
      "step: 245200 | loss: 0.5172\n",
      "step: 245400 | loss: 0.4039\n",
      "step: 245600 | loss: 0.4475\n",
      "step: 245800 | loss: 0.4188\n",
      "step: 246000 | loss: 0.4680\n",
      "step: 246200 | loss: 0.4447\n",
      "step: 246400 | loss: 0.4459\n",
      "step: 246600 | loss: 0.4306\n",
      "step: 246800 | loss: 0.4749\n",
      "step: 247000 | loss: 0.4591\n",
      "step: 247200 | loss: 0.4893\n",
      "step: 247400 | loss: 0.4419\n",
      "step: 247600 | loss: 0.4261\n",
      "step: 247800 | loss: 0.4684\n",
      "step: 248000 | loss: 0.4313\n",
      "step: 248200 | loss: 0.4476\n",
      "step: 248400 | loss: 0.4386\n",
      "step: 248600 | loss: 0.4378\n",
      "step: 248800 | loss: 0.5173\n",
      "step: 249000 | loss: 0.4873\n",
      "step: 249200 | loss: 0.4678\n",
      "step: 249400 | loss: 0.5829\n",
      "step: 249600 | loss: 0.4620\n",
      "step: 249800 | loss: 0.5181\n",
      "step: 250000 | loss: 0.5373\n",
      "step: 250200 | loss: 0.4260\n",
      "step: 250400 | loss: 0.4360\n",
      "step: 250600 | loss: 0.4222\n",
      "step: 250800 | loss: 0.4714\n",
      "step: 251000 | loss: 0.4396\n",
      "step: 251200 | loss: 0.4361\n",
      "step: 251400 | loss: 0.4732\n",
      "step: 251600 | loss: 0.4944\n",
      "step: 251800 | loss: 0.4987\n",
      "step: 252000 | loss: 0.4756\n",
      "step: 252200 | loss: 0.4764\n",
      "step: 252400 | loss: 0.6152\n",
      "step: 252600 | loss: 0.5169\n",
      "step: 252800 | loss: 0.5676\n",
      "step: 253000 | loss: 0.4497\n",
      "step: 253200 | loss: 0.4676\n",
      "step: 253400 | loss: 0.4257\n",
      "step: 253600 | loss: 0.5110\n",
      "step: 253800 | loss: 0.5283\n",
      "step: 254000 | loss: 0.6395\n",
      "step: 254200 | loss: 0.7187\n",
      "step: 254400 | loss: 0.6318\n",
      "step: 254600 | loss: 0.6384\n",
      "step: 254800 | loss: 0.6363\n",
      "step: 255000 | loss: 0.5971\n",
      "step: 255200 | loss: 0.5013\n",
      "step: 255400 | loss: 0.6280\n",
      "step: 255600 | loss: 0.5511\n",
      "step: 255800 | loss: 0.4806\n",
      "step: 256000 | loss: 0.4949\n",
      "step: 256200 | loss: 0.4779\n",
      "step: 256400 | loss: 0.4724\n",
      "step: 256600 | loss: 0.4626\n",
      "step: 256800 | loss: 0.4464\n",
      "step: 257000 | loss: 0.4730\n",
      "step: 257200 | loss: 0.5280\n",
      "step: 257400 | loss: 0.5146\n",
      "step: 257600 | loss: 0.5047\n",
      "step: 257800 | loss: 0.5409\n",
      "step: 258000 | loss: 0.4017\n",
      "step: 258200 | loss: 0.4648\n",
      "step: 258400 | loss: 0.5476\n",
      "step: 258600 | loss: 0.4291\n",
      "step: 258800 | loss: 0.5445\n",
      "step: 259000 | loss: 0.4490\n",
      "step: 259200 | loss: 0.5243\n",
      "step: 259400 | loss: 0.4619\n",
      "step: 259600 | loss: 0.3947\n",
      "step: 259800 | loss: 0.4688\n",
      "step: 260000 | loss: 0.4119\n",
      "step: 260200 | loss: 0.4675\n",
      "step: 260400 | loss: 0.5099\n",
      "step: 260600 | loss: 0.4735\n",
      "step: 260800 | loss: 0.4337\n",
      "step: 261000 | loss: 0.4275\n",
      "step: 261200 | loss: 0.4533\n",
      "step: 261400 | loss: 0.4146\n",
      "step: 261600 | loss: 0.4856\n",
      "step: 261800 | loss: 0.3882\n",
      "step: 262000 | loss: 0.4410\n",
      "step: 262200 | loss: 0.4884\n",
      "step: 262400 | loss: 0.4786\n",
      "step: 262600 | loss: 0.3590\n",
      "step: 262800 | loss: 0.4654\n",
      "step: 263000 | loss: 0.4666\n",
      "step: 263200 | loss: 0.5077\n",
      "step: 263400 | loss: 0.3958\n",
      "step: 263600 | loss: 0.4627\n",
      "step: 263800 | loss: 0.6201\n",
      "step: 264000 | loss: 0.6498\n",
      "step: 264200 | loss: 0.6458\n",
      "step: 264400 | loss: 0.6088\n",
      "step: 264600 | loss: 0.5210\n",
      "step: 264800 | loss: 0.4317\n",
      "step: 265000 | loss: 0.5164\n",
      "step: 265200 | loss: 0.5114\n",
      "step: 265400 | loss: 0.5018\n",
      "step: 265600 | loss: 0.4749\n",
      "step: 265800 | loss: 0.4810\n",
      "step: 266000 | loss: 0.5008\n",
      "step: 266200 | loss: 0.4730\n",
      "step: 266400 | loss: 0.5255\n",
      "step: 266600 | loss: 0.4883\n",
      "step: 266800 | loss: 0.4219\n",
      "step: 267000 | loss: 0.5592\n",
      "step: 267200 | loss: 0.4517\n",
      "step: 267400 | loss: 0.4321\n",
      "step: 267600 | loss: 0.4351\n",
      "step: 267800 | loss: 0.3865\n",
      "step: 268000 | loss: 0.3271\n",
      "step: 268200 | loss: 0.3755\n",
      "step: 268400 | loss: 0.4953\n",
      "step: 268600 | loss: 0.4996\n",
      "step: 268800 | loss: 0.4740\n",
      "step: 269000 | loss: 0.4326\n",
      "step: 269200 | loss: 0.3515\n",
      "step: 269400 | loss: 0.3835\n",
      "step: 269600 | loss: 0.3710\n",
      "step: 269800 | loss: 0.4190\n",
      "step: 270000 | loss: 0.3524\n",
      "step: 270200 | loss: 0.3333\n",
      "step: 270400 | loss: 0.3534\n",
      "step: 270600 | loss: 0.3965\n",
      "step: 270800 | loss: 0.4206\n",
      "step: 271000 | loss: 0.3918\n",
      "step: 271200 | loss: 0.3266\n",
      "step: 271400 | loss: 0.3815\n",
      "step: 271600 | loss: 0.3272\n",
      "step: 271800 | loss: 0.3517\n",
      "step: 272000 | loss: 0.4165\n",
      "step: 272200 | loss: 0.3356\n",
      "step: 272400 | loss: 0.3548\n",
      "step: 272600 | loss: 0.3015\n",
      "step: 272800 | loss: 0.3526\n",
      "step: 273000 | loss: 0.3919\n",
      "step: 273200 | loss: 0.3294\n",
      "step: 273400 | loss: 0.4338\n",
      "step: 273600 | loss: 0.4790\n",
      "step: 273800 | loss: 0.3968\n",
      "step: 274000 | loss: 0.3860\n",
      "step: 274200 | loss: 0.3388\n",
      "step: 274400 | loss: 0.3847\n",
      "step: 274600 | loss: 0.3470\n",
      "step: 274800 | loss: 0.3264\n",
      "step: 275000 | loss: 0.3356\n",
      "step: 275200 | loss: 0.3205\n",
      "step: 275400 | loss: 0.3090\n",
      "step: 275600 | loss: 0.2783\n",
      "step: 275800 | loss: 0.3010\n",
      "step: 276000 | loss: 0.3800\n",
      "step: 276200 | loss: 0.3552\n",
      "step: 276400 | loss: 0.3156\n",
      "step: 276600 | loss: 0.3329\n",
      "step: 276800 | loss: 0.3300\n",
      "step: 277000 | loss: 0.2711\n",
      "step: 277200 | loss: 0.3860\n",
      "step: 277400 | loss: 0.3880\n",
      "step: 277600 | loss: 0.3030\n",
      "step: 277800 | loss: 0.3197\n",
      "step: 278000 | loss: 0.2839\n",
      "step: 278200 | loss: 0.3241\n",
      "step: 278400 | loss: 0.3444\n",
      "step: 278600 | loss: 0.3225\n",
      "step: 278800 | loss: 0.3630\n",
      "step: 279000 | loss: 0.2792\n",
      "step: 279200 | loss: 0.3595\n",
      "step: 279400 | loss: 0.2661\n",
      "step: 279600 | loss: 0.3791\n",
      "step: 279800 | loss: 0.3182\n",
      "step: 280000 | loss: 0.2934\n",
      "step: 280200 | loss: 0.4060\n",
      "step: 280400 | loss: 0.4035\n",
      "step: 280600 | loss: 0.3420\n",
      "step: 280800 | loss: 0.3436\n",
      "step: 281000 | loss: 0.3196\n",
      "step: 281200 | loss: 0.3469\n",
      "step: 281400 | loss: 0.4320\n",
      "step: 281600 | loss: 0.4211\n",
      "step: 281800 | loss: 0.3363\n",
      "step: 282000 | loss: 0.3837\n",
      "step: 282200 | loss: 0.3822\n",
      "step: 282400 | loss: 0.3962\n",
      "step: 282600 | loss: 0.3357\n",
      "step: 282800 | loss: 0.3467\n",
      "step: 283000 | loss: 0.3338\n",
      "step: 283200 | loss: 0.2894\n",
      "step: 283400 | loss: 0.5129\n",
      "step: 283600 | loss: 0.5642\n",
      "step: 283800 | loss: 0.4310\n",
      "step: 284000 | loss: 0.4278\n",
      "step: 284200 | loss: 0.4768\n",
      "step: 284400 | loss: 0.4578\n",
      "step: 284600 | loss: 0.4441\n",
      "step: 284800 | loss: 0.3897\n",
      "step: 285000 | loss: 0.4344\n",
      "step: 285200 | loss: 0.3875\n",
      "step: 285400 | loss: 0.3926\n",
      "step: 285600 | loss: 0.3495\n",
      "step: 285800 | loss: 0.3802\n",
      "step: 286000 | loss: 0.4497\n",
      "step: 286200 | loss: 0.3745\n",
      "step: 286400 | loss: 0.4477\n",
      "step: 286600 | loss: 0.3893\n",
      "step: 286800 | loss: 0.4234\n",
      "step: 287000 | loss: 0.4589\n",
      "step: 287200 | loss: 0.4467\n",
      "step: 287400 | loss: 0.4403\n",
      "step: 287600 | loss: 0.5353\n",
      "step: 287800 | loss: 0.5593\n",
      "step: 288000 | loss: 0.5261\n",
      "step: 288200 | loss: 0.4676\n",
      "step: 288400 | loss: 0.4681\n",
      "step: 288600 | loss: 0.4715\n",
      "step: 288800 | loss: 0.4839\n",
      "step: 289000 | loss: 0.4801\n",
      "step: 289200 | loss: 0.5239\n",
      "step: 289400 | loss: 0.4502\n",
      "step: 289600 | loss: 0.5965\n",
      "step: 289800 | loss: 0.5557\n",
      "step: 290000 | loss: 0.5753\n",
      "step: 290200 | loss: 0.5642\n",
      "step: 290400 | loss: 0.6502\n",
      "step: 290600 | loss: 0.5737\n",
      "step: 290800 | loss: 0.7276\n",
      "step: 291000 | loss: 0.6205\n",
      "step: 291200 | loss: 0.6357\n",
      "step: 291400 | loss: 0.5869\n",
      "step: 291600 | loss: 0.5897\n",
      "step: 291800 | loss: 0.6050\n",
      "step: 292000 | loss: 0.6268\n",
      "step: 292200 | loss: 0.5118\n",
      "step: 292400 | loss: 0.6770\n",
      "step: 292600 | loss: 0.6623\n",
      "step: 292800 | loss: 0.6273\n",
      "step: 293000 | loss: 0.6284\n",
      "step: 293200 | loss: 0.6286\n",
      "step: 293400 | loss: 0.7672\n",
      "step: 293600 | loss: 0.5613\n",
      "step: 293800 | loss: 0.5166\n",
      "step: 294000 | loss: 0.4604\n",
      "step: 294200 | loss: 0.4091\n",
      "step: 294400 | loss: 0.4388\n",
      "step: 294600 | loss: 0.3625\n",
      "step: 294800 | loss: 0.3446\n",
      "step: 295000 | loss: 0.3180\n",
      "step: 295200 | loss: 0.3946\n",
      "step: 295400 | loss: 0.4468\n",
      "step: 295600 | loss: 0.4163\n",
      "step: 295800 | loss: 0.4327\n",
      "step: 296000 | loss: 0.4584\n",
      "step: 296200 | loss: 0.4830\n",
      "step: 296400 | loss: 0.4428\n",
      "step: 296600 | loss: 0.4103\n",
      "step: 296800 | loss: 0.4791\n",
      "step: 297000 | loss: 0.5118\n",
      "step: 297200 | loss: 0.4289\n",
      "step: 297400 | loss: 0.5105\n",
      "step: 297600 | loss: 0.4131\n",
      "step: 297800 | loss: 0.4549\n",
      "step: 298000 | loss: 0.4292\n",
      "step: 298200 | loss: 0.4695\n",
      "step: 298400 | loss: 0.4772\n",
      "step: 298600 | loss: 0.4394\n",
      "step: 298800 | loss: 0.4543\n",
      "step: 299000 | loss: 0.4945\n",
      "step: 299200 | loss: 0.4929\n",
      "step: 299400 | loss: 0.4048\n",
      "step: 299600 | loss: 0.5145\n",
      "step: 299800 | loss: 0.5866\n",
      "step: 300000 | loss: 0.5388\n",
      "step: 300200 | loss: 0.5236\n",
      "step: 300400 | loss: 0.4996\n",
      "step: 300600 | loss: 0.3964\n",
      "step: 300800 | loss: 0.5026\n",
      "step: 301000 | loss: 0.4690\n",
      "step: 301200 | loss: 0.4831\n",
      "step: 301400 | loss: 0.4996\n",
      "step: 301600 | loss: 0.5757\n",
      "step: 301800 | loss: 0.4130\n",
      "step: 302000 | loss: 0.4358\n",
      "step: 302200 | loss: 0.4417\n",
      "step: 302400 | loss: 0.4600\n",
      "step: 302600 | loss: 0.4299\n",
      "step: 302800 | loss: 0.3651\n",
      "step: 303000 | loss: 0.4524\n",
      "step: 303200 | loss: 0.5691\n",
      "step: 303400 | loss: 0.4899\n",
      "step: 303600 | loss: 0.3749\n",
      "step: 303800 | loss: 0.3863\n",
      "step: 304000 | loss: 0.3108\n",
      "step: 304200 | loss: 0.4026\n",
      "step: 304400 | loss: 0.3193\n",
      "step: 304600 | loss: 0.3573\n",
      "step: 304800 | loss: 0.3605\n",
      "step: 305000 | loss: 0.3156\n",
      "step: 305200 | loss: 0.3898\n",
      "step: 305400 | loss: 0.3411\n",
      "step: 305600 | loss: 0.3291\n",
      "step: 305800 | loss: 0.3223\n",
      "step: 306000 | loss: 0.3362\n",
      "step: 306200 | loss: 0.2856\n",
      "step: 306400 | loss: 0.3676\n",
      "step: 306600 | loss: 0.3467\n",
      "step: 306800 | loss: 0.3004\n",
      "step: 307000 | loss: 0.2776\n",
      "step: 307200 | loss: 0.3003\n",
      "step: 307400 | loss: 0.2954\n",
      "step: 307600 | loss: 0.2919\n",
      "step: 307800 | loss: 0.3349\n",
      "step: 308000 | loss: 0.3206\n",
      "step: 308200 | loss: 0.2668\n",
      "step: 308400 | loss: 0.2856\n",
      "step: 308600 | loss: 0.3299\n",
      "step: 308800 | loss: 0.3046\n",
      "step: 309000 | loss: 0.2933\n",
      "step: 309200 | loss: 0.2797\n",
      "step: 309400 | loss: 0.3201\n",
      "step: 309600 | loss: 0.3774\n",
      "step: 309800 | loss: 0.4763\n",
      "step: 310000 | loss: 0.5650\n",
      "step: 310200 | loss: 0.4724\n",
      "step: 310400 | loss: 0.4542\n",
      "step: 310600 | loss: 0.4472\n",
      "step: 310800 | loss: 0.4438\n",
      "step: 311000 | loss: 0.5864\n",
      "step: 311200 | loss: 0.4358\n",
      "step: 311400 | loss: 0.3818\n",
      "step: 311600 | loss: 0.4068\n",
      "step: 311800 | loss: 0.3631\n",
      "step: 312000 | loss: 0.4054\n",
      "step: 312200 | loss: 0.3706\n",
      "step: 312400 | loss: 0.3669\n",
      "step: 312600 | loss: 0.4628\n",
      "step: 312800 | loss: 0.4006\n",
      "step: 313000 | loss: 0.4247\n",
      "step: 313200 | loss: 0.3545\n",
      "step: 313400 | loss: 0.4251\n",
      "step: 313600 | loss: 0.3987\n",
      "step: 313800 | loss: 0.3980\n",
      "step: 314000 | loss: 0.3879\n",
      "step: 314200 | loss: 0.3985\n",
      "step: 314400 | loss: 0.2924\n",
      "step: 314600 | loss: 0.3366\n",
      "step: 314800 | loss: 0.3468\n",
      "step: 315000 | loss: 0.4477\n",
      "step: 315200 | loss: 0.3680\n",
      "step: 315400 | loss: 0.3245\n",
      "step: 315600 | loss: 0.3456\n",
      "step: 315800 | loss: 0.2910\n",
      "step: 316000 | loss: 0.2921\n",
      "step: 316200 | loss: 0.4979\n",
      "step: 316400 | loss: 0.6227\n",
      "step: 316600 | loss: 0.5597\n",
      "step: 316800 | loss: 0.4993\n",
      "step: 317000 | loss: 0.5877\n",
      "step: 317200 | loss: 0.5497\n",
      "step: 317400 | loss: 0.4234\n",
      "step: 317600 | loss: 0.5137\n",
      "step: 317800 | loss: 0.4375\n",
      "step: 318000 | loss: 0.5746\n",
      "step: 318200 | loss: 0.4937\n",
      "step: 318400 | loss: 0.5746\n",
      "step: 318600 | loss: 0.4356\n",
      "step: 318800 | loss: 0.5119\n",
      "step: 319000 | loss: 0.5194\n",
      "step: 319200 | loss: 0.4768\n",
      "step: 319400 | loss: 0.4430\n",
      "step: 319600 | loss: 0.4789\n",
      "step: 319800 | loss: 0.4415\n",
      "step: 320000 | loss: 0.5253\n",
      "step: 320200 | loss: 0.5249\n",
      "step: 320400 | loss: 0.5337\n",
      "step: 320600 | loss: 0.5337\n",
      "step: 320800 | loss: 0.5019\n",
      "step: 321000 | loss: 0.5299\n",
      "step: 321200 | loss: 0.4600\n",
      "step: 321400 | loss: 0.4917\n",
      "step: 321600 | loss: 0.3962\n",
      "step: 321800 | loss: 0.4961\n",
      "step: 322000 | loss: 0.4391\n",
      "step: 322200 | loss: 0.3777\n",
      "step: 322400 | loss: 0.4540\n",
      "step: 322600 | loss: 0.4489\n",
      "step: 322800 | loss: 0.4422\n",
      "step: 323000 | loss: 0.4627\n",
      "step: 323200 | loss: 0.4252\n",
      "step: 323400 | loss: 0.4175\n",
      "step: 323600 | loss: 0.3996\n",
      "step: 323800 | loss: 0.3714\n",
      "step: 324000 | loss: 0.4947\n",
      "step: 324200 | loss: 0.4423\n",
      "step: 324400 | loss: 0.5149\n",
      "step: 324600 | loss: 0.3634\n",
      "step: 324800 | loss: 0.4526\n",
      "step: 325000 | loss: 0.5663\n",
      "step: 325200 | loss: 0.4612\n",
      "step: 325400 | loss: 0.5098\n",
      "step: 325600 | loss: 0.4688\n",
      "step: 325800 | loss: 0.5115\n",
      "step: 326000 | loss: 0.4930\n",
      "step: 326200 | loss: 0.5071\n",
      "step: 326400 | loss: 0.5457\n",
      "step: 326600 | loss: 0.5490\n",
      "step: 326800 | loss: 0.5545\n",
      "step: 327000 | loss: 0.5648\n",
      "step: 327200 | loss: 0.5265\n",
      "step: 327400 | loss: 0.5230\n",
      "step: 327600 | loss: 0.5480\n",
      "step: 327800 | loss: 0.4724\n",
      "step: 328000 | loss: 0.5435\n",
      "step: 328200 | loss: 0.5187\n",
      "step: 328400 | loss: 0.5561\n",
      "step: 328600 | loss: 0.4775\n",
      "step: 328800 | loss: 0.5148\n",
      "step: 329000 | loss: 0.5610\n",
      "step: 329200 | loss: 0.6091\n",
      "step: 329400 | loss: 0.5360\n",
      "step: 329600 | loss: 0.5817\n",
      "step: 329800 | loss: 0.5700\n",
      "step: 330000 | loss: 0.4512\n",
      "step: 330200 | loss: 0.5836\n",
      "step: 330400 | loss: 0.4874\n",
      "step: 330600 | loss: 0.5342\n",
      "step: 330800 | loss: 0.5408\n",
      "step: 331000 | loss: 0.4215\n",
      "step: 331200 | loss: 0.4190\n",
      "step: 331400 | loss: 0.5320\n",
      "step: 331600 | loss: 0.5110\n",
      "step: 331800 | loss: 0.6201\n",
      "step: 332000 | loss: 0.5213\n",
      "step: 332200 | loss: 0.6250\n",
      "step: 332400 | loss: 0.5883\n",
      "step: 332600 | loss: 0.6189\n",
      "step: 332800 | loss: 0.5746\n",
      "step: 333000 | loss: 0.4701\n",
      "step: 333200 | loss: 0.4773\n",
      "step: 333400 | loss: 0.5643\n",
      "step: 333600 | loss: 0.6350\n",
      "step: 333800 | loss: 0.6109\n",
      "step: 334000 | loss: 0.5953\n",
      "step: 334200 | loss: 0.5769\n",
      "step: 334400 | loss: 0.6562\n",
      "step: 334600 | loss: 0.5138\n",
      "step: 334800 | loss: 0.6144\n",
      "step: 335000 | loss: 0.5468\n",
      "step: 335200 | loss: 0.5772\n",
      "step: 335400 | loss: 0.5459\n",
      "step: 335600 | loss: 0.6017\n",
      "step: 335800 | loss: 0.5357\n",
      "step: 336000 | loss: 0.5910\n",
      "step: 336200 | loss: 0.6728\n",
      "step: 336400 | loss: 0.6864\n",
      "step: 336600 | loss: 0.5540\n",
      "step: 336800 | loss: 0.6178\n",
      "step: 337000 | loss: 0.5540\n",
      "step: 337200 | loss: 0.6670\n",
      "step: 337400 | loss: 0.5204\n",
      "step: 337600 | loss: 0.5636\n",
      "step: 337800 | loss: 0.6087\n",
      "step: 338000 | loss: 0.6201\n",
      "step: 338200 | loss: 0.4397\n",
      "step: 338400 | loss: 0.5565\n",
      "step: 338600 | loss: 0.6250\n",
      "step: 338800 | loss: 0.5542\n",
      "step: 339000 | loss: 0.5147\n",
      "step: 339200 | loss: 0.6884\n",
      "step: 339400 | loss: 0.6618\n",
      "step: 339600 | loss: 0.6241\n",
      "step: 339800 | loss: 0.5378\n",
      "step: 340000 | loss: 0.6828\n",
      "step: 340200 | loss: 0.6975\n",
      "step: 340400 | loss: 0.7580\n",
      "step: 340600 | loss: 0.6231\n",
      "step: 340800 | loss: 0.7282\n",
      "step: 341000 | loss: 0.6464\n",
      "step: 341200 | loss: 0.6480\n",
      "step: 341400 | loss: 0.5561\n",
      "step: 341600 | loss: 0.6271\n",
      "step: 341800 | loss: 0.5333\n",
      "step: 342000 | loss: 0.6042\n",
      "step: 342200 | loss: 0.6164\n",
      "step: 342400 | loss: 0.7727\n",
      "step: 342600 | loss: 0.6597\n",
      "step: 342800 | loss: 0.6273\n",
      "step: 343000 | loss: 0.4315\n",
      "step: 343200 | loss: 0.4573\n",
      "step: 343400 | loss: 0.4701\n",
      "step: 343600 | loss: 0.3891\n",
      "step: 343800 | loss: 0.3852\n",
      "step: 344000 | loss: 0.4866\n",
      "step: 344200 | loss: 0.3802\n",
      "step: 344400 | loss: 0.4803\n",
      "step: 344600 | loss: 0.5536\n",
      "step: 344800 | loss: 0.4391\n",
      "step: 345000 | loss: 0.4161\n",
      "step: 345200 | loss: 0.4358\n",
      "step: 345400 | loss: 0.4805\n",
      "step: 345600 | loss: 0.4738\n",
      "step: 345800 | loss: 0.4795\n",
      "step: 346000 | loss: 0.3539\n",
      "step: 346200 | loss: 0.3798\n",
      "step: 346400 | loss: 0.3195\n",
      "step: 346600 | loss: 0.3540\n",
      "step: 346800 | loss: 0.4336\n",
      "step: 347000 | loss: 0.4102\n",
      "step: 347200 | loss: 0.3959\n",
      "step: 347400 | loss: 0.3568\n",
      "step: 347600 | loss: 0.3153\n",
      "step: 347800 | loss: 0.3855\n",
      "step: 348000 | loss: 0.4013\n",
      "step: 348200 | loss: 0.3485\n",
      "step: 348400 | loss: 0.3650\n",
      "step: 348600 | loss: 0.3541\n",
      "step: 348800 | loss: 0.3806\n",
      "step: 349000 | loss: 0.4193\n",
      "step: 349200 | loss: 0.3503\n",
      "step: 349400 | loss: 0.4082\n",
      "step: 349600 | loss: 0.3579\n",
      "step: 349800 | loss: 0.4798\n",
      "step: 350000 | loss: 0.4610\n",
      "step: 350200 | loss: 0.4754\n",
      "step: 350400 | loss: 0.3959\n",
      "step: 350600 | loss: 0.4176\n",
      "step: 350800 | loss: 0.3684\n",
      "step: 351000 | loss: 0.3917\n",
      "step: 351200 | loss: 0.4204\n",
      "step: 351400 | loss: 0.3539\n",
      "step: 351600 | loss: 0.5001\n",
      "step: 351800 | loss: 0.3969\n",
      "step: 352000 | loss: 0.3406\n",
      "step: 352200 | loss: 0.4054\n",
      "step: 352400 | loss: 0.4760\n",
      "step: 352600 | loss: 0.4732\n",
      "step: 352800 | loss: 0.4159\n",
      "step: 353000 | loss: 0.4488\n",
      "step: 353200 | loss: 0.4501\n",
      "step: 353400 | loss: 0.3508\n",
      "step: 353600 | loss: 0.3057\n",
      "step: 353800 | loss: 0.4016\n",
      "step: 354000 | loss: 0.3322\n",
      "step: 354200 | loss: 0.3554\n",
      "step: 354400 | loss: 0.4175\n",
      "step: 354600 | loss: 0.3558\n",
      "step: 354800 | loss: 0.4203\n",
      "step: 355000 | loss: 0.3486\n",
      "step: 355200 | loss: 0.3966\n",
      "step: 355400 | loss: 0.3632\n",
      "step: 355600 | loss: 0.3750\n",
      "step: 355800 | loss: 0.3628\n",
      "step: 356000 | loss: 0.3502\n",
      "step: 356200 | loss: 0.4701\n",
      "step: 356400 | loss: 0.3101\n",
      "step: 356600 | loss: 0.3257\n",
      "step: 356800 | loss: 0.4391\n",
      "step: 357000 | loss: 0.3608\n",
      "step: 357200 | loss: 0.3647\n",
      "step: 357400 | loss: 0.3719\n",
      "step: 357600 | loss: 0.3948\n",
      "step: 357800 | loss: 0.4175\n",
      "step: 358000 | loss: 0.3798\n",
      "step: 358200 | loss: 0.3260\n",
      "step: 358400 | loss: 0.3158\n",
      "step: 358600 | loss: 0.3175\n",
      "step: 358800 | loss: 0.3257\n",
      "step: 359000 | loss: 0.3420\n",
      "step: 359200 | loss: 0.3026\n",
      "step: 359400 | loss: 0.3417\n",
      "step: 359600 | loss: 0.2910\n",
      "step: 359800 | loss: 0.2747\n",
      "step: 360000 | loss: 0.3379\n",
      "step: 360200 | loss: 0.3029\n",
      "step: 360400 | loss: 0.2655\n",
      "step: 360600 | loss: 0.3023\n",
      "step: 360800 | loss: 0.3112\n",
      "step: 361000 | loss: 0.3425\n",
      "step: 361200 | loss: 0.2446\n",
      "step: 361400 | loss: 0.3283\n",
      "step: 361600 | loss: 0.3045\n",
      "step: 361800 | loss: 0.2920\n",
      "step: 362000 | loss: 0.3690\n",
      "step: 362200 | loss: 0.3505\n",
      "step: 362400 | loss: 0.3392\n",
      "step: 362600 | loss: 0.3238\n",
      "step: 362800 | loss: 0.3125\n",
      "step: 363000 | loss: 0.3375\n",
      "step: 363200 | loss: 0.3263\n",
      "step: 363400 | loss: 0.3109\n",
      "step: 363600 | loss: 0.3153\n",
      "step: 363800 | loss: 0.3230\n",
      "step: 364000 | loss: 0.3723\n",
      "step: 364200 | loss: 0.4163\n",
      "step: 364400 | loss: 0.3875\n",
      "step: 364600 | loss: 0.3841\n",
      "step: 364800 | loss: 0.3391\n",
      "step: 365000 | loss: 0.3919\n",
      "step: 365200 | loss: 0.3442\n",
      "step: 365400 | loss: 0.4490\n",
      "step: 365600 | loss: 0.4480\n",
      "step: 365800 | loss: 0.4222\n",
      "step: 366000 | loss: 0.4876\n",
      "step: 366200 | loss: 0.4028\n",
      "step: 366400 | loss: 0.4212\n",
      "step: 366600 | loss: 0.4256\n",
      "step: 366800 | loss: 0.4410\n",
      "step: 367000 | loss: 0.4745\n",
      "step: 367200 | loss: 0.4726\n",
      "step: 367400 | loss: 0.4733\n",
      "step: 367600 | loss: 0.4452\n",
      "step: 367800 | loss: 0.4392\n",
      "step: 368000 | loss: 0.4174\n",
      "step: 368200 | loss: 0.3797\n",
      "step: 368400 | loss: 0.3723\n",
      "step: 368600 | loss: 0.4448\n",
      "step: 368800 | loss: 0.4790\n",
      "step: 369000 | loss: 0.4478\n",
      "step: 369200 | loss: 0.4966\n",
      "step: 369400 | loss: 0.4116\n",
      "step: 369600 | loss: 0.4434\n",
      "step: 369800 | loss: 0.5109\n",
      "step: 370000 | loss: 0.4321\n",
      "step: 370200 | loss: 0.4706\n",
      "step: 370400 | loss: 0.5063\n",
      "step: 370600 | loss: 0.5372\n",
      "step: 370800 | loss: 0.4722\n",
      "step: 371000 | loss: 0.4364\n",
      "step: 371200 | loss: 0.5021\n",
      "step: 371400 | loss: 0.4746\n",
      "step: 371600 | loss: 0.5570\n",
      "step: 371800 | loss: 0.4691\n",
      "step: 372000 | loss: 0.4552\n",
      "step: 372200 | loss: 0.4878\n",
      "step: 372400 | loss: 0.5461\n",
      "step: 372600 | loss: 0.5044\n",
      "step: 372800 | loss: 0.5694\n",
      "step: 373000 | loss: 0.4760\n",
      "step: 373200 | loss: 0.4019\n",
      "step: 373400 | loss: 0.4734\n",
      "step: 373600 | loss: 0.4465\n",
      "step: 373800 | loss: 0.5486\n",
      "step: 374000 | loss: 0.4476\n",
      "step: 374200 | loss: 0.4514\n",
      "step: 374400 | loss: 0.3753\n",
      "step: 374600 | loss: 0.4394\n",
      "step: 374800 | loss: 0.4140\n",
      "step: 375000 | loss: 0.4756\n",
      "step: 375200 | loss: 0.5991\n",
      "step: 375400 | loss: 0.7279\n",
      "step: 375600 | loss: 0.5129\n",
      "step: 375800 | loss: 0.4237\n",
      "step: 376000 | loss: 0.5728\n",
      "step: 376200 | loss: 0.4655\n",
      "step: 376400 | loss: 0.5049\n",
      "step: 376600 | loss: 0.5061\n",
      "step: 376800 | loss: 0.4373\n",
      "step: 377000 | loss: 0.4750\n",
      "step: 377200 | loss: 0.4467\n",
      "step: 377400 | loss: 0.4114\n",
      "step: 377600 | loss: 0.4514\n",
      "step: 377800 | loss: 0.3403\n",
      "step: 378000 | loss: 0.3777\n",
      "step: 378200 | loss: 0.4108\n",
      "step: 378400 | loss: 0.4591\n",
      "step: 378600 | loss: 0.4485\n",
      "step: 378800 | loss: 0.4326\n",
      "step: 379000 | loss: 0.4815\n",
      "step: 379200 | loss: 0.4277\n",
      "step: 379400 | loss: 0.3019\n",
      "step: 379600 | loss: 0.5394\n",
      "step: 379800 | loss: 0.4283\n",
      "step: 380000 | loss: 0.3886\n",
      "step: 380200 | loss: 0.3718\n",
      "step: 380400 | loss: 0.3880\n",
      "step: 380600 | loss: 0.4435\n",
      "step: 380800 | loss: 0.4299\n",
      "step: 381000 | loss: 0.3359\n",
      "step: 381200 | loss: 0.3701\n",
      "step: 381400 | loss: 0.3542\n",
      "step: 381600 | loss: 0.3323\n",
      "step: 381800 | loss: 0.5371\n",
      "step: 382000 | loss: 0.4976\n",
      "step: 382200 | loss: 0.3652\n",
      "step: 382400 | loss: 0.3806\n",
      "step: 382600 | loss: 0.3704\n",
      "step: 382800 | loss: 0.2691\n",
      "step: 383000 | loss: 0.4167\n",
      "step: 383200 | loss: 0.4684\n",
      "step: 383400 | loss: 0.4275\n",
      "step: 383600 | loss: 0.3638\n",
      "step: 383800 | loss: 0.3997\n",
      "step: 384000 | loss: 0.4073\n",
      "step: 384200 | loss: 0.4362\n",
      "step: 384400 | loss: 0.3522\n",
      "step: 384600 | loss: 0.3345\n",
      "step: 384800 | loss: 0.3799\n",
      "step: 385000 | loss: 0.3579\n",
      "step: 385200 | loss: 0.3922\n",
      "step: 385400 | loss: 0.3466\n",
      "step: 385600 | loss: 0.3776\n",
      "step: 385800 | loss: 0.3592\n",
      "step: 386000 | loss: 0.3785\n",
      "step: 386200 | loss: 0.2882\n",
      "step: 386400 | loss: 0.3116\n",
      "step: 386600 | loss: 0.3316\n",
      "step: 386800 | loss: 0.3576\n",
      "step: 387000 | loss: 0.4400\n",
      "step: 387200 | loss: 0.3859\n",
      "step: 387400 | loss: 0.3685\n",
      "step: 387600 | loss: 0.4144\n",
      "step: 387800 | loss: 0.4077\n",
      "step: 388000 | loss: 0.4062\n",
      "step: 388200 | loss: 0.3965\n",
      "step: 388400 | loss: 0.4902\n",
      "step: 388600 | loss: 0.3945\n",
      "step: 388800 | loss: 0.4454\n",
      "step: 389000 | loss: 0.3927\n",
      "step: 389200 | loss: 0.4363\n",
      "step: 389400 | loss: 0.4045\n",
      "step: 389600 | loss: 0.4496\n",
      "step: 389800 | loss: 0.4291\n",
      "step: 390000 | loss: 0.3510\n",
      "step: 390200 | loss: 0.3293\n",
      "step: 390400 | loss: 0.4009\n",
      "step: 390600 | loss: 0.3249\n",
      "step: 390800 | loss: 0.3930\n",
      "step: 391000 | loss: 0.4379\n",
      "step: 391200 | loss: 0.3811\n",
      "step: 391400 | loss: 0.4224\n",
      "step: 391600 | loss: 0.6259\n",
      "step: 391800 | loss: 0.5509\n",
      "step: 392000 | loss: 0.6617\n",
      "step: 392200 | loss: 0.4908\n",
      "step: 392400 | loss: 0.5979\n",
      "step: 392600 | loss: 0.5925\n",
      "step: 392800 | loss: 0.5622\n",
      "step: 393000 | loss: 0.4259\n",
      "step: 393200 | loss: 0.5066\n",
      "step: 393400 | loss: 0.5599\n",
      "step: 393600 | loss: 0.5096\n",
      "step: 393800 | loss: 0.4966\n",
      "step: 394000 | loss: 0.4796\n",
      "step: 394200 | loss: 0.4346\n",
      "step: 394400 | loss: 0.4574\n",
      "step: 394600 | loss: 0.3635\n",
      "step: 394800 | loss: 0.4752\n",
      "step: 395000 | loss: 0.4435\n",
      "step: 395200 | loss: 0.4844\n",
      "step: 395400 | loss: 0.4433\n",
      "step: 395600 | loss: 0.5307\n",
      "step: 395800 | loss: 0.4336\n",
      "step: 396000 | loss: 0.4811\n",
      "step: 396200 | loss: 0.4679\n",
      "step: 396400 | loss: 0.4460\n",
      "step: 396600 | loss: 0.5181\n",
      "step: 396800 | loss: 0.4702\n",
      "step: 397000 | loss: 0.4479\n",
      "step: 397200 | loss: 0.4546\n",
      "step: 397400 | loss: 0.5403\n",
      "step: 397600 | loss: 0.5041\n",
      "step: 397800 | loss: 0.4356\n",
      "step: 398000 | loss: 0.4547\n",
      "step: 398200 | loss: 0.4798\n",
      "step: 398400 | loss: 0.4812\n",
      "step: 398600 | loss: 0.4670\n",
      "step: 398800 | loss: 0.4736\n",
      "step: 399000 | loss: 0.5341\n",
      "step: 399200 | loss: 0.3982\n",
      "step: 399400 | loss: 0.3923\n",
      "step: 399600 | loss: 0.4513\n",
      "step: 399800 | loss: 0.5021\n",
      "step: 400000 | loss: 0.5148\n",
      "step: 400200 | loss: 0.5558\n",
      "step: 400400 | loss: 0.5188\n",
      "step: 400600 | loss: 0.4743\n",
      "step: 400800 | loss: 0.4102\n",
      "step: 401000 | loss: 0.4321\n",
      "step: 401200 | loss: 0.6074\n",
      "step: 401400 | loss: 0.5319\n",
      "step: 401600 | loss: 0.5032\n",
      "step: 401800 | loss: 0.5605\n",
      "step: 402000 | loss: 0.5061\n",
      "step: 402200 | loss: 0.4758\n",
      "step: 402400 | loss: 0.5232\n",
      "step: 402600 | loss: 0.4692\n",
      "step: 402800 | loss: 0.5805\n",
      "step: 403000 | loss: 0.5515\n",
      "step: 403200 | loss: 0.5680\n",
      "step: 403400 | loss: 0.5210\n",
      "step: 403600 | loss: 0.6680\n",
      "step: 403800 | loss: 0.5614\n",
      "step: 404000 | loss: 0.5285\n",
      "step: 404200 | loss: 0.5641\n",
      "step: 404400 | loss: 0.5085\n",
      "step: 404600 | loss: 0.5974\n",
      "step: 404800 | loss: 0.5480\n",
      "step: 405000 | loss: 0.5148\n",
      "step: 405200 | loss: 0.6289\n",
      "step: 405400 | loss: 0.5520\n",
      "step: 405600 | loss: 0.5255\n",
      "step: 405800 | loss: 0.4847\n",
      "step: 406000 | loss: 0.4716\n",
      "step: 406200 | loss: 0.4837\n",
      "step: 406400 | loss: 0.5033\n",
      "step: 406600 | loss: 0.4816\n",
      "step: 406800 | loss: 0.5372\n",
      "step: 407000 | loss: 0.5522\n",
      "step: 407200 | loss: 0.4402\n",
      "step: 407400 | loss: 0.5057\n",
      "step: 407600 | loss: 0.5515\n",
      "step: 407800 | loss: 0.5048\n",
      "step: 408000 | loss: 0.6511\n",
      "step: 408200 | loss: 0.7063\n",
      "step: 408400 | loss: 0.6232\n",
      "step: 408600 | loss: 0.4378\n",
      "step: 408800 | loss: 0.4454\n",
      "step: 409000 | loss: 0.5184\n",
      "step: 409200 | loss: 0.5212\n",
      "step: 409400 | loss: 0.5161\n",
      "step: 409600 | loss: 0.4646\n",
      "step: 409800 | loss: 0.4240\n",
      "step: 410000 | loss: 0.4662\n",
      "step: 410200 | loss: 0.4979\n",
      "step: 410400 | loss: 0.4768\n",
      "step: 410600 | loss: 0.4080\n",
      "step: 410800 | loss: 0.3692\n",
      "step: 411000 | loss: 0.5023\n",
      "step: 411200 | loss: 0.4304\n",
      "step: 411400 | loss: 0.5425\n",
      "step: 411600 | loss: 0.5090\n",
      "step: 411800 | loss: 0.4716\n",
      "step: 412000 | loss: 0.3916\n",
      "step: 412200 | loss: 0.4220\n",
      "step: 412400 | loss: 0.3617\n",
      "step: 412600 | loss: 0.3918\n",
      "step: 412800 | loss: 0.3730\n",
      "step: 413000 | loss: 0.3205\n",
      "step: 413200 | loss: 0.3648\n",
      "step: 413400 | loss: 0.4250\n",
      "step: 413600 | loss: 0.3575\n",
      "step: 413800 | loss: 0.4067\n",
      "step: 414000 | loss: 0.3798\n",
      "step: 414200 | loss: 0.3979\n",
      "step: 414400 | loss: 0.3605\n",
      "step: 414600 | loss: 0.3659\n",
      "step: 414800 | loss: 0.4501\n",
      "step: 415000 | loss: 0.3379\n",
      "step: 415200 | loss: 0.4502\n",
      "step: 415400 | loss: 0.4456\n",
      "step: 415600 | loss: 0.3727\n",
      "step: 415800 | loss: 0.3829\n",
      "step: 416000 | loss: 0.3686\n",
      "step: 416200 | loss: 0.3955\n",
      "step: 416400 | loss: 0.3476\n",
      "step: 416600 | loss: 0.3241\n",
      "step: 416800 | loss: 0.3710\n",
      "step: 417000 | loss: 0.3581\n",
      "step: 417200 | loss: 0.3668\n",
      "step: 417400 | loss: 0.3311\n",
      "step: 417600 | loss: 0.4172\n",
      "step: 417800 | loss: 0.4184\n",
      "step: 418000 | loss: 0.5085\n",
      "step: 418200 | loss: 0.5541\n",
      "step: 418400 | loss: 0.4154\n",
      "step: 418600 | loss: 0.4048\n",
      "step: 418800 | loss: 0.4537\n",
      "step: 419000 | loss: 0.3895\n",
      "step: 419200 | loss: 0.4640\n",
      "step: 419400 | loss: 0.3907\n",
      "step: 419600 | loss: 0.3901\n",
      "step: 419800 | loss: 0.3937\n",
      "step: 420000 | loss: 0.3731\n",
      "step: 420200 | loss: 0.3904\n",
      "step: 420400 | loss: 0.4363\n",
      "step: 420600 | loss: 0.4214\n",
      "step: 420800 | loss: 0.3667\n",
      "step: 421000 | loss: 0.4290\n",
      "step: 421200 | loss: 0.4709\n",
      "step: 421400 | loss: 0.4494\n",
      "step: 421600 | loss: 0.4199\n",
      "step: 421800 | loss: 0.3632\n",
      "step: 422000 | loss: 0.3243\n",
      "step: 422200 | loss: 0.4224\n",
      "step: 422400 | loss: 0.3948\n",
      "step: 422600 | loss: 0.4110\n",
      "step: 422800 | loss: 0.4008\n",
      "step: 423000 | loss: 0.4367\n",
      "step: 423200 | loss: 0.4370\n",
      "step: 423400 | loss: 0.4200\n",
      "step: 423600 | loss: 0.4247\n",
      "step: 423800 | loss: 0.3755\n",
      "step: 424000 | loss: 0.4085\n",
      "step: 424200 | loss: 0.3746\n",
      "step: 424400 | loss: 0.4432\n",
      "step: 424600 | loss: 0.4643\n",
      "step: 424800 | loss: 0.6420\n",
      "step: 425000 | loss: 0.5993\n",
      "step: 425200 | loss: 0.4900\n",
      "step: 425400 | loss: 0.5130\n",
      "step: 425600 | loss: 0.5286\n",
      "step: 425800 | loss: 0.5038\n",
      "step: 426000 | loss: 0.4693\n",
      "step: 426200 | loss: 0.4662\n",
      "step: 426400 | loss: 0.4350\n",
      "step: 426600 | loss: 0.4572\n",
      "step: 426800 | loss: 0.4976\n",
      "step: 427000 | loss: 0.3621\n",
      "step: 427200 | loss: 0.4647\n",
      "step: 427400 | loss: 0.4401\n",
      "step: 427600 | loss: 0.4517\n",
      "step: 427800 | loss: 0.4841\n",
      "step: 428000 | loss: 0.5272\n",
      "step: 428200 | loss: 0.4833\n",
      "step: 428400 | loss: 0.4729\n",
      "step: 428600 | loss: 0.4456\n",
      "step: 428800 | loss: 0.4857\n",
      "step: 429000 | loss: 0.5728\n",
      "step: 429200 | loss: 0.3656\n",
      "step: 429400 | loss: 0.4808\n",
      "step: 429600 | loss: 0.4938\n",
      "step: 429800 | loss: 0.4418\n",
      "step: 430000 | loss: 0.4823\n",
      "step: 430200 | loss: 0.4601\n",
      "step: 430400 | loss: 0.5061\n",
      "step: 430600 | loss: 0.4354\n",
      "step: 430800 | loss: 0.4930\n",
      "step: 431000 | loss: 0.6222\n",
      "step: 431200 | loss: 0.6538\n",
      "step: 431400 | loss: 0.7101\n",
      "step: 431600 | loss: 0.6305\n",
      "step: 431800 | loss: 0.6312\n",
      "step: 432000 | loss: 0.6721\n",
      "step: 432200 | loss: 0.5929\n",
      "step: 432400 | loss: 0.5898\n",
      "step: 432600 | loss: 0.5638\n",
      "step: 432800 | loss: 0.6841\n",
      "step: 433000 | loss: 0.6125\n",
      "step: 433200 | loss: 0.5162\n",
      "step: 433400 | loss: 0.5958\n",
      "step: 433600 | loss: 0.6194\n",
      "step: 433800 | loss: 0.6950\n",
      "step: 434000 | loss: 0.4022\n",
      "step: 434200 | loss: 0.7830\n",
      "step: 434400 | loss: 0.6411\n",
      "step: 434600 | loss: 0.4974\n",
      "step: 434800 | loss: 0.5209\n",
      "step: 435000 | loss: 0.5174\n",
      "step: 435200 | loss: 0.4471\n",
      "step: 435400 | loss: 0.4429\n",
      "step: 435600 | loss: 0.3338\n",
      "step: 435800 | loss: 0.4364\n",
      "step: 436000 | loss: 0.4775\n",
      "step: 436200 | loss: 0.4245\n",
      "step: 436400 | loss: 0.4193\n",
      "step: 436600 | loss: 0.4089\n",
      "step: 436800 | loss: 0.3845\n",
      "step: 437000 | loss: 0.3533\n",
      "step: 437200 | loss: 0.4229\n",
      "step: 437400 | loss: 0.3650\n",
      "step: 437600 | loss: 0.4216\n",
      "step: 437800 | loss: 0.4739\n",
      "step: 438000 | loss: 0.4180\n",
      "step: 438200 | loss: 0.3997\n",
      "step: 438400 | loss: 0.4261\n",
      "step: 438600 | loss: 0.3881\n",
      "step: 438800 | loss: 0.3667\n",
      "step: 439000 | loss: 0.3878\n",
      "step: 439200 | loss: 0.3434\n",
      "step: 439400 | loss: 0.3644\n",
      "step: 439600 | loss: 0.3839\n",
      "step: 439800 | loss: 0.3976\n",
      "step: 440000 | loss: 0.3893\n",
      "step: 440200 | loss: 0.4384\n",
      "step: 440400 | loss: 0.3980\n",
      "step: 440600 | loss: 0.4537\n",
      "step: 440800 | loss: 0.3828\n",
      "step: 441000 | loss: 0.3668\n",
      "step: 441200 | loss: 0.5001\n",
      "step: 441400 | loss: 0.3429\n",
      "step: 441600 | loss: 0.3435\n",
      "step: 441800 | loss: 0.3111\n",
      "step: 442000 | loss: 0.3750\n",
      "step: 442200 | loss: 0.3237\n",
      "step: 442400 | loss: 0.4157\n",
      "step: 442600 | loss: 0.3668\n",
      "step: 442800 | loss: 0.4473\n",
      "step: 443000 | loss: 0.4232\n",
      "step: 443200 | loss: 0.4169\n",
      "step: 443400 | loss: 0.3438\n",
      "step: 443600 | loss: 0.3735\n",
      "step: 443800 | loss: 0.3496\n",
      "step: 444000 | loss: 0.4230\n",
      "step: 444200 | loss: 0.4831\n",
      "step: 444400 | loss: 0.4296\n",
      "step: 444600 | loss: 0.3184\n",
      "step: 444800 | loss: 0.3068\n",
      "step: 445000 | loss: 0.3118\n",
      "step: 445200 | loss: 0.3140\n",
      "step: 445400 | loss: 0.3420\n",
      "step: 445600 | loss: 0.3353\n",
      "step: 445800 | loss: 0.3654\n",
      "step: 446000 | loss: 0.3603\n",
      "step: 446200 | loss: 0.4384\n",
      "step: 446400 | loss: 0.3645\n",
      "step: 446600 | loss: 0.3491\n",
      "step: 446800 | loss: 0.3704\n",
      "step: 447000 | loss: 0.4185\n",
      "step: 447200 | loss: 0.2720\n",
      "step: 447400 | loss: 0.5814\n",
      "step: 447600 | loss: 0.4434\n",
      "step: 447800 | loss: 0.4418\n",
      "step: 448000 | loss: 0.4396\n",
      "step: 448200 | loss: 0.5541\n",
      "step: 448400 | loss: 0.4313\n",
      "step: 448600 | loss: 0.3563\n",
      "step: 448800 | loss: 0.4146\n",
      "step: 449000 | loss: 0.4305\n",
      "step: 449200 | loss: 0.3668\n",
      "step: 449400 | loss: 0.3768\n",
      "step: 449600 | loss: 0.4067\n",
      "step: 449800 | loss: 0.4339\n",
      "step: 450000 | loss: 0.3915\n",
      "step: 450200 | loss: 0.4437\n",
      "step: 450400 | loss: 0.4167\n",
      "step: 450600 | loss: 0.5149\n",
      "step: 450800 | loss: 0.5879\n",
      "step: 451000 | loss: 0.5230\n",
      "step: 451200 | loss: 0.6255\n",
      "step: 451400 | loss: 0.5265\n",
      "step: 451600 | loss: 0.5497\n",
      "step: 451800 | loss: 0.5463\n",
      "step: 452000 | loss: 0.5472\n",
      "step: 452200 | loss: 0.5761\n",
      "step: 452400 | loss: 0.5370\n",
      "step: 452600 | loss: 0.5351\n",
      "step: 452800 | loss: 0.4778\n",
      "step: 453000 | loss: 0.5764\n",
      "step: 453200 | loss: 0.5803\n",
      "step: 453400 | loss: 0.5457\n",
      "step: 453600 | loss: 0.5906\n",
      "step: 453800 | loss: 0.5800\n",
      "step: 454000 | loss: 0.5328\n",
      "step: 454200 | loss: 0.5887\n",
      "step: 454400 | loss: 0.5865\n",
      "step: 454600 | loss: 0.3866\n",
      "step: 454800 | loss: 0.5717\n",
      "step: 455000 | loss: 0.4787\n",
      "step: 455200 | loss: 0.5182\n",
      "step: 455400 | loss: 0.5088\n",
      "step: 455600 | loss: 0.5083\n",
      "step: 455800 | loss: 0.4706\n",
      "step: 456000 | loss: 0.4180\n",
      "step: 456200 | loss: 0.5024\n",
      "step: 456400 | loss: 0.5378\n",
      "step: 456600 | loss: 0.4869\n",
      "step: 456800 | loss: 0.4042\n",
      "step: 457000 | loss: 0.5062\n",
      "step: 457200 | loss: 0.4166\n",
      "step: 457400 | loss: 0.4162\n",
      "step: 457600 | loss: 0.4840\n",
      "step: 457800 | loss: 0.4738\n",
      "step: 458000 | loss: 0.4927\n",
      "step: 458200 | loss: 0.4856\n",
      "step: 458400 | loss: 0.4880\n",
      "step: 458600 | loss: 0.4521\n",
      "step: 458800 | loss: 0.4583\n",
      "step: 459000 | loss: 0.5718\n",
      "step: 459200 | loss: 0.5804\n",
      "step: 459400 | loss: 0.4511\n",
      "step: 459600 | loss: 0.5232\n",
      "step: 459800 | loss: 0.5070\n",
      "step: 460000 | loss: 0.4106\n",
      "step: 460200 | loss: 0.5574\n",
      "step: 460400 | loss: 0.5015\n",
      "step: 460600 | loss: 0.5214\n",
      "step: 460800 | loss: 0.5639\n",
      "step: 461000 | loss: 0.5202\n",
      "step: 461200 | loss: 0.5410\n",
      "step: 461400 | loss: 0.4826\n",
      "step: 461600 | loss: 0.4694\n",
      "step: 461800 | loss: 0.4942\n",
      "step: 462000 | loss: 0.4534\n",
      "step: 462200 | loss: 0.4121\n",
      "step: 462400 | loss: 0.4235\n",
      "step: 462600 | loss: 0.4473\n",
      "step: 462800 | loss: 0.3996\n",
      "step: 463000 | loss: 0.4668\n",
      "step: 463200 | loss: 0.4138\n",
      "step: 463400 | loss: 0.4233\n",
      "step: 463600 | loss: 0.4535\n",
      "step: 463800 | loss: 0.4453\n",
      "step: 464000 | loss: 0.4702\n",
      "step: 464200 | loss: 0.4352\n",
      "step: 464400 | loss: 0.3630\n",
      "step: 464600 | loss: 0.3749\n",
      "step: 464800 | loss: 0.3463\n",
      "step: 465000 | loss: 0.4066\n",
      "step: 465200 | loss: 0.3670\n",
      "step: 465400 | loss: 0.3425\n",
      "step: 465600 | loss: 0.4244\n",
      "step: 465800 | loss: 0.3366\n",
      "step: 466000 | loss: 0.3087\n",
      "step: 466200 | loss: 0.3723\n",
      "step: 466400 | loss: 0.4450\n",
      "step: 466600 | loss: 0.4240\n",
      "step: 466800 | loss: 0.3730\n",
      "step: 467000 | loss: 0.3954\n",
      "step: 467200 | loss: 0.3706\n",
      "step: 467400 | loss: 0.3789\n",
      "step: 467600 | loss: 0.3305\n",
      "step: 467800 | loss: 0.3469\n",
      "step: 468000 | loss: 0.3007\n",
      "step: 468200 | loss: 0.3593\n",
      "step: 468400 | loss: 0.3831\n",
      "step: 468600 | loss: 0.3871\n",
      "step: 468800 | loss: 0.3604\n",
      "step: 469000 | loss: 0.3346\n",
      "step: 469200 | loss: 0.3795\n",
      "step: 469400 | loss: 0.3217\n",
      "step: 469600 | loss: 0.2882\n",
      "step: 469800 | loss: 0.3663\n",
      "step: 470000 | loss: 0.3803\n",
      "step: 470200 | loss: 0.4704\n",
      "step: 470400 | loss: 0.4723\n",
      "step: 470600 | loss: 0.5249\n",
      "step: 470800 | loss: 0.4589\n",
      "step: 471000 | loss: 0.3755\n",
      "step: 471200 | loss: 0.4930\n",
      "step: 471400 | loss: 0.4322\n",
      "step: 471600 | loss: 0.3813\n",
      "step: 471800 | loss: 0.3949\n",
      "step: 472000 | loss: 0.4799\n",
      "step: 472200 | loss: 0.3789\n",
      "step: 472400 | loss: 0.3824\n",
      "step: 472600 | loss: 0.3901\n",
      "step: 472800 | loss: 0.3747\n",
      "step: 473000 | loss: 0.3945\n",
      "step: 473200 | loss: 0.4019\n",
      "step: 473400 | loss: 0.3453\n",
      "step: 473600 | loss: 0.5016\n",
      "step: 473800 | loss: 0.4544\n",
      "step: 474000 | loss: 0.4431\n",
      "step: 474200 | loss: 0.4597\n",
      "step: 474400 | loss: 0.3875\n",
      "step: 474600 | loss: 0.4438\n",
      "step: 474800 | loss: 0.3350\n",
      "step: 475000 | loss: 0.4289\n",
      "step: 475200 | loss: 0.4340\n",
      "step: 475400 | loss: 0.4593\n",
      "step: 475600 | loss: 0.4707\n",
      "step: 475800 | loss: 0.4098\n",
      "step: 476000 | loss: 0.4695\n",
      "step: 476200 | loss: 0.4587\n",
      "step: 476400 | loss: 0.4158\n",
      "step: 476600 | loss: 0.3993\n",
      "step: 476800 | loss: 0.4965\n",
      "step: 477000 | loss: 0.6512\n",
      "step: 477200 | loss: 0.5360\n",
      "step: 477400 | loss: 0.4017\n",
      "step: 477600 | loss: 0.4597\n",
      "step: 477800 | loss: 0.5005\n",
      "step: 478000 | loss: 0.5452\n",
      "step: 478200 | loss: 0.5346\n",
      "step: 478400 | loss: 0.4596\n",
      "step: 478600 | loss: 0.3883\n",
      "step: 478800 | loss: 0.5149\n",
      "step: 479000 | loss: 0.4480\n",
      "step: 479200 | loss: 0.5082\n",
      "step: 479400 | loss: 0.4840\n",
      "step: 479600 | loss: 0.3680\n",
      "step: 479800 | loss: 0.3418\n",
      "step: 480000 | loss: 0.3441\n",
      "step: 480200 | loss: 0.3102\n",
      "step: 480400 | loss: 0.3780\n",
      "step: 480600 | loss: 0.3364\n",
      "step: 480800 | loss: 0.4075\n",
      "step: 481000 | loss: 0.3834\n",
      "step: 481200 | loss: 0.4012\n",
      "step: 481400 | loss: 0.4216\n",
      "step: 481600 | loss: 0.3967\n",
      "step: 481800 | loss: 0.3992\n",
      "step: 482000 | loss: 0.3962\n",
      "step: 482200 | loss: 0.4139\n",
      "step: 482400 | loss: 0.4242\n",
      "step: 482600 | loss: 0.4040\n",
      "step: 482800 | loss: 0.3449\n",
      "step: 483000 | loss: 0.3681\n",
      "step: 483200 | loss: 0.3561\n",
      "step: 483400 | loss: 0.3313\n",
      "step: 483600 | loss: 0.3514\n",
      "step: 483800 | loss: 0.3902\n",
      "step: 484000 | loss: 0.4651\n",
      "step: 484200 | loss: 0.3974\n",
      "step: 484400 | loss: 0.3831\n",
      "step: 484600 | loss: 0.3840\n",
      "step: 484800 | loss: 0.4423\n",
      "step: 485000 | loss: 0.3660\n",
      "step: 485200 | loss: 0.3691\n",
      "step: 485400 | loss: 0.3999\n",
      "step: 485600 | loss: 0.3914\n",
      "step: 485800 | loss: 0.4143\n",
      "step: 486000 | loss: 0.3806\n",
      "step: 486200 | loss: 0.3783\n",
      "step: 486400 | loss: 0.3154\n",
      "step: 486600 | loss: 0.3504\n",
      "step: 486800 | loss: 0.3905\n",
      "step: 487000 | loss: 0.4013\n",
      "step: 487200 | loss: 0.3566\n",
      "step: 487400 | loss: 0.3417\n",
      "step: 487600 | loss: 0.3374\n",
      "step: 487800 | loss: 0.3815\n",
      "step: 488000 | loss: 0.4351\n",
      "step: 488200 | loss: 0.3469\n",
      "step: 488400 | loss: 0.3959\n",
      "step: 488600 | loss: 0.4125\n",
      "step: 488800 | loss: 0.4418\n",
      "step: 489000 | loss: 0.3786\n",
      "step: 489200 | loss: 0.4314\n",
      "step: 489400 | loss: 0.4179\n",
      "step: 489600 | loss: 0.3890\n",
      "step: 489800 | loss: 0.4133\n",
      "step: 490000 | loss: 0.4728\n",
      "step: 490200 | loss: 0.4287\n",
      "step: 490400 | loss: 0.4267\n",
      "step: 490600 | loss: 0.4365\n",
      "step: 490800 | loss: 0.3703\n",
      "step: 491000 | loss: 0.4123\n",
      "step: 491200 | loss: 0.4232\n",
      "step: 491400 | loss: 0.4030\n",
      "step: 491600 | loss: 0.4809\n",
      "step: 491800 | loss: 0.4465\n",
      "step: 492000 | loss: 0.4071\n",
      "step: 492200 | loss: 0.4605\n",
      "step: 492400 | loss: 0.4032\n",
      "step: 492600 | loss: 0.3860\n",
      "step: 492800 | loss: 0.4736\n",
      "step: 493000 | loss: 0.5175\n",
      "step: 493200 | loss: 0.7616\n",
      "step: 493400 | loss: 0.6687\n",
      "step: 493600 | loss: 0.5539\n",
      "step: 493800 | loss: 0.5890\n",
      "step: 494000 | loss: 0.5395\n",
      "step: 494200 | loss: 0.4443\n",
      "step: 494400 | loss: 0.5005\n",
      "step: 494600 | loss: 0.4687\n",
      "step: 494800 | loss: 0.5913\n",
      "step: 495000 | loss: 0.4952\n",
      "step: 495200 | loss: 0.5899\n",
      "step: 495400 | loss: 0.5015\n",
      "step: 495600 | loss: 0.5530\n",
      "step: 495800 | loss: 0.4988\n",
      "step: 496000 | loss: 0.5278\n",
      "step: 496200 | loss: 0.5126\n",
      "step: 496400 | loss: 0.7511\n",
      "step: 496600 | loss: 0.6197\n",
      "step: 496800 | loss: 0.5169\n",
      "step: 497000 | loss: 0.4972\n",
      "step: 497200 | loss: 0.4230\n",
      "step: 497400 | loss: 0.3812\n",
      "step: 497600 | loss: 0.4684\n",
      "step: 497800 | loss: 0.4476\n",
      "step: 498000 | loss: 0.4314\n",
      "step: 498200 | loss: 0.3849\n",
      "step: 498400 | loss: 0.5038\n",
      "step: 498600 | loss: 0.4686\n",
      "step: 498800 | loss: 0.4805\n",
      "step: 499000 | loss: 0.4089\n",
      "step: 499200 | loss: 0.4006\n",
      "step: 499400 | loss: 0.4338\n",
      "step: 499600 | loss: 0.4999\n",
      "step: 499800 | loss: 0.5316\n",
      "step: 500000 | loss: 0.4367\n",
      "step: 500200 | loss: 0.4910\n",
      "step: 500400 | loss: 0.5192\n",
      "step: 500600 | loss: 0.4335\n",
      "step: 500800 | loss: 0.5252\n",
      "step: 501000 | loss: 0.4513\n",
      "step: 501200 | loss: 0.4706\n",
      "step: 501400 | loss: 0.5439\n",
      "step: 501600 | loss: 0.4048\n",
      "step: 501800 | loss: 0.5544\n",
      "step: 502000 | loss: 0.5595\n",
      "step: 502200 | loss: 0.5407\n",
      "step: 502400 | loss: 0.5917\n",
      "step: 502600 | loss: 0.6022\n",
      "step: 502800 | loss: 0.5170\n",
      "step: 503000 | loss: 0.5196\n",
      "step: 503200 | loss: 0.6202\n",
      "step: 503400 | loss: 0.6600\n",
      "step: 503600 | loss: 0.6508\n",
      "step: 503800 | loss: 0.5784\n",
      "step: 504000 | loss: 0.6179\n",
      "step: 504200 | loss: 0.6037\n",
      "step: 504400 | loss: 0.6190\n",
      "step: 504600 | loss: 0.6364\n",
      "step: 504800 | loss: 0.6127\n",
      "step: 505000 | loss: 0.5305\n",
      "step: 505200 | loss: 0.6122\n",
      "step: 505400 | loss: 0.6440\n",
      "step: 505600 | loss: 0.6031\n",
      "step: 505800 | loss: 0.5248\n",
      "step: 506000 | loss: 0.5802\n",
      "step: 506200 | loss: 0.4945\n",
      "step: 506400 | loss: 0.7048\n",
      "step: 506600 | loss: 0.5304\n",
      "step: 506800 | loss: 0.5407\n",
      "step: 507000 | loss: 0.4720\n",
      "step: 507200 | loss: 0.3920\n",
      "step: 507400 | loss: 0.6759\n",
      "step: 507600 | loss: 0.5334\n",
      "step: 507800 | loss: 0.6072\n",
      "step: 508000 | loss: 0.5914\n",
      "step: 508200 | loss: 0.5103\n",
      "step: 508400 | loss: 0.4941\n",
      "step: 508600 | loss: 0.5279\n",
      "step: 508800 | loss: 0.4908\n",
      "step: 509000 | loss: 0.5050\n",
      "step: 509200 | loss: 0.5277\n",
      "step: 509400 | loss: 0.5441\n",
      "step: 509600 | loss: 0.5059\n",
      "step: 509800 | loss: 0.5263\n",
      "step: 510000 | loss: 0.5164\n",
      "step: 510200 | loss: 0.6460\n",
      "step: 510400 | loss: 0.4991\n",
      "step: 510600 | loss: 0.5204\n",
      "step: 510800 | loss: 0.5092\n",
      "step: 511000 | loss: 0.5372\n",
      "step: 511200 | loss: 0.4980\n",
      "step: 511400 | loss: 0.4879\n",
      "step: 511600 | loss: 0.5647\n",
      "step: 511800 | loss: 0.4909\n",
      "step: 512000 | loss: 0.5265\n",
      "step: 512200 | loss: 0.5119\n",
      "step: 512400 | loss: 0.6431\n",
      "step: 512600 | loss: 0.5496\n",
      "step: 512800 | loss: 0.4666\n",
      "step: 513000 | loss: 0.5069\n",
      "step: 513200 | loss: 0.4313\n",
      "step: 513400 | loss: 0.4816\n",
      "step: 513600 | loss: 0.3880\n",
      "step: 513800 | loss: 0.5117\n",
      "step: 514000 | loss: 0.4272\n",
      "step: 514200 | loss: 0.4338\n",
      "step: 514400 | loss: 0.4942\n",
      "step: 514600 | loss: 0.3624\n",
      "step: 514800 | loss: 0.4578\n",
      "step: 515000 | loss: 0.4237\n",
      "step: 515200 | loss: 0.4451\n",
      "step: 515400 | loss: 0.4345\n",
      "step: 515600 | loss: 0.5021\n",
      "step: 515800 | loss: 0.4187\n",
      "step: 516000 | loss: 0.4118\n",
      "step: 516200 | loss: 0.4979\n",
      "step: 516400 | loss: 0.3446\n",
      "step: 516600 | loss: 0.3770\n",
      "step: 516800 | loss: 0.4022\n",
      "step: 517000 | loss: 0.3839\n",
      "step: 517200 | loss: 0.3595\n",
      "step: 517400 | loss: 0.3842\n",
      "step: 517600 | loss: 0.3898\n",
      "step: 517800 | loss: 0.3984\n",
      "step: 518000 | loss: 0.4567\n",
      "step: 518200 | loss: 0.4358\n",
      "step: 518400 | loss: 0.3396\n",
      "step: 518600 | loss: 0.3719\n",
      "step: 518800 | loss: 0.2958\n",
      "step: 519000 | loss: 0.3576\n",
      "step: 519200 | loss: 0.3541\n",
      "step: 519400 | loss: 0.5284\n",
      "step: 519600 | loss: 0.5807\n",
      "step: 519800 | loss: 0.5810\n",
      "step: 520000 | loss: 0.5149\n",
      "step: 520200 | loss: 0.4717\n",
      "step: 520400 | loss: 0.4822\n",
      "step: 520600 | loss: 0.4762\n",
      "step: 520800 | loss: 0.5216\n",
      "step: 521000 | loss: 0.5835\n",
      "step: 521200 | loss: 0.5865\n",
      "step: 521400 | loss: 0.4772\n",
      "step: 521600 | loss: 0.5720\n",
      "step: 521800 | loss: 0.5059\n",
      "step: 522000 | loss: 0.4944\n",
      "step: 522200 | loss: 0.5641\n",
      "step: 522400 | loss: 0.5243\n",
      "step: 522600 | loss: 0.5664\n",
      "step: 522800 | loss: 0.5964\n",
      "step: 523000 | loss: 0.6104\n",
      "step: 523200 | loss: 0.6720\n",
      "step: 523400 | loss: 0.4700\n",
      "step: 523600 | loss: 0.5464\n",
      "step: 523800 | loss: 0.5854\n",
      "step: 524000 | loss: 0.7270\n",
      "step: 524200 | loss: 0.5112\n",
      "step: 524400 | loss: 0.5357\n",
      "step: 524600 | loss: 0.6432\n",
      "step: 524800 | loss: 0.5664\n",
      "step: 525000 | loss: 0.5030\n",
      "step: 525200 | loss: 0.5478\n",
      "step: 525400 | loss: 0.5463\n",
      "step: 525600 | loss: 0.6620\n",
      "step: 525800 | loss: 0.5931\n",
      "step: 526000 | loss: 0.5923\n",
      "step: 526200 | loss: 0.5996\n",
      "step: 526400 | loss: 0.5660\n",
      "step: 526600 | loss: 0.5894\n",
      "step: 526800 | loss: 0.5192\n",
      "step: 527000 | loss: 0.5195\n",
      "step: 527200 | loss: 0.4939\n",
      "step: 527400 | loss: 0.4138\n",
      "step: 527600 | loss: 0.4854\n",
      "step: 527800 | loss: 0.4085\n",
      "step: 528000 | loss: 0.5015\n",
      "step: 528200 | loss: 0.4751\n",
      "step: 528400 | loss: 0.5143\n",
      "step: 528600 | loss: 0.5434\n",
      "step: 528800 | loss: 0.4938\n",
      "step: 529000 | loss: 0.5255\n",
      "step: 529200 | loss: 0.6669\n",
      "step: 529400 | loss: 0.5547\n",
      "step: 529600 | loss: 0.5030\n",
      "step: 529800 | loss: 0.4563\n",
      "step: 530000 | loss: 0.4823\n",
      "step: 530200 | loss: 0.6060\n",
      "step: 530400 | loss: 0.6362\n",
      "step: 530600 | loss: 0.6989\n",
      "step: 530800 | loss: 0.4909\n",
      "step: 531000 | loss: 0.5494\n",
      "step: 531200 | loss: 0.4779\n",
      "step: 531400 | loss: 0.4687\n",
      "step: 531600 | loss: 0.5683\n",
      "step: 531800 | loss: 0.5989\n",
      "step: 532000 | loss: 0.6265\n",
      "step: 532200 | loss: 0.4967\n",
      "step: 532400 | loss: 0.6292\n",
      "step: 532600 | loss: 0.7917\n",
      "step: 532800 | loss: 0.6076\n",
      "step: 533000 | loss: 0.5644\n",
      "step: 533200 | loss: 0.5095\n",
      "step: 533400 | loss: 0.5100\n",
      "step: 533600 | loss: 0.4329\n",
      "step: 533800 | loss: 0.4188\n",
      "step: 534000 | loss: 0.4359\n",
      "step: 534200 | loss: 0.3778\n",
      "step: 534400 | loss: 0.4266\n",
      "step: 534600 | loss: 0.4054\n",
      "step: 534800 | loss: 0.4507\n",
      "step: 535000 | loss: 0.4621\n",
      "step: 535200 | loss: 0.4228\n",
      "step: 535400 | loss: 0.3908\n",
      "step: 535600 | loss: 0.4461\n",
      "step: 535800 | loss: 0.7326\n",
      "step: 536000 | loss: 0.7217\n",
      "step: 536200 | loss: 0.6095\n",
      "step: 536400 | loss: 0.5163\n",
      "step: 536600 | loss: 0.5227\n",
      "step: 536800 | loss: 0.5038\n",
      "step: 537000 | loss: 0.4961\n",
      "step: 537200 | loss: 0.5481\n",
      "step: 537400 | loss: 0.4878\n",
      "step: 537600 | loss: 0.5447\n",
      "step: 537800 | loss: 0.4776\n",
      "step: 538000 | loss: 0.5228\n",
      "step: 538200 | loss: 0.5060\n",
      "step: 538400 | loss: 0.5823\n",
      "step: 538600 | loss: 0.4698\n",
      "step: 538800 | loss: 0.4617\n",
      "step: 539000 | loss: 0.6341\n",
      "step: 539200 | loss: 0.6154\n",
      "step: 539400 | loss: 0.5591\n",
      "step: 539600 | loss: 0.5133\n",
      "step: 539800 | loss: 0.4174\n",
      "step: 540000 | loss: 0.4430\n",
      "step: 540200 | loss: 0.4974\n",
      "step: 540400 | loss: 0.5135\n",
      "step: 540600 | loss: 0.4678\n",
      "step: 540800 | loss: 0.4425\n",
      "step: 541000 | loss: 0.4146\n",
      "step: 541200 | loss: 0.4266\n",
      "step: 541400 | loss: 0.4868\n",
      "step: 541600 | loss: 0.5894\n",
      "step: 541800 | loss: 0.4724\n",
      "step: 542000 | loss: 0.4955\n",
      "step: 542200 | loss: 0.4618\n",
      "step: 542400 | loss: 0.4403\n",
      "step: 542600 | loss: 0.5866\n",
      "step: 542800 | loss: 0.5466\n",
      "step: 543000 | loss: 0.4609\n",
      "step: 543200 | loss: 0.5009\n",
      "step: 543400 | loss: 0.4708\n",
      "step: 543600 | loss: 0.4499\n",
      "step: 543800 | loss: 0.4968\n",
      "step: 544000 | loss: 0.4968\n",
      "step: 544200 | loss: 0.5282\n",
      "step: 544400 | loss: 0.4651\n",
      "step: 544600 | loss: 0.4471\n",
      "step: 544800 | loss: 0.4973\n",
      "step: 545000 | loss: 0.4117\n",
      "step: 545200 | loss: 0.4760\n",
      "step: 545400 | loss: 0.4231\n",
      "step: 545600 | loss: 0.7222\n",
      "step: 545800 | loss: 0.5421\n",
      "step: 546000 | loss: 0.5889\n",
      "step: 546200 | loss: 0.5383\n",
      "step: 546400 | loss: 0.5447\n",
      "step: 546600 | loss: 0.5173\n",
      "step: 546800 | loss: 0.5562\n",
      "step: 547000 | loss: 0.4911\n",
      "step: 547200 | loss: 0.4773\n",
      "step: 547400 | loss: 0.4128\n",
      "step: 547600 | loss: 0.4830\n",
      "step: 547800 | loss: 0.4302\n",
      "step: 548000 | loss: 0.4069\n",
      "step: 548200 | loss: 0.4153\n",
      "step: 548400 | loss: 0.3431\n",
      "step: 548600 | loss: 0.4529\n",
      "step: 548800 | loss: 0.4604\n",
      "step: 549000 | loss: 0.4116\n",
      "step: 549200 | loss: 0.5089\n",
      "step: 549400 | loss: 0.4501\n",
      "step: 549600 | loss: 0.4398\n",
      "step: 549800 | loss: 0.4525\n",
      "step: 550000 | loss: 0.4332\n",
      "step: 550200 | loss: 0.4288\n",
      "step: 550400 | loss: 0.4414\n",
      "step: 550600 | loss: 0.4736\n",
      "step: 550800 | loss: 0.4490\n",
      "step: 551000 | loss: 0.4282\n",
      "step: 551200 | loss: 0.4888\n",
      "step: 551400 | loss: 0.4586\n",
      "step: 551600 | loss: 0.3953\n",
      "step: 551800 | loss: 0.3452\n",
      "step: 552000 | loss: 0.3680\n",
      "step: 552200 | loss: 0.3706\n",
      "step: 552400 | loss: 0.4099\n",
      "step: 552600 | loss: 0.3948\n",
      "step: 552800 | loss: 0.4147\n",
      "step: 553000 | loss: 0.4580\n",
      "step: 553200 | loss: 0.3755\n",
      "step: 553400 | loss: 0.4340\n",
      "step: 553600 | loss: 0.3731\n",
      "step: 553800 | loss: 0.4004\n",
      "step: 554000 | loss: 0.3719\n",
      "step: 554200 | loss: 0.4210\n",
      "step: 554400 | loss: 0.3945\n",
      "step: 554600 | loss: 0.4693\n",
      "step: 554800 | loss: 0.4043\n",
      "step: 555000 | loss: 0.3666\n",
      "step: 555200 | loss: 0.4179\n",
      "step: 555400 | loss: 0.5108\n",
      "step: 555600 | loss: 0.6469\n",
      "step: 555800 | loss: 0.5854\n",
      "step: 556000 | loss: 0.4820\n",
      "step: 556200 | loss: 0.4175\n",
      "step: 556400 | loss: 0.3676\n",
      "step: 556600 | loss: 0.4211\n",
      "step: 556800 | loss: 0.5054\n",
      "step: 557000 | loss: 0.4411\n",
      "step: 557200 | loss: 0.5006\n",
      "step: 557400 | loss: 0.4289\n",
      "step: 557600 | loss: 0.3543\n",
      "step: 557800 | loss: 0.4563\n",
      "step: 558000 | loss: 0.5010\n",
      "step: 558200 | loss: 0.5423\n",
      "step: 558400 | loss: 0.5239\n",
      "step: 558600 | loss: 0.4421\n",
      "step: 558800 | loss: 0.7703\n",
      "step: 559000 | loss: 0.7695\n",
      "step: 559200 | loss: 0.6397\n",
      "step: 559400 | loss: 0.6018\n",
      "step: 559600 | loss: 0.5219\n",
      "step: 559800 | loss: 0.5878\n",
      "step: 560000 | loss: 0.5791\n",
      "step: 560200 | loss: 0.5078\n",
      "step: 560400 | loss: 0.5398\n",
      "step: 560600 | loss: 0.5012\n",
      "step: 560800 | loss: 0.4927\n",
      "step: 561000 | loss: 0.4848\n",
      "step: 561200 | loss: 0.4759\n",
      "step: 561400 | loss: 0.5322\n",
      "step: 561600 | loss: 0.5584\n",
      "step: 561800 | loss: 0.4697\n",
      "step: 562000 | loss: 0.4856\n",
      "step: 562200 | loss: 0.4460\n",
      "step: 562400 | loss: 0.3730\n",
      "step: 562600 | loss: 0.4334\n",
      "step: 562800 | loss: 0.4784\n",
      "step: 563000 | loss: 0.4054\n",
      "step: 563200 | loss: 0.4089\n",
      "step: 563400 | loss: 0.4847\n",
      "step: 563600 | loss: 0.4733\n",
      "step: 563800 | loss: 0.5328\n",
      "step: 564000 | loss: 0.4735\n",
      "step: 564200 | loss: 0.4591\n",
      "step: 564400 | loss: 0.3612\n",
      "step: 564600 | loss: 0.4322\n",
      "step: 564800 | loss: 0.4701\n",
      "step: 565000 | loss: 0.3699\n",
      "step: 565200 | loss: 0.4675\n",
      "step: 565400 | loss: 0.4670\n",
      "step: 565600 | loss: 0.5833\n",
      "step: 565800 | loss: 0.5059\n",
      "step: 566000 | loss: 0.5140\n",
      "step: 566200 | loss: 0.4509\n",
      "step: 566400 | loss: 0.5079\n",
      "step: 566600 | loss: 0.5075\n",
      "step: 566800 | loss: 0.4674\n",
      "step: 567000 | loss: 0.4521\n",
      "step: 567200 | loss: 0.4056\n",
      "step: 567400 | loss: 0.5087\n",
      "step: 567600 | loss: 0.5184\n",
      "step: 567800 | loss: 0.4431\n",
      "step: 568000 | loss: 0.5088\n",
      "step: 568200 | loss: 0.4650\n",
      "step: 568400 | loss: 0.6079\n",
      "step: 568600 | loss: 0.5212\n",
      "step: 568800 | loss: 0.4023\n",
      "step: 569000 | loss: 0.4228\n",
      "step: 569200 | loss: 0.5793\n",
      "step: 569400 | loss: 0.5035\n",
      "step: 569600 | loss: 0.5005\n",
      "step: 569800 | loss: 0.4728\n",
      "step: 570000 | loss: 0.4751\n",
      "step: 570200 | loss: 0.4269\n",
      "step: 570400 | loss: 0.3994\n",
      "step: 570600 | loss: 0.3824\n",
      "step: 570800 | loss: 0.4633\n",
      "step: 571000 | loss: 0.4164\n",
      "step: 571200 | loss: 0.4689\n",
      "step: 571400 | loss: 0.4296\n",
      "step: 571600 | loss: 0.5313\n",
      "step: 571800 | loss: 0.4635\n",
      "step: 572000 | loss: 0.5600\n",
      "step: 572200 | loss: 0.5499\n",
      "step: 572400 | loss: 0.4876\n",
      "step: 572600 | loss: 0.4559\n",
      "step: 572800 | loss: 0.4977\n",
      "step: 573000 | loss: 0.5326\n",
      "step: 573200 | loss: 0.4969\n",
      "step: 573400 | loss: 0.4721\n",
      "step: 573600 | loss: 0.4877\n",
      "step: 573800 | loss: 0.5363\n",
      "step: 574000 | loss: 0.5505\n",
      "step: 574200 | loss: 0.6479\n",
      "step: 574400 | loss: 0.5149\n",
      "step: 574600 | loss: 0.5986\n",
      "step: 574800 | loss: 0.5338\n",
      "step: 575000 | loss: 0.4658\n",
      "step: 575200 | loss: 0.6398\n",
      "step: 575400 | loss: 0.7173\n",
      "step: 575600 | loss: 0.7149\n",
      "step: 575800 | loss: 0.5846\n",
      "step: 576000 | loss: 0.5235\n",
      "step: 576200 | loss: 0.6707\n",
      "step: 576400 | loss: 0.5808\n",
      "step: 576600 | loss: 0.5432\n",
      "step: 576800 | loss: 0.5743\n",
      "step: 577000 | loss: 0.5373\n",
      "step: 577200 | loss: 0.5962\n",
      "step: 577400 | loss: 0.5678\n",
      "step: 577600 | loss: 0.4904\n",
      "step: 577800 | loss: 0.6130\n",
      "step: 578000 | loss: 0.5236\n",
      "step: 578200 | loss: 0.5268\n",
      "step: 578400 | loss: 0.5563\n",
      "step: 578600 | loss: 0.6357\n",
      "step: 578800 | loss: 0.5792\n",
      "step: 579000 | loss: 0.7213\n",
      "step: 579200 | loss: 0.4923\n",
      "step: 579400 | loss: 0.6128\n",
      "step: 579600 | loss: 0.5590\n",
      "step: 579800 | loss: 0.5092\n",
      "step: 580000 | loss: 0.4652\n",
      "step: 580200 | loss: 0.7250\n",
      "step: 580400 | loss: 0.6409\n",
      "step: 580600 | loss: 0.6069\n",
      "step: 580800 | loss: 0.4498\n",
      "step: 581000 | loss: 0.3896\n",
      "step: 581200 | loss: 0.4689\n",
      "step: 581400 | loss: 0.4523\n",
      "step: 581600 | loss: 0.4517\n",
      "step: 581800 | loss: 0.4525\n",
      "step: 582000 | loss: 0.5395\n",
      "step: 582200 | loss: 0.4872\n",
      "step: 582400 | loss: 0.4566\n",
      "step: 582600 | loss: 0.4962\n",
      "step: 582800 | loss: 0.5475\n",
      "step: 583000 | loss: 0.4546\n",
      "step: 583200 | loss: 0.4471\n",
      "step: 583400 | loss: 0.5779\n",
      "step: 583600 | loss: 0.5556\n",
      "step: 583800 | loss: 0.3890\n",
      "step: 584000 | loss: 0.4227\n",
      "step: 584200 | loss: 0.5259\n",
      "step: 584400 | loss: 0.5437\n",
      "step: 584600 | loss: 0.4981\n",
      "step: 584800 | loss: 0.4047\n",
      "step: 585000 | loss: 0.5173\n",
      "step: 585200 | loss: 0.4974\n",
      "step: 585400 | loss: 0.4656\n",
      "step: 585600 | loss: 0.4021\n",
      "step: 585800 | loss: 0.4970\n",
      "step: 586000 | loss: 0.4279\n",
      "step: 586200 | loss: 0.4095\n",
      "step: 586400 | loss: 0.3435\n",
      "step: 586600 | loss: 0.3692\n",
      "step: 586800 | loss: 0.5187\n",
      "step: 587000 | loss: 0.4864\n",
      "step: 587200 | loss: 0.4490\n",
      "step: 587400 | loss: 0.4457\n",
      "step: 587600 | loss: 0.4633\n",
      "step: 587800 | loss: 0.4342\n",
      "step: 588000 | loss: 0.4123\n",
      "step: 588200 | loss: 0.5720\n",
      "step: 588400 | loss: 0.6405\n",
      "step: 588600 | loss: 0.5464\n",
      "step: 588800 | loss: 0.4321\n",
      "step: 589000 | loss: 0.4219\n",
      "step: 589200 | loss: 0.5015\n",
      "step: 589400 | loss: 0.4769\n",
      "step: 589600 | loss: 0.4102\n",
      "step: 589800 | loss: 0.4158\n",
      "step: 590000 | loss: 0.4887\n",
      "step: 590200 | loss: 0.4203\n",
      "step: 590400 | loss: 0.4592\n",
      "step: 590600 | loss: 0.3655\n",
      "step: 590800 | loss: 0.3707\n",
      "step: 591000 | loss: 0.5098\n",
      "step: 591200 | loss: 0.4200\n",
      "step: 591400 | loss: 0.4696\n",
      "step: 591600 | loss: 0.6358\n",
      "step: 591800 | loss: 0.5172\n",
      "step: 592000 | loss: 0.4709\n",
      "step: 592200 | loss: 0.4961\n",
      "step: 592400 | loss: 0.4112\n",
      "step: 592600 | loss: 0.4405\n",
      "step: 592800 | loss: 0.4087\n",
      "step: 593000 | loss: 0.4555\n",
      "step: 593200 | loss: 0.3849\n",
      "step: 593400 | loss: 0.3190\n",
      "step: 593600 | loss: 0.3419\n",
      "step: 593800 | loss: 0.3771\n",
      "step: 594000 | loss: 0.3453\n",
      "step: 594200 | loss: 0.2954\n",
      "step: 594400 | loss: 0.3156\n",
      "step: 594600 | loss: 0.2849\n",
      "step: 594800 | loss: 0.4223\n",
      "step: 595000 | loss: 0.4282\n",
      "step: 595200 | loss: 0.4925\n",
      "step: 595400 | loss: 0.4429\n",
      "step: 595600 | loss: 0.4592\n",
      "step: 595800 | loss: 0.3627\n",
      "step: 596000 | loss: 0.3230\n",
      "step: 596200 | loss: 0.3258\n",
      "step: 596400 | loss: 0.3459\n",
      "step: 596600 | loss: 0.3674\n",
      "step: 596800 | loss: 0.3404\n",
      "step: 597000 | loss: 0.3163\n",
      "step: 597200 | loss: 0.3922\n",
      "step: 597400 | loss: 0.3438\n",
      "step: 597600 | loss: 0.4164\n",
      "step: 597800 | loss: 0.4287\n",
      "step: 598000 | loss: 0.5161\n",
      "step: 598200 | loss: 0.4915\n",
      "step: 598400 | loss: 0.5326\n",
      "step: 598600 | loss: 0.6456\n",
      "step: 598800 | loss: 0.4620\n",
      "step: 599000 | loss: 0.4689\n",
      "step: 599200 | loss: 0.5965\n",
      "step: 599400 | loss: 0.5050\n",
      "step: 599600 | loss: 0.5476\n",
      "step: 599800 | loss: 0.4434\n",
      "step: 600000 | loss: 0.4365\n",
      "step: 600200 | loss: 0.6001\n",
      "step: 600400 | loss: 0.4984\n",
      "step: 600600 | loss: 0.5944\n",
      "step: 600800 | loss: 0.5049\n",
      "step: 601000 | loss: 0.4101\n",
      "step: 601200 | loss: 0.5682\n",
      "step: 601400 | loss: 0.7889\n",
      "step: 601600 | loss: 0.5781\n",
      "step: 601800 | loss: 0.5235\n",
      "step: 602000 | loss: 0.4898\n",
      "step: 602200 | loss: 0.5593\n",
      "step: 602400 | loss: 0.4036\n",
      "step: 602600 | loss: 0.5536\n",
      "step: 602800 | loss: 0.5398\n",
      "step: 603000 | loss: 0.4871\n",
      "step: 603200 | loss: 0.5986\n",
      "step: 603400 | loss: 0.5404\n",
      "step: 603600 | loss: 0.5048\n",
      "step: 603800 | loss: 0.5336\n",
      "step: 604000 | loss: 0.5451\n",
      "step: 604200 | loss: 0.5146\n",
      "step: 604400 | loss: 0.5567\n",
      "step: 604600 | loss: 0.5212\n",
      "step: 604800 | loss: 0.4582\n",
      "step: 605000 | loss: 0.4507\n",
      "step: 605200 | loss: 0.3833\n",
      "step: 605400 | loss: 0.5231\n",
      "step: 605600 | loss: 0.4250\n",
      "step: 605800 | loss: 0.4030\n",
      "step: 606000 | loss: 0.4655\n",
      "step: 606200 | loss: 0.5365\n",
      "step: 606400 | loss: 0.5163\n",
      "step: 606600 | loss: 0.5340\n",
      "step: 606800 | loss: 0.4440\n",
      "step: 607000 | loss: 0.4803\n",
      "step: 607200 | loss: 0.4060\n",
      "step: 607400 | loss: 0.4315\n",
      "step: 607600 | loss: 0.3351\n",
      "step: 607800 | loss: 0.4043\n",
      "step: 608000 | loss: 0.4627\n",
      "step: 608200 | loss: 0.4445\n",
      "step: 608400 | loss: 0.3675\n",
      "step: 608600 | loss: 0.4861\n",
      "step: 608800 | loss: 0.4927\n",
      "step: 609000 | loss: 0.5278\n",
      "step: 609200 | loss: 0.4718\n",
      "step: 609400 | loss: 0.3961\n",
      "step: 609600 | loss: 0.4771\n",
      "step: 609800 | loss: 0.2887\n",
      "step: 610000 | loss: 0.4041\n",
      "step: 610200 | loss: 0.4603\n",
      "step: 610400 | loss: 0.3755\n",
      "step: 610600 | loss: 0.3937\n",
      "step: 610800 | loss: 0.4222\n",
      "step: 611000 | loss: 0.4270\n",
      "step: 611200 | loss: 0.4225\n",
      "step: 611400 | loss: 0.3905\n",
      "step: 611600 | loss: 0.5416\n",
      "step: 611800 | loss: 0.4330\n",
      "step: 612000 | loss: 0.4032\n",
      "step: 612200 | loss: 0.3560\n",
      "step: 612400 | loss: 0.3580\n",
      "step: 612600 | loss: 0.3410\n",
      "step: 612800 | loss: 0.3374\n",
      "step: 613000 | loss: 0.3719\n",
      "step: 613200 | loss: 0.4702\n",
      "step: 613400 | loss: 0.3303\n",
      "step: 613600 | loss: 0.3653\n",
      "step: 613800 | loss: 0.3516\n",
      "step: 614000 | loss: 0.3165\n",
      "step: 614200 | loss: 0.3085\n",
      "step: 614400 | loss: 0.3594\n",
      "step: 614600 | loss: 0.3958\n",
      "step: 614800 | loss: 0.4377\n",
      "step: 615000 | loss: 0.4102\n",
      "step: 615200 | loss: 0.4664\n",
      "step: 615400 | loss: 0.4309\n",
      "step: 615600 | loss: 0.4342\n",
      "step: 615800 | loss: 0.5537\n",
      "step: 616000 | loss: 0.4668\n",
      "step: 616200 | loss: 0.4320\n",
      "step: 616400 | loss: 0.4146\n",
      "step: 616600 | loss: 0.4747\n",
      "step: 616800 | loss: 0.4994\n",
      "step: 617000 | loss: 0.5324\n",
      "step: 617200 | loss: 0.3890\n",
      "step: 617400 | loss: 0.5224\n",
      "step: 617600 | loss: 0.4512\n",
      "step: 617800 | loss: 0.4094\n",
      "step: 618000 | loss: 0.3638\n",
      "step: 618200 | loss: 0.4295\n",
      "step: 618400 | loss: 0.4189\n",
      "step: 618600 | loss: 0.3861\n",
      "step: 618800 | loss: 0.4081\n",
      "step: 619000 | loss: 0.4014\n",
      "step: 619200 | loss: 0.3745\n",
      "step: 619400 | loss: 0.4548\n",
      "step: 619600 | loss: 0.4756\n",
      "step: 619800 | loss: 0.3770\n",
      "step: 620000 | loss: 0.3318\n",
      "step: 620200 | loss: 0.4904\n",
      "step: 620400 | loss: 0.3802\n",
      "step: 620600 | loss: 0.3917\n",
      "step: 620800 | loss: 0.4366\n",
      "step: 621000 | loss: 0.4283\n",
      "step: 621200 | loss: 0.4390\n",
      "step: 621400 | loss: 0.3275\n",
      "step: 621600 | loss: 0.3590\n",
      "step: 621800 | loss: 0.2882\n",
      "step: 622000 | loss: 0.3640\n",
      "step: 622200 | loss: 0.4057\n",
      "step: 622400 | loss: 0.4374\n",
      "step: 622600 | loss: 0.3217\n",
      "step: 622800 | loss: 0.3477\n",
      "step: 623000 | loss: 0.3330\n",
      "step: 623200 | loss: 0.3782\n",
      "step: 623400 | loss: 0.3940\n",
      "step: 623600 | loss: 0.3629\n",
      "step: 623800 | loss: 0.3190\n",
      "step: 624000 | loss: 0.3187\n",
      "step: 624200 | loss: 0.3922\n",
      "step: 624400 | loss: 0.3975\n",
      "step: 624600 | loss: 0.3280\n",
      "step: 624800 | loss: 0.3084\n",
      "step: 625000 | loss: 0.3548\n",
      "step: 625200 | loss: 0.4074\n",
      "step: 625400 | loss: 0.3711\n",
      "step: 625600 | loss: 0.3215\n",
      "step: 625800 | loss: 0.3708\n",
      "step: 626000 | loss: 0.3335\n",
      "step: 626200 | loss: 0.4212\n",
      "step: 626400 | loss: 0.4320\n",
      "step: 626600 | loss: 0.3518\n",
      "step: 626800 | loss: 0.3787\n",
      "step: 627000 | loss: 0.4451\n",
      "step: 627200 | loss: 0.3984\n",
      "step: 627400 | loss: 0.3636\n",
      "step: 627600 | loss: 0.3510\n",
      "step: 627800 | loss: 0.3982\n",
      "step: 628000 | loss: 0.3426\n",
      "step: 628200 | loss: 0.3642\n",
      "step: 628400 | loss: 0.3472\n",
      "step: 628600 | loss: 0.3705\n",
      "step: 628800 | loss: 0.3987\n",
      "step: 629000 | loss: 0.4581\n",
      "step: 629200 | loss: 0.3707\n",
      "step: 629400 | loss: 0.4002\n",
      "step: 629600 | loss: 0.3806\n",
      "step: 629800 | loss: 0.3287\n",
      "step: 630000 | loss: 0.3561\n",
      "step: 630200 | loss: 0.3698\n",
      "step: 630400 | loss: 0.3219\n",
      "step: 630600 | loss: 0.4016\n",
      "step: 630800 | loss: 0.5467\n",
      "step: 631000 | loss: 0.6072\n",
      "step: 631200 | loss: 0.4673\n",
      "step: 631400 | loss: 0.5220\n",
      "step: 631600 | loss: 0.5336\n",
      "step: 631800 | loss: 0.6086\n",
      "step: 632000 | loss: 0.5106\n",
      "step: 632200 | loss: 0.5257\n",
      "step: 632400 | loss: 0.4879\n",
      "step: 632600 | loss: 0.4763\n",
      "step: 632800 | loss: 0.5638\n",
      "step: 633000 | loss: 0.5148\n",
      "step: 633200 | loss: 0.4682\n",
      "step: 633400 | loss: 0.4931\n",
      "step: 633600 | loss: 0.4657\n",
      "step: 633800 | loss: 0.4494\n",
      "step: 634000 | loss: 0.5747\n",
      "step: 634200 | loss: 0.5933\n",
      "step: 634400 | loss: 0.4214\n",
      "step: 634600 | loss: 0.4440\n",
      "step: 634800 | loss: 0.5207\n",
      "step: 635000 | loss: 0.4744\n",
      "step: 635200 | loss: 0.5273\n",
      "step: 635400 | loss: 0.4132\n",
      "step: 635600 | loss: 0.5357\n",
      "step: 635800 | loss: 0.4701\n",
      "step: 636000 | loss: 0.5491\n",
      "step: 636200 | loss: 0.5032\n",
      "step: 636400 | loss: 0.4905\n",
      "step: 636600 | loss: 0.3936\n",
      "step: 636800 | loss: 0.4519\n",
      "step: 637000 | loss: 0.5379\n",
      "step: 637200 | loss: 0.5258\n",
      "step: 637400 | loss: 0.7074\n",
      "step: 637600 | loss: 0.6113\n",
      "step: 637800 | loss: 0.5896\n",
      "step: 638000 | loss: 0.6112\n",
      "step: 638200 | loss: 0.5929\n",
      "step: 638400 | loss: 0.5370\n",
      "step: 638600 | loss: 0.5744\n",
      "step: 638800 | loss: 0.5804\n",
      "step: 639000 | loss: 0.5252\n",
      "step: 639200 | loss: 0.5229\n",
      "step: 639400 | loss: 0.6021\n",
      "step: 639600 | loss: 0.6145\n",
      "step: 639800 | loss: 0.6643\n",
      "step: 640000 | loss: 0.6270\n",
      "step: 640200 | loss: 0.5495\n",
      "step: 640400 | loss: 0.6522\n",
      "step: 640600 | loss: 0.5839\n",
      "step: 640800 | loss: 0.6743\n",
      "step: 641000 | loss: 0.5212\n",
      "step: 641200 | loss: 0.5837\n",
      "step: 641400 | loss: 0.6014\n",
      "step: 641600 | loss: 0.6114\n",
      "step: 641800 | loss: 0.5999\n",
      "step: 642000 | loss: 0.6182\n",
      "step: 642200 | loss: 0.4892\n",
      "step: 642400 | loss: 0.4632\n",
      "step: 642600 | loss: 0.4579\n",
      "step: 642800 | loss: 0.4110\n",
      "step: 643000 | loss: 0.4615\n",
      "step: 643200 | loss: 0.5067\n",
      "step: 643400 | loss: 0.5238\n",
      "step: 643600 | loss: 0.5492\n",
      "step: 643800 | loss: 0.5890\n",
      "step: 644000 | loss: 0.5795\n",
      "step: 644200 | loss: 0.5303\n",
      "step: 644400 | loss: 0.6033\n",
      "step: 644600 | loss: 0.4848\n",
      "step: 644800 | loss: 0.5823\n",
      "step: 645000 | loss: 0.6256\n",
      "step: 645200 | loss: 0.4700\n",
      "step: 645400 | loss: 0.6127\n",
      "step: 645600 | loss: 0.4603\n",
      "step: 645800 | loss: 0.5770\n",
      "step: 646000 | loss: 0.5115\n",
      "step: 646200 | loss: 0.4456\n",
      "step: 646400 | loss: 0.5293\n",
      "step: 646600 | loss: 0.4260\n",
      "step: 646800 | loss: 0.6798\n",
      "step: 647000 | loss: 0.5241\n",
      "step: 647200 | loss: 0.5850\n",
      "step: 647400 | loss: 0.5055\n",
      "step: 647600 | loss: 0.5617\n",
      "step: 647800 | loss: 0.5686\n",
      "step: 648000 | loss: 0.5781\n",
      "step: 648200 | loss: 0.5552\n",
      "step: 648400 | loss: 0.5006\n",
      "step: 648600 | loss: 0.4938\n",
      "step: 648800 | loss: 0.5345\n",
      "step: 649000 | loss: 0.4553\n",
      "step: 649200 | loss: 0.5039\n",
      "step: 649400 | loss: 0.4705\n",
      "step: 649600 | loss: 0.6296\n",
      "step: 649800 | loss: 0.5017\n",
      "step: 650000 | loss: 0.4955\n",
      "step: 650200 | loss: 0.4854\n",
      "step: 650400 | loss: 0.6133\n",
      "step: 650600 | loss: 0.6136\n",
      "step: 650800 | loss: 0.4601\n",
      "step: 651000 | loss: 0.5025\n",
      "step: 651200 | loss: 0.5419\n",
      "step: 651400 | loss: 0.4759\n",
      "step: 651600 | loss: 0.5282\n",
      "step: 651800 | loss: 0.4826\n",
      "step: 652000 | loss: 0.5173\n",
      "step: 652200 | loss: 0.4899\n",
      "step: 652400 | loss: 0.5197\n",
      "step: 652600 | loss: 0.6088\n",
      "step: 652800 | loss: 0.4659\n",
      "step: 653000 | loss: 0.5624\n",
      "step: 653200 | loss: 0.5538\n",
      "step: 653400 | loss: 0.4537\n",
      "step: 653600 | loss: 0.5444\n",
      "step: 653800 | loss: 0.4771\n",
      "step: 654000 | loss: 0.4814\n",
      "step: 654200 | loss: 0.4552\n",
      "step: 654400 | loss: 0.4464\n",
      "step: 654600 | loss: 0.5680\n",
      "step: 654800 | loss: 0.4765\n",
      "step: 655000 | loss: 0.5036\n",
      "step: 655200 | loss: 0.4244\n",
      "step: 655400 | loss: 0.5199\n",
      "step: 655600 | loss: 0.4325\n",
      "step: 655800 | loss: 0.4759\n",
      "step: 656000 | loss: 0.4173\n",
      "step: 656200 | loss: 0.3646\n",
      "step: 656400 | loss: 0.4487\n",
      "step: 656600 | loss: 0.4339\n",
      "step: 656800 | loss: 0.4544\n",
      "step: 657000 | loss: 0.6497\n",
      "step: 657200 | loss: 0.6384\n",
      "step: 657400 | loss: 0.4716\n",
      "step: 657600 | loss: 0.5227\n",
      "step: 657800 | loss: 0.5016\n",
      "step: 658000 | loss: 0.3955\n",
      "step: 658200 | loss: 0.4115\n",
      "step: 658400 | loss: 0.4565\n",
      "step: 658600 | loss: 0.4857\n",
      "step: 658800 | loss: 0.4567\n",
      "step: 659000 | loss: 0.4821\n",
      "step: 659200 | loss: 0.4872\n",
      "step: 659400 | loss: 0.4242\n",
      "step: 659600 | loss: 0.4190\n",
      "step: 659800 | loss: 0.4157\n",
      "step: 660000 | loss: 0.3600\n",
      "step: 660200 | loss: 0.4726\n",
      "step: 660400 | loss: 0.4770\n",
      "step: 660600 | loss: 0.3503\n",
      "step: 660800 | loss: 0.4431\n",
      "step: 661000 | loss: 0.3959\n",
      "step: 661200 | loss: 0.3909\n",
      "step: 661400 | loss: 0.3671\n",
      "step: 661600 | loss: 0.3915\n",
      "step: 661800 | loss: 0.3649\n",
      "step: 662000 | loss: 0.4498\n",
      "step: 662200 | loss: 0.4458\n",
      "step: 662400 | loss: 0.4804\n",
      "step: 662600 | loss: 0.3429\n",
      "step: 662800 | loss: 0.3363\n",
      "step: 663000 | loss: 0.4393\n",
      "step: 663200 | loss: 0.3345\n",
      "step: 663400 | loss: 0.3960\n",
      "step: 663600 | loss: 0.6198\n",
      "step: 663800 | loss: 0.4943\n",
      "step: 664000 | loss: 0.4557\n",
      "step: 664200 | loss: 0.3690\n",
      "step: 664400 | loss: 0.3334\n",
      "step: 664600 | loss: 0.3105\n",
      "step: 664800 | loss: 0.3064\n",
      "step: 665000 | loss: 0.4007\n",
      "step: 665200 | loss: 0.3339\n",
      "step: 665400 | loss: 0.3425\n",
      "step: 665600 | loss: 0.3367\n",
      "step: 665800 | loss: 0.3678\n",
      "step: 666000 | loss: 0.3038\n",
      "step: 666200 | loss: 0.3265\n",
      "step: 666400 | loss: 0.2943\n",
      "step: 666600 | loss: 0.3388\n",
      "step: 666800 | loss: 0.3028\n",
      "step: 667000 | loss: 0.3105\n",
      "step: 667200 | loss: 0.3592\n",
      "step: 667400 | loss: 0.3350\n",
      "step: 667600 | loss: 0.2958\n",
      "step: 667800 | loss: 0.3433\n",
      "step: 668000 | loss: 0.3048\n",
      "step: 668200 | loss: 0.3611\n",
      "step: 668400 | loss: 0.3504\n",
      "step: 668600 | loss: 0.3526\n",
      "step: 668800 | loss: 0.2828\n",
      "step: 669000 | loss: 0.2977\n",
      "step: 669200 | loss: 0.2711\n",
      "step: 669400 | loss: 0.2861\n",
      "step: 669600 | loss: 0.3135\n",
      "step: 669800 | loss: 0.3267\n",
      "step: 670000 | loss: 0.3451\n",
      "step: 670200 | loss: 0.3300\n",
      "step: 670400 | loss: 0.2911\n",
      "step: 670600 | loss: 0.3506\n",
      "step: 670800 | loss: 0.3011\n",
      "step: 671000 | loss: 0.2544\n",
      "step: 671200 | loss: 0.3431\n",
      "step: 671400 | loss: 0.3280\n",
      "step: 671600 | loss: 0.2745\n",
      "step: 671800 | loss: 0.2774\n",
      "step: 672000 | loss: 0.2485\n",
      "step: 672200 | loss: 0.3309\n",
      "step: 672400 | loss: 0.3102\n",
      "step: 672600 | loss: 0.3204\n",
      "step: 672800 | loss: 0.3420\n",
      "step: 673000 | loss: 0.2799\n",
      "step: 673200 | loss: 0.4108\n",
      "step: 673400 | loss: 0.4811\n",
      "step: 673600 | loss: 0.3964\n",
      "step: 673800 | loss: 0.4105\n",
      "step: 674000 | loss: 0.3656\n",
      "step: 674200 | loss: 0.3964\n",
      "step: 674400 | loss: 0.3429\n",
      "step: 674600 | loss: 0.3910\n",
      "step: 674800 | loss: 0.4212\n",
      "step: 675000 | loss: 0.4018\n",
      "step: 675200 | loss: 0.3878\n",
      "step: 675400 | loss: 0.4368\n",
      "step: 675600 | loss: 0.3645\n",
      "step: 675800 | loss: 0.3818\n",
      "step: 676000 | loss: 0.3713\n",
      "step: 676200 | loss: 0.5094\n",
      "step: 676400 | loss: 0.3835\n",
      "step: 676600 | loss: 0.4447\n",
      "step: 676800 | loss: 0.3564\n",
      "step: 677000 | loss: 0.3680\n",
      "step: 677200 | loss: 0.3234\n",
      "step: 677400 | loss: 0.3073\n",
      "step: 677600 | loss: 0.3347\n",
      "step: 677800 | loss: 0.3669\n",
      "step: 678000 | loss: 0.4303\n",
      "step: 678200 | loss: 0.3701\n",
      "step: 678400 | loss: 0.4391\n",
      "step: 678600 | loss: 0.4097\n",
      "step: 678800 | loss: 0.4064\n",
      "step: 679000 | loss: 0.4286\n",
      "step: 679200 | loss: 0.3874\n",
      "step: 679400 | loss: 0.3708\n",
      "step: 679600 | loss: 0.3336\n",
      "step: 679800 | loss: 0.3467\n",
      "step: 680000 | loss: 0.3594\n",
      "step: 680200 | loss: 0.3754\n",
      "step: 680400 | loss: 0.3814\n",
      "step: 680600 | loss: 0.3726\n",
      "step: 680800 | loss: 0.3917\n",
      "step: 681000 | loss: 0.3752\n",
      "step: 681200 | loss: 0.3657\n",
      "step: 681400 | loss: 0.3696\n",
      "step: 681600 | loss: 0.3227\n",
      "step: 681800 | loss: 0.3408\n",
      "step: 682000 | loss: 0.3536\n",
      "step: 682200 | loss: 0.3778\n",
      "step: 682400 | loss: 0.4255\n",
      "step: 682600 | loss: 0.4221\n",
      "step: 682800 | loss: 0.5005\n",
      "step: 683000 | loss: 0.3678\n",
      "step: 683200 | loss: 0.6459\n",
      "step: 683400 | loss: 0.7264\n",
      "step: 683600 | loss: 0.6150\n",
      "step: 683800 | loss: 0.6995\n",
      "step: 684000 | loss: 0.5297\n",
      "step: 684200 | loss: 0.6232\n",
      "step: 684400 | loss: 0.5643\n",
      "step: 684600 | loss: 0.5758\n",
      "step: 684800 | loss: 0.4705\n",
      "step: 685000 | loss: 0.4488\n",
      "step: 685200 | loss: 0.5562\n",
      "step: 685400 | loss: 0.5940\n",
      "step: 685600 | loss: 0.5497\n",
      "step: 685800 | loss: 0.5038\n",
      "step: 686000 | loss: 0.4433\n",
      "step: 686200 | loss: 0.5708\n",
      "step: 686400 | loss: 0.5486\n",
      "step: 686600 | loss: 0.7696\n",
      "step: 686800 | loss: 0.6525\n",
      "step: 687000 | loss: 0.7014\n",
      "step: 687200 | loss: 0.6216\n",
      "step: 687400 | loss: 0.5405\n",
      "step: 687600 | loss: 0.6996\n",
      "step: 687800 | loss: 0.6680\n",
      "step: 688000 | loss: 0.7319\n",
      "step: 688200 | loss: 0.5250\n",
      "step: 688400 | loss: 0.6432\n",
      "step: 688600 | loss: 0.6569\n",
      "step: 688800 | loss: 0.6431\n",
      "step: 689000 | loss: 0.5461\n",
      "step: 689200 | loss: 0.5804\n",
      "step: 689400 | loss: 0.5836\n",
      "step: 689600 | loss: 0.6647\n",
      "step: 689800 | loss: 0.6024\n",
      "step: 690000 | loss: 0.4628\n",
      "step: 690200 | loss: 0.5108\n",
      "step: 690400 | loss: 0.4652\n",
      "step: 690600 | loss: 0.4592\n",
      "step: 690800 | loss: 0.4837\n",
      "step: 691000 | loss: 0.4596\n",
      "step: 691200 | loss: 0.3990\n",
      "step: 691400 | loss: 0.4385\n",
      "step: 691600 | loss: 0.4052\n",
      "step: 691800 | loss: 0.3908\n",
      "step: 692000 | loss: 0.4341\n",
      "step: 692200 | loss: 0.5573\n",
      "step: 692400 | loss: 0.3951\n",
      "step: 692600 | loss: 0.4041\n",
      "step: 692800 | loss: 0.4252\n",
      "step: 693000 | loss: 0.3879\n",
      "step: 693200 | loss: 0.5020\n",
      "step: 693400 | loss: 0.4227\n",
      "step: 693600 | loss: 0.5016\n",
      "step: 693800 | loss: 0.4499\n",
      "step: 694000 | loss: 0.5183\n",
      "step: 694200 | loss: 0.4274\n",
      "step: 694400 | loss: 0.4618\n",
      "step: 694600 | loss: 0.4774\n",
      "step: 694800 | loss: 0.3478\n",
      "step: 695000 | loss: 0.4287\n",
      "step: 695200 | loss: 0.4301\n",
      "step: 695400 | loss: 0.4325\n",
      "step: 695600 | loss: 0.3592\n",
      "step: 695800 | loss: 0.4232\n",
      "step: 696000 | loss: 0.4120\n",
      "step: 696200 | loss: 0.3865\n",
      "step: 696400 | loss: 0.5063\n",
      "step: 696600 | loss: 0.4544\n",
      "step: 696800 | loss: 0.4710\n",
      "step: 697000 | loss: 0.4056\n",
      "step: 697200 | loss: 0.3651\n",
      "step: 697400 | loss: 0.4545\n",
      "step: 697600 | loss: 0.3289\n",
      "step: 697800 | loss: 0.3542\n",
      "step: 698000 | loss: 0.3471\n",
      "step: 698200 | loss: 0.3960\n",
      "step: 698400 | loss: 0.3472\n",
      "step: 698600 | loss: 0.3620\n",
      "step: 698800 | loss: 0.4160\n",
      "step: 699000 | loss: 0.3908\n",
      "step: 699200 | loss: 0.4233\n",
      "step: 699400 | loss: 0.3137\n",
      "step: 699600 | loss: 0.3870\n",
      "step: 699800 | loss: 0.3512\n",
      "step: 700000 | loss: 0.2840\n",
      "step: 700200 | loss: 0.2835\n",
      "step: 700400 | loss: 0.3544\n",
      "step: 700600 | loss: 0.3210\n",
      "step: 700800 | loss: 0.3232\n",
      "step: 701000 | loss: 0.3030\n",
      "step: 701200 | loss: 0.2922\n",
      "step: 701400 | loss: 0.3023\n",
      "step: 701600 | loss: 0.2853\n",
      "step: 701800 | loss: 0.3436\n",
      "step: 702000 | loss: 0.3652\n",
      "step: 702200 | loss: 0.3798\n",
      "step: 702400 | loss: 0.3128\n",
      "step: 702600 | loss: 0.3238\n",
      "step: 702800 | loss: 0.2884\n",
      "step: 703000 | loss: 0.4220\n",
      "step: 703200 | loss: 0.3742\n",
      "step: 703400 | loss: 0.4016\n",
      "step: 703600 | loss: 0.3754\n",
      "step: 703800 | loss: 0.3927\n",
      "step: 704000 | loss: 0.3876\n",
      "step: 704200 | loss: 0.4431\n",
      "step: 704400 | loss: 0.4067\n",
      "step: 704600 | loss: 0.3629\n",
      "step: 704800 | loss: 0.3667\n",
      "step: 705000 | loss: 0.4418\n",
      "step: 705200 | loss: 0.5358\n",
      "step: 705400 | loss: 0.4101\n",
      "step: 705600 | loss: 0.4600\n",
      "step: 705800 | loss: 0.5531\n",
      "step: 706000 | loss: 0.5785\n",
      "step: 706200 | loss: 0.5695\n",
      "step: 706400 | loss: 0.5483\n",
      "step: 706600 | loss: 0.5195\n",
      "step: 706800 | loss: 0.5176\n",
      "step: 707000 | loss: 0.4777\n",
      "step: 707200 | loss: 0.5122\n",
      "step: 707400 | loss: 0.4985\n",
      "step: 707600 | loss: 0.5001\n",
      "step: 707800 | loss: 0.4495\n",
      "step: 708000 | loss: 0.4334\n",
      "step: 708200 | loss: 0.5036\n",
      "step: 708400 | loss: 0.4066\n",
      "step: 708600 | loss: 0.4315\n",
      "step: 708800 | loss: 0.5416\n",
      "step: 709000 | loss: 0.4750\n",
      "step: 709200 | loss: 0.4972\n",
      "step: 709400 | loss: 0.5674\n",
      "step: 709600 | loss: 0.5495\n",
      "step: 709800 | loss: 0.5296\n",
      "step: 710000 | loss: 0.6763\n",
      "step: 710200 | loss: 0.5228\n",
      "step: 710400 | loss: 0.5474\n",
      "step: 710600 | loss: 0.4042\n",
      "step: 710800 | loss: 0.5319\n",
      "step: 711000 | loss: 0.5065\n",
      "step: 711200 | loss: 0.4937\n",
      "step: 711400 | loss: 0.4342\n",
      "step: 711600 | loss: 0.4734\n",
      "step: 711800 | loss: 0.4604\n",
      "step: 712000 | loss: 0.5199\n",
      "step: 712200 | loss: 0.4932\n",
      "step: 712400 | loss: 0.4685\n",
      "step: 712600 | loss: 0.4519\n",
      "step: 712800 | loss: 0.5066\n",
      "step: 713000 | loss: 0.5196\n",
      "step: 713200 | loss: 0.3980\n",
      "step: 713400 | loss: 0.3987\n",
      "step: 713600 | loss: 0.3587\n",
      "step: 713800 | loss: 0.4089\n",
      "step: 714000 | loss: 0.4224\n",
      "step: 714200 | loss: 0.3989\n",
      "step: 714400 | loss: 0.4533\n",
      "step: 714600 | loss: 0.4073\n",
      "step: 714800 | loss: 0.4311\n",
      "step: 715000 | loss: 0.4333\n",
      "step: 715200 | loss: 0.4588\n",
      "step: 715400 | loss: 0.4484\n",
      "step: 715600 | loss: 0.3807\n",
      "step: 715800 | loss: 0.4807\n",
      "step: 716000 | loss: 0.4529\n",
      "step: 716200 | loss: 0.4378\n",
      "step: 716400 | loss: 0.5045\n",
      "step: 716600 | loss: 0.4501\n",
      "step: 716800 | loss: 0.4083\n",
      "step: 717000 | loss: 0.4446\n",
      "step: 717200 | loss: 0.3880\n",
      "step: 717400 | loss: 0.4283\n",
      "step: 717600 | loss: 0.4142\n",
      "step: 717800 | loss: 0.4203\n",
      "step: 718000 | loss: 0.5308\n",
      "step: 718200 | loss: 0.4722\n",
      "step: 718400 | loss: 0.3741\n",
      "step: 718600 | loss: 0.4886\n",
      "step: 718800 | loss: 0.3943\n",
      "step: 719000 | loss: 0.4260\n",
      "step: 719200 | loss: 0.4916\n",
      "step: 719400 | loss: 0.2923\n",
      "step: 719600 | loss: 0.4570\n",
      "step: 719800 | loss: 0.4620\n",
      "step: 720000 | loss: 0.4179\n",
      "step: 720200 | loss: 0.3602\n",
      "step: 720400 | loss: 0.3331\n",
      "step: 720600 | loss: 0.3549\n",
      "step: 720800 | loss: 0.3396\n",
      "step: 721000 | loss: 0.3819\n",
      "step: 721200 | loss: 0.3686\n",
      "step: 721400 | loss: 0.3941\n",
      "step: 721600 | loss: 0.3561\n",
      "step: 721800 | loss: 0.4966\n",
      "step: 722000 | loss: 0.4897\n",
      "step: 722200 | loss: 0.4468\n",
      "step: 722400 | loss: 0.4144\n",
      "step: 722600 | loss: 0.4851\n",
      "step: 722800 | loss: 0.5173\n",
      "step: 723000 | loss: 0.5644\n",
      "step: 723200 | loss: 0.4055\n",
      "step: 723400 | loss: 0.4152\n",
      "step: 723600 | loss: 0.4211\n",
      "step: 723800 | loss: 0.4139\n",
      "step: 724000 | loss: 0.3353\n",
      "step: 724200 | loss: 0.3877\n",
      "step: 724400 | loss: 0.3960\n",
      "step: 724600 | loss: 0.3761\n",
      "step: 724800 | loss: 0.4160\n",
      "step: 725000 | loss: 0.5428\n",
      "step: 725200 | loss: 0.3189\n",
      "step: 725400 | loss: 0.3541\n",
      "step: 725600 | loss: 0.3760\n",
      "step: 725800 | loss: 0.4224\n",
      "step: 726000 | loss: 0.4603\n",
      "step: 726200 | loss: 0.4724\n",
      "step: 726400 | loss: 0.4436\n",
      "step: 726600 | loss: 0.4122\n",
      "step: 726800 | loss: 0.4024\n",
      "step: 727000 | loss: 0.5045\n",
      "step: 727200 | loss: 0.4595\n",
      "step: 727400 | loss: 0.4828\n",
      "step: 727600 | loss: 0.5239\n",
      "step: 727800 | loss: 0.4814\n",
      "step: 728000 | loss: 0.4907\n",
      "step: 728200 | loss: 0.4327\n",
      "step: 728400 | loss: 0.3934\n",
      "step: 728600 | loss: 0.5098\n",
      "step: 728800 | loss: 0.4307\n",
      "step: 729000 | loss: 0.4096\n",
      "step: 729200 | loss: 0.5169\n",
      "step: 729400 | loss: 0.6778\n",
      "step: 729600 | loss: 0.5093\n",
      "step: 729800 | loss: 0.5292\n",
      "step: 730000 | loss: 0.6248\n",
      "step: 730200 | loss: 0.6547\n",
      "step: 730400 | loss: 0.6156\n",
      "step: 730600 | loss: 0.7065\n",
      "step: 730800 | loss: 0.4737\n",
      "step: 731000 | loss: 0.5618\n",
      "step: 731200 | loss: 0.6681\n",
      "step: 731400 | loss: 0.6438\n",
      "step: 731600 | loss: 0.6345\n",
      "step: 731800 | loss: 0.5856\n",
      "step: 732000 | loss: 0.7667\n",
      "step: 732200 | loss: 0.6357\n",
      "step: 732400 | loss: 0.6401\n",
      "step: 732600 | loss: 0.5478\n",
      "step: 732800 | loss: 0.7486\n",
      "step: 733000 | loss: 0.5478\n",
      "step: 733200 | loss: 0.6578\n",
      "step: 733400 | loss: 0.5809\n",
      "step: 733600 | loss: 0.6415\n",
      "step: 733800 | loss: 0.5521\n",
      "step: 734000 | loss: 0.6132\n",
      "step: 734200 | loss: 0.4828\n",
      "step: 734400 | loss: 0.6775\n",
      "step: 734600 | loss: 0.5363\n",
      "step: 734800 | loss: 0.4945\n",
      "step: 735000 | loss: 0.6178\n",
      "step: 735200 | loss: 0.5939\n",
      "step: 735400 | loss: 0.5116\n",
      "step: 735600 | loss: 0.5768\n",
      "step: 735800 | loss: 0.6050\n",
      "step: 736000 | loss: 0.6319\n",
      "step: 736200 | loss: 0.5261\n",
      "step: 736400 | loss: 0.5851\n",
      "step: 736600 | loss: 0.4970\n",
      "step: 736800 | loss: 0.4835\n",
      "step: 737000 | loss: 0.5361\n",
      "step: 737200 | loss: 0.4644\n",
      "step: 737400 | loss: 0.5010\n",
      "step: 737600 | loss: 0.4883\n",
      "step: 737800 | loss: 0.5156\n",
      "step: 738000 | loss: 0.5247\n",
      "step: 738200 | loss: 0.5596\n",
      "step: 738400 | loss: 0.5509\n",
      "step: 738600 | loss: 0.4018\n",
      "step: 738800 | loss: 0.5689\n",
      "step: 739000 | loss: 0.5052\n",
      "step: 739200 | loss: 0.5197\n",
      "step: 739400 | loss: 0.5562\n",
      "step: 739600 | loss: 0.4426\n",
      "step: 739800 | loss: 0.4803\n",
      "step: 740000 | loss: 0.4422\n",
      "step: 740200 | loss: 0.4597\n",
      "step: 740400 | loss: 0.4184\n",
      "step: 740600 | loss: 0.5445\n"
     ]
    }
   ],
   "source": [
    "while step < cfg.train.total_steps:\n",
    "\n",
    "    # if step == 10_000:\n",
    "    #     task.flip_rate = future_flip_rate\n",
    "\n",
    "    # Generate batch of data\n",
    "    inputs, targets = next(task_iterator)\n",
    "\n",
    "    # Add noise to targets\n",
    "    if cfg.task.noise_std > 0:\n",
    "        targets += torch.randn_like(targets) * cfg.task.noise_std\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        standardized_targets, cumulant_stats = standardize_targets(targets, cumulant_stats)\n",
    "    \n",
    "    if cfg.train.standardize_cumulants:\n",
    "        targets = standardized_targets\n",
    "    target_buffer.extend(targets.view(-1).tolist())\n",
    "    \n",
    "    features, targets = inputs.to(cfg.device), targets.to(cfg.device)\n",
    "\n",
    "    # Reset weights and optimizer states for recycled features\n",
    "    if cbp_tracker is not None:\n",
    "        if log_pruning_stats:\n",
    "            pre_prune_utilities = cbp_tracker.get_statistics(prune_layer)['utility']\n",
    "\n",
    "        if isinstance(cbp_tracker, SignedCBPTracker):\n",
    "            pruned_idxs = cbp_tracker.prune_features(targets)\n",
    "        else:\n",
    "            pruned_idxs = cbp_tracker.prune_features()\n",
    "        n_pruned = sum([len(idxs) for idxs in pruned_idxs.values()])\n",
    "        total_pruned += n_pruned\n",
    "\n",
    "        if prune_layer in pruned_idxs and len(pruned_idxs[prune_layer]) > 0:\n",
    "            new_feature_idxs = pruned_idxs[prune_layer].tolist()\n",
    "            distractor_process_idxs = new_feature_idxs\n",
    "\n",
    "            # Don't turn bias into a distractor\n",
    "            if use_bias:\n",
    "                distractor_process_idxs = [idx for idx in distractor_process_idxs if idx != 0]\n",
    "\n",
    "            # Turn some features into distractors\n",
    "            distractor_tracker.process_new_features(distractor_process_idxs)\n",
    "\n",
    "            # Log pruning statistics\n",
    "            pruned_accum += len(new_feature_idxs)\n",
    "            n_new_pruned_features = len(set(new_feature_idxs).intersection(prev_pruned_idxs))\n",
    "            pruned_newest_feature_accum += n_new_pruned_features\n",
    "            prev_pruned_idxs = set(new_feature_idxs)\n",
    "            \n",
    "            if log_pruning_stats:\n",
    "                prune_thresholds.append(pre_prune_utilities[new_feature_idxs].max().item())\n",
    "    \n",
    "    # Train step\n",
    "    if use_replay_buffer:\n",
    "        samples = replay_buffer.sample(n_replay_steps)\n",
    "        samples += [(features, targets)]\n",
    "        for sample in samples:\n",
    "            loss, mean_pred_loss, effective_lr_accum = train_step(\n",
    "                model, criterion, optimizer, repr_optimizer, use_bias, cumulant_stats,\n",
    "                distractor_tracker, *sample, effective_lr_accum)\n",
    "        replay_buffer.append((features.clone(), targets.clone()))\n",
    "        \n",
    "    else:\n",
    "        losses = []\n",
    "        for _ in range(n_replay_steps):\n",
    "            loss, mean_pred_loss, effective_lr_accum = train_step(\n",
    "                model, criterion, optimizer, repr_optimizer, use_bias, cumulant_stats,\n",
    "                distractor_tracker, features, targets, effective_lr_accum)\n",
    "            losses.append(loss)\n",
    "        loss = losses[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Accumulate metrics\n",
    "    loss_accum += loss\n",
    "    cumulative_loss += loss\n",
    "    mean_pred_loss_accum += mean_pred_loss\n",
    "    n_steps_since_log += 1\n",
    "    \n",
    "    # Log metrics\n",
    "    if step % cfg.train.log_freq == 0:\n",
    "        n_distractors = distractor_tracker.distractor_mask.sum().item()\n",
    "        n_real_features = distractor_tracker.distractor_mask.numel() - n_distractors\n",
    "        metrics = {\n",
    "            'step': step,\n",
    "            'samples': step * cfg.train.batch_size,\n",
    "            'loss': loss_accum / n_steps_since_log,\n",
    "            'cumulative_loss': float(cumulative_loss),\n",
    "            'mean_prediction_loss': mean_pred_loss_accum / n_steps_since_log,\n",
    "            'squared_targets': torch.tensor(target_buffer).square().mean().item(),\n",
    "            'n_distractors': n_distractors,\n",
    "            'n_real_features': n_real_features,\n",
    "        }\n",
    "\n",
    "        if log_pruning_stats:\n",
    "            if pruned_accum > 0:\n",
    "                metrics['fraction_pruned_were_new'] = pruned_newest_feature_accum / pruned_accum\n",
    "                pruned_newest_feature_accum = 0\n",
    "                pruned_accum = 0\n",
    "            metrics['units_pruned'] = total_pruned\n",
    "            if len(prune_thresholds) > 0:\n",
    "                metrics['prune_threshold'] = np.mean(prune_thresholds)\n",
    "            prune_thresholds.clear()\n",
    "        \n",
    "        if log_utility_stats:\n",
    "            all_utilities = cbp_tracker.get_statistics(prune_layer)['utility']\n",
    "            distractor_mask = distractor_tracker.distractor_mask\n",
    "            real_utilities = all_utilities[~distractor_mask]\n",
    "            distractor_utilities = all_utilities[distractor_mask]\n",
    "            \n",
    "            cumulative_utility = all_utilities.sum().item()\n",
    "            metrics['cumulative_utility'] = cumulative_utility\n",
    "            \n",
    "            if len(real_utilities) > 0:\n",
    "                metrics['real_utility_median'] = real_utilities.median().item()\n",
    "                metrics['real_utility_25th'] = real_utilities.quantile(0.25).item()\n",
    "                metrics['real_utility_75th'] = real_utilities.quantile(0.75).item()\n",
    "            \n",
    "            if len(distractor_utilities) > 0:\n",
    "                metrics['distractor_utility_median'] = distractor_utilities.median().item()\n",
    "                metrics['distractor_utility_25th'] = distractor_utilities.quantile(0.25).item() \n",
    "                metrics['distractor_utility_75th'] = distractor_utilities.quantile(0.75).item()\n",
    "        \n",
    "        if log_optimizer_stats and isinstance(optimizer, IDBD):\n",
    "            states = list(optimizer.state.values())\n",
    "            assert len(states) == 1, \"There should not be more than one optimizer state!\"\n",
    "            state = states[0]\n",
    "            step_sizes = torch.exp(state['beta'])\n",
    "            metrics['mean_step_size'] = step_sizes.mean().item()\n",
    "            metrics['median_step_size'] = step_sizes.median().item()\n",
    "            metrics['effective_lr'] = effective_lr_accum / n_steps_since_log\n",
    "        effective_lr_accum = 0.0\n",
    "\n",
    "        log_metrics(metrics, cfg, step=step)\n",
    "        \n",
    "        print(f'step: {step} | loss: {metrics[\"loss\"]:.4f}')\n",
    "        # pbar.set_postfix(loss=metrics['loss'])\n",
    "        # pbar.update(cfg.train.log_freq)\n",
    "        \n",
    "        # Reset accumulators\n",
    "        loss_accum = 0.0\n",
    "        mean_pred_loss_accum = 0.0\n",
    "        n_steps_since_log = 0\n",
    "        target_buffer = []\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13346ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pre_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x7f44b941b490>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f44b95e1cc0, raw_cell=\"finish_experiment(cfg)\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://wsl%2Bubuntu-24.04/home/ejmejm/local_projects/phd_research/phd/feature_search/notebooks/feature_search_with_replay.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:603\u001b[0m, in \u001b[0;36m_WandbInit._pre_run_cell_hook\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpausing backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:770\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    769\u001b[0m     pause \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mPauseRequest()\n\u001b[0;32m--> 770\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:289\u001b[0m, in \u001b[0;36mInterfaceShared._publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb\u001b[38;5;241m.\u001b[39mPauseRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(pause\u001b[38;5;241m=\u001b[39mpause)\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfinish_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/local_projects/phd_research/phd/research_utils/logging.py:311\u001b[0m, in \u001b[0;36mfinish_experiment\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Finish and clean up the active experiment.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    config: Hydra configuration object specifying logging framework.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mwandb:\n\u001b[0;32m--> 311\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mcomet_ml:\n\u001b[1;32m    313\u001b[0m     experiment \u001b[38;5;241m=\u001b[39m comet_ml\u001b[38;5;241m.\u001b[39mget_global_experiment()\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:4362\u001b[0m, in \u001b[0;36mfinish\u001b[0;34m(exit_code, quiet)\u001b[0m\n\u001b[1;32m   4345\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Finish a run and upload any remaining data.\u001b[39;00m\n\u001b[1;32m   4346\u001b[0m \n\u001b[1;32m   4347\u001b[0m \u001b[38;5;124;03mMarks the completion of a W&B run and ensures all data is synced to the server.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4359\u001b[0m \u001b[38;5;124;03m    quiet: Deprecated. Configure logging verbosity using `wandb.Settings(quiet=...)`.\u001b[39;00m\n\u001b[1;32m   4360\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mrun:\n\u001b[0;32m-> 4362\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexit_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:406\u001b[0m, in \u001b[0;36m_log_to_run.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m     run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_id\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging\u001b[38;5;241m.\u001b[39mlog_to_run(run_id):\n\u001b[0;32m--> 406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:503\u001b[0m, in \u001b[0;36m_noop_if_forked_with_no_service.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m init_pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_pid\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_using_service \u001b[38;5;129;01mor\u001b[39;00m init_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid():\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` ignored (called from pid=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mgetpid()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `init` called from pid=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minit_pid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m See: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_registry\u001b[38;5;241m.\u001b[39murl(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiprocess\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m )\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# This attribute may not exist because it is not included in the run's\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# pickled state.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:451\u001b[0m, in \u001b[0;36m_attach.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    449\u001b[0m         _is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2309\u001b[0m, in \u001b[0;36mRun.finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   2300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m quiet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2301\u001b[0m     deprecate\u001b[38;5;241m.\u001b[39mdeprecate(\n\u001b[1;32m   2302\u001b[0m         field_name\u001b[38;5;241m=\u001b[39mDeprecated\u001b[38;5;241m.\u001b[39mrun__finish_quiet,\n\u001b[1;32m   2303\u001b[0m         warning_message\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2307\u001b[0m         run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2308\u001b[0m     )\n\u001b[0;32m-> 2309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:406\u001b[0m, in \u001b[0;36m_log_to_run.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m     run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_id\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging\u001b[38;5;241m.\u001b[39mlog_to_run(run_id):\n\u001b[0;32m--> 406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2322\u001b[0m, in \u001b[0;36mRun._finish\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl\n\u001b[1;32m   2321\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinishing run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_path()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m telemetry\u001b[38;5;241m.\u001b[39mcontext(run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tel:\n\u001b[1;32m   2323\u001b[0m     tel\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mfinish \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2325\u001b[0m \u001b[38;5;66;03m# Run hooks that need to happen before the last messages to the\u001b[39;00m\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;66;03m# internal service, like Jupyter hooks.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/lib/telemetry.py:42\u001b[0m, in \u001b[0;36m_TelemetryObject.__exit__\u001b[0;34m(self, exctype, excinst, exctb)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_telemetry_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:821\u001b[0m, in \u001b[0;36mRun._telemetry_callback\u001b[0;34m(self, telem_obj)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj\u001b[38;5;241m.\u001b[39mMergeFrom(telem_obj)\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj_dirty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 821\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_telemetry_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:834\u001b[0m, in \u001b[0;36mRun._telemetry_flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m serialized \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj_flushed:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 834\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_telemetry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_telemetry_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj_flushed \u001b[38;5;241m=\u001b[39m serialized\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry_obj_dirty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:60\u001b[0m, in \u001b[0;36mInterfaceShared._publish_telemetry\u001b[0;34m(self, telem)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish_telemetry\u001b[39m(\u001b[38;5;28mself\u001b[39m, telem: tpb\u001b[38;5;241m.\u001b[39mTelemetryRecord) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_record(telemetry\u001b[38;5;241m=\u001b[39mtelem)\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x7f44b941b490>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f44b95e1390, execution_count=9 error_before_exec=None error_in_exec=[Errno 32] Broken pipe info=<ExecutionInfo object at 7f44b95e1cc0, raw_cell=\"finish_experiment(cfg)\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://wsl%2Bubuntu-24.04/home/ejmejm/local_projects/phd_research/phd/feature_search/notebooks/feature_search_with_replay.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:614\u001b[0m, in \u001b[0;36m_WandbInit._post_run_cell_hook\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 614\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:778\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 778\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:293\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "finish_experiment(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedcb75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5bdc62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121ecaee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5479af0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9806c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c60acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
