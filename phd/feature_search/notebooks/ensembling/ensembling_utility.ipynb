{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8d329e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ejmejm/local_projects/phd_research/phd/feature_search/scripts/ensemble_feature_search.py:685: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path='../conf', config_name='full_feature_search')\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "import hydra\n",
    "\n",
    "from phd.feature_search.scripts.ensemble_feature_search import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cda782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hydra.core.global_hydra.GlobalHydra().is_initialized():\n",
    "    hydra.initialize(config_path=\"../../conf\")\n",
    "\n",
    "# Load hydra config\n",
    "cfg = hydra.compose(\n",
    "    config_name = \"comet_sweeps/nonlinear_geoff_ablation_v5/base_config.yaml\",\n",
    "    overrides = [\n",
    "        \"task.n_features=30\",\n",
    "        \"task.n_real_features=30\",\n",
    "        \"task.noise_std=0.0\",\n",
    "        \"train.total_steps=15_000\",\n",
    "        \"seed=20250812\",\n",
    "        \"train.log_freq=500\",\n",
    "        \"comet_ml=false\",\n",
    "        \"wandb=false\",\n",
    "        \"model.hidden_dim=1280\", #1280\",\n",
    "        \"+model.ensemble_feature_selection_method=random\",\n",
    "        \"+model.feature_utility_mode=mean\",\n",
    "        \"+model.ensemble_utility_mode=objective_improvement\",\n",
    "        \"+model.prediction_mode=mean\",\n",
    "        \"+feature_recycling.ensemble_recycle_rate=0.0\",\n",
    "        \"+model.ensemble_dim=1280\",\n",
    "        \"+model.n_ensemble_members=1\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# yaml_cfg = omegaconf.OmegaConf.to_container(cfg, resolve=True)\n",
    "# print(yaml.dump(yaml_cfg, indent=2, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a074e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every time a new feature is introduced, check if it should be kept or not\n",
    "# If it should be kept, track how long for the feature to rise in utility to top 10%\n",
    "# But what is utility in an ensemble? I think for now I can just say it's the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0810dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_best_feature_match_counts(model, task):\n",
    "    \"\"\"Get, for each target feature, how closely the closest learning network hidden unit matches it.\"\"\"\n",
    "    optimal_feature_weights = task.weights[0].T\n",
    "    feature_matches = model.input_layer.weight.unsqueeze(dim=1) == optimal_feature_weights.unsqueeze(dim=0)\n",
    "    positive_feature_match_counts = feature_matches.sum(dim=2)\n",
    "    negative_feature_match_counts = (~feature_matches).sum(dim=2)\n",
    "    feature_match_counts = torch.maximum(positive_feature_match_counts, negative_feature_match_counts)\n",
    "    best_feature_match_counts = feature_match_counts.max(dim=0).values\n",
    "    return best_feature_match_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "207cfb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_experiment(cfg, imitate_no_ensemble=False):\n",
    "    cfg = init_experiment(cfg.project, cfg)\n",
    "\n",
    "    task, task_iterator, model, criterion, optimizer, repr_optimizer, recycler = \\\n",
    "        prepare_ltu_geoff_experiment(cfg)\n",
    "    model.forward = model_distractor_forward_pass.__get__(model)\n",
    "\n",
    "    distractor_tracker = DistractorTracker(\n",
    "        model,\n",
    "        cfg.task.distractor_chance,\n",
    "        tuple(cfg.task.distractor_mean_range),\n",
    "        tuple(cfg.task.distractor_std_range),\n",
    "        seed = seed_from_string(cfg.seed, 'distractor_tracker'),\n",
    "    )\n",
    "\n",
    "    # Gives a 1:1 hidden dim to ensemble input mapping\n",
    "    if imitate_no_ensemble:\n",
    "        model.ensemble_input_ids = torch.arange(\n",
    "                0, model.n_ensemble_members * model.ensemble_dim,\n",
    "                dtype = torch.long,\n",
    "                device = model.ensemble_input_ids.device,\n",
    "            ).reshape(model.n_ensemble_members, model.ensemble_dim)\n",
    "\n",
    "    # Distractor setup\n",
    "    n_hidden_units = model.input_layer.out_features\n",
    "    distractor_tracker.process_new_features(torch.arange(n_hidden_units))\n",
    "    \n",
    "    return task, task_iterator, model, criterion, optimizer, repr_optimizer, recycler, distractor_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5455902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "        cfg: DictConfig,\n",
    "        task: NonlinearGEOFFTask,\n",
    "        task_iterator: Iterator[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        model: EnsembleMLP,\n",
    "        criterion: nn.Module,\n",
    "        optimizer: Optimizer,\n",
    "        repr_optimizer: Optional[Optimizer],\n",
    "        distractor_tracker: DistractorTracker,\n",
    "    ):\n",
    "    # Training loop\n",
    "    step = 0\n",
    "    prev_pruned_idxs = set()\n",
    "    prune_layer = model.activation\n",
    "    optimal_feature_weights = task.weights[0].T\n",
    "    best_feature_match_counts = compute_best_feature_match_counts(model, task)\n",
    "    print(f'Best feature match counts: {best_feature_match_counts.tolist()}')\n",
    "    \n",
    "    # Setup progress bar\n",
    "    for pbar in list(tqdm._instances):\n",
    "        pbar.close()\n",
    "    pbar = tqdm(total=cfg.train.total_steps, desc='Training')\n",
    "\n",
    "    # Flags\n",
    "    log_utility_stats = cfg.train.get('log_utility_stats', False)\n",
    "    log_pruning_stats = cfg.train.get('log_pruning_stats', False)\n",
    "    log_model_stats = cfg.train.get('log_model_stats', False)\n",
    "\n",
    "    # Initialize accumulators\n",
    "    cumulant_stats = StandardizationStats(gamma=0.99)\n",
    "    pruning_state = PruningState()\n",
    "    cumulative_loss = np.float128(0.0)\n",
    "    loss_accum = 0.0\n",
    "    ensemble_loss_accum = 0.0\n",
    "    mean_pred_loss_accum = 0.0\n",
    "    pruned_accum = 0\n",
    "    pruned_newest_feature_accum = 0\n",
    "    n_steps_since_log = 0\n",
    "    total_features_pruned = 0\n",
    "    total_ensembles_pruned = 0\n",
    "    prune_thresholds = []\n",
    "    target_buffer = []\n",
    "\n",
    "    times_to_top_10_percent = [] # Tuples of feature creation step and time to top 10%\n",
    "    suboptimal_prune_hist = [] # One entry per step that gives the number of prunes that step that were suboptimal\n",
    "    tracked_features = {} # Maps feature idx to the step it was introduced\n",
    "    \n",
    "    \n",
    "    while step < cfg.train.total_steps:\n",
    "\n",
    "        ### Data Processing ###\n",
    "\n",
    "        # Generate batch of data\n",
    "        inputs, targets = next(task_iterator)\n",
    "\n",
    "        # Add noise to targets\n",
    "        if cfg.task.noise_std > 0:\n",
    "            targets += torch.randn_like(targets) * cfg.task.noise_std\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            standardized_targets, cumulant_stats = standardize_targets(targets, cumulant_stats)\n",
    "        \n",
    "        if cfg.train.standardize_cumulants:\n",
    "            targets = standardized_targets\n",
    "        target_buffer.extend(targets.view(-1).tolist())\n",
    "        \n",
    "        features, targets = inputs.to(cfg.device), targets.to(cfg.device)\n",
    "\n",
    "\n",
    "        ### Pruning ###\n",
    "\n",
    "        if log_pruning_stats:\n",
    "            pre_prune_feature_utilities = model.feature_utilities.ravel().cpu().clone().numpy()\n",
    "            pre_prune_ensemble_utilities = model.ensemble_utilities.cpu().clone().numpy()\n",
    "        \n",
    "        # TODO: 1. Get pruned feature weights before they are discarded\n",
    "        pre_prune_feature_weights = model.input_layer.weight.clone()\n",
    "        \n",
    "        # Prune the model if necessary\n",
    "        prune_results = prune_model(\n",
    "            model, optimizer, distractor_tracker, pruning_state, cfg)\n",
    "        \n",
    "        # TODO: 2. Check for all features which ones were as good as best feature at the time of pruning\n",
    "        # TODO: 3. Log the step for the number of features which meet the criteria and were pruned\n",
    "        \n",
    "        # Update pruning metrics\n",
    "        total_features_pruned += prune_results['n_features_pruned']\n",
    "        total_ensembles_pruned += prune_results['n_ensembles_pruned']\n",
    "        \n",
    "        if prune_results['n_features_pruned'] > 0:\n",
    "            feature_idxs_pruned = prune_results['feature_idxs_pruned'].tolist()\n",
    "            \n",
    "            # Log pruning statistics\n",
    "            pruned_accum += len(feature_idxs_pruned)\n",
    "            n_new_pruned_features = len(set(feature_idxs_pruned).intersection(prev_pruned_idxs))\n",
    "            pruned_newest_feature_accum += n_new_pruned_features\n",
    "            prev_pruned_idxs = set(feature_idxs_pruned)\n",
    "            \n",
    "            if log_pruning_stats:\n",
    "                prune_thresholds.append(pre_prune_feature_utilities[feature_idxs_pruned].max())\n",
    "                \n",
    "            ## Stop tracking pruned features\n",
    "            \n",
    "            for feature_idx in feature_idxs_pruned:\n",
    "                # If the feature is pruned while still being tracked, then it never made it to top 10%\n",
    "                if feature_idx in tracked_features:\n",
    "                    times_to_top_10_percent.append(\n",
    "                        (tracked_features[feature_idx], -1))\n",
    "                    tracked_features.pop(feature_idx)\n",
    "            \n",
    "            ## Check if any of the pruned feature should not have been pruned\n",
    "            \n",
    "            pruned_feature_weights = pre_prune_feature_weights[feature_idxs_pruned] # (n_features_pruned, input_dim)\n",
    "            feature_matches = pruned_feature_weights.unsqueeze(dim=1) == optimal_feature_weights.unsqueeze(dim=0)\n",
    "            positive_match_counts = feature_matches.sum(dim=2)\n",
    "            negative_match_counts = (~feature_matches).sum(dim=2)\n",
    "            match_counts = torch.maximum(positive_match_counts, negative_match_counts) # (n_features_pruned, n_target_features)\n",
    "            tied_or_better_feature_matrix = match_counts >= best_feature_match_counts.unsqueeze(dim=0)\n",
    "            tied_or_better_feature_arr = tied_or_better_feature_matrix.any(dim=1) # (n_features_pruned,)\n",
    "            n_suboptimal_prunes = tied_or_better_feature_arr.sum().item()\n",
    "            suboptimal_prune_hist.append(n_suboptimal_prunes)\n",
    "            \n",
    "            ## Track features that are the new best features\n",
    "            \n",
    "            new_feature_weights = model.input_layer.weight[feature_idxs_pruned]\n",
    "            feature_matches = new_feature_weights.unsqueeze(dim=1) == optimal_feature_weights.unsqueeze(dim=0)\n",
    "            positive_match_counts = feature_matches.sum(dim=2)\n",
    "            negative_match_counts = (~feature_matches).sum(dim=2)\n",
    "            match_counts = torch.maximum(positive_match_counts, negative_match_counts) # (n_features_pruned, n_target_features)\n",
    "            feature_improvement_matrix = match_counts > best_feature_match_counts.unsqueeze(dim=0)\n",
    "            \n",
    "            # Bool for each new feature that indicates if it is an overall improvement\n",
    "            new_best_feature = feature_improvement_matrix.any(dim=1) # (n_features_pruned,)\n",
    "            \n",
    "            for i in range(len(feature_idxs_pruned)):\n",
    "                feature_idx = feature_idxs_pruned[i]\n",
    "                if new_best_feature[i]:\n",
    "                    tracked_features[feature_idx] = step\n",
    "                    \n",
    "            best_feature_match_counts = compute_best_feature_match_counts(model, task)\n",
    "        \n",
    "        \n",
    "        ### Track time to top 10% ###\n",
    "        \n",
    "        utility_treshold = torch.quantile(model.feature_utilities, 0.9)\n",
    "        feature_utilities = model.feature_utilities.cpu().clone().numpy()\n",
    "        for feature_idx in list(tracked_features.keys()):\n",
    "            if feature_utilities[feature_idx] > utility_treshold:\n",
    "                times_to_top_10_percent.append(\n",
    "                    (tracked_features[feature_idx], step - tracked_features[feature_idx]))\n",
    "                tracked_features.pop(feature_idx)\n",
    "        \n",
    "        \n",
    "        ### Forward Pass ###\n",
    "        \n",
    "        outputs, param_inputs, aux = model(\n",
    "            features, targets, update_state=True,\n",
    "            distractor_callback=distractor_tracker.replace_features,\n",
    "        )\n",
    "        loss = aux['loss']\n",
    "        ensemble_loss_sum = aux['ensemble_losses'].sum()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if cfg.train.standardize_cumulants:\n",
    "                baseline_pred = torch.zeros_like(targets)\n",
    "            else:\n",
    "                baseline_pred = cumulant_stats.running_mean.cpu().view(1, 1)\n",
    "            mean_pred_loss = criterion(baseline_pred, targets)\n",
    "\n",
    "\n",
    "        ### Backward Pass ###\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        if repr_optimizer is not None:\n",
    "            repr_optimizer.zero_grad()\n",
    "        \n",
    "        if isinstance(optimizer, IDBD):\n",
    "            # Mean over batch dimension\n",
    "            param_inputs = {k: v.mean(dim=0) for k, v in param_inputs.items()}\n",
    "            retain_graph = optimizer.version == 'squared_grads'\n",
    "            ensemble_loss_sum.backward(retain_graph=retain_graph)\n",
    "            optimizer.step(aux['ensemble_predictions'], param_inputs)\n",
    "        else:\n",
    "            ensemble_loss_sum.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if repr_optimizer is not None:\n",
    "            repr_optimizer.step()\n",
    "            \n",
    "        \n",
    "        ### Metrics ###\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        loss_accum += loss.item()\n",
    "        ensemble_loss_accum += ensemble_loss_sum.item()\n",
    "        cumulative_loss += loss.item()\n",
    "        mean_pred_loss_accum += mean_pred_loss.item()\n",
    "        n_steps_since_log += 1\n",
    "        \n",
    "        \n",
    "        ### Logging ###\n",
    "        \n",
    "        if step % cfg.train.log_freq == 0:\n",
    "            n_distractors = distractor_tracker.distractor_mask.sum().item()\n",
    "            n_real_features = distractor_tracker.distractor_mask.numel() - n_distractors\n",
    "            metrics = {\n",
    "                'step': step,\n",
    "                'samples': step * cfg.train.batch_size,\n",
    "                'loss': loss_accum / n_steps_since_log,\n",
    "                'avg_ensemble_loss': ensemble_loss_accum / model.n_ensemble_members / n_steps_since_log,\n",
    "                'cumulative_loss': float(cumulative_loss),\n",
    "                'mean_prediction_loss': mean_pred_loss_accum / n_steps_since_log,\n",
    "                'squared_targets': torch.tensor(target_buffer).square().mean().item(),\n",
    "                'n_distractors': n_distractors,\n",
    "                'n_real_features': n_real_features,\n",
    "            }\n",
    "\n",
    "            # Add model statistics separately for real and distractor features\n",
    "            if log_model_stats:\n",
    "                real_feature_masks = [\n",
    "                    torch.ones(model.layers[0].weight.shape[1], dtype=torch.bool, device=model.layers[0].weight.device),\n",
    "                    ~distractor_tracker.distractor_mask,\n",
    "                ]\n",
    "                metrics.update(get_model_statistics(\n",
    "                    model, features, param_inputs, real_feature_masks, metric_prefix='real_'))\n",
    "                \n",
    "                distractor_feature_masks = [\n",
    "                    real_feature_masks[0],\n",
    "                    distractor_tracker.distractor_mask,\n",
    "                ]\n",
    "                metrics.update(get_model_statistics(\n",
    "                    model, features, param_inputs, distractor_feature_masks, metric_prefix='distractor_'))\n",
    "\n",
    "            log_metrics(metrics, cfg, step=step)\n",
    "            \n",
    "            pbar.set_postfix(loss=metrics['loss'])\n",
    "            pbar.update(cfg.train.log_freq)\n",
    "            \n",
    "            # Reset accumulators\n",
    "            loss_accum = 0.0\n",
    "            mean_pred_loss_accum = 0.0\n",
    "            ensemble_loss_accum = 0.0\n",
    "            n_steps_since_log = 0\n",
    "            target_buffer = []\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    pbar.close()\n",
    "    \n",
    "    return {\n",
    "        'times_to_top_10_percent': times_to_top_10_percent,\n",
    "        'suboptimal_prune_hist': suboptimal_prune_hist,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c13db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train.total_steps = 15_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87bbced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 2000/20000 [00:34<05:08, 58.38it/s, loss=0.443] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature match counts: [24, 24, 25, 24, 24, 23, 24, 24, 25, 25, 23, 24, 24, 26, 24, 23, 23, 24, 26, 24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 3500/20000 [00:22<01:57, 140.39it/s, loss=0.426]"
     ]
    }
   ],
   "source": [
    "cfg.task.noise_std = 10.0\n",
    "\n",
    "(task, task_iterator, model, criterion, optimizer,\n",
    "    repr_optimizer, recycler, distractor_tracker) = prepare_experiment(cfg, imitate_no_ensemble=True)\n",
    "results = run_experiment(cfg, task, task_iterator, model, criterion, optimizer, repr_optimizer, distractor_tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debce8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.19918893129770993)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(suboptimal_prune_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25a21a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, -1),\n",
       " (29, -1),\n",
       " (58, -1),\n",
       " (58, -1),\n",
       " (4, -1),\n",
       " (199, -1),\n",
       " (172, -1),\n",
       " (252, -1),\n",
       " (345, -1),\n",
       " (385, -1),\n",
       " (398, -1),\n",
       " (432, -1),\n",
       " (445, -1),\n",
       " (461, -1),\n",
       " (424, -1),\n",
       " (493, -1),\n",
       " (505, -1),\n",
       " (84, -1),\n",
       " (150, -1),\n",
       " (622, -1),\n",
       " (34, 654),\n",
       " (671, -1),\n",
       " (669, -1),\n",
       " (678, -1),\n",
       " (733, -1),\n",
       " (649, -1),\n",
       " (785, -1),\n",
       " (780, -1),\n",
       " (740, -1),\n",
       " (803, -1),\n",
       " (817, -1),\n",
       " (862, -1),\n",
       " (864, -1),\n",
       " (847, -1),\n",
       " (883, -1),\n",
       " (900, -1),\n",
       " (904, -1),\n",
       " (907, -1),\n",
       " (930, -1),\n",
       " (963, -1),\n",
       " (983, -1),\n",
       " (1010, -1),\n",
       " (105, 972),\n",
       " (1029, -1),\n",
       " (1046, -1),\n",
       " (1069, -1),\n",
       " (1070, -1),\n",
       " (1137, -1),\n",
       " (1172, -1),\n",
       " (1209, -1),\n",
       " (1220, -1),\n",
       " (1188, -1),\n",
       " (1270, -1),\n",
       " (1264, -1),\n",
       " (1303, -1),\n",
       " (1316, -1),\n",
       " (1334, -1),\n",
       " (1394, -1),\n",
       " (1392, -1),\n",
       " (1399, -1),\n",
       " (1434, -1),\n",
       " (1449, -1),\n",
       " (1446, -1),\n",
       " (1526, -1),\n",
       " (1552, -1),\n",
       " (1572, -1),\n",
       " (1607, -1),\n",
       " (1585, -1),\n",
       " (1620, -1),\n",
       " (1609, -1),\n",
       " (1655, -1),\n",
       " (1578, -1),\n",
       " (1696, -1),\n",
       " (1677, -1),\n",
       " (1838, -1),\n",
       " (1775, -1),\n",
       " (1888, -1),\n",
       " (1914, -1),\n",
       " (1932, -1),\n",
       " (1950, -1),\n",
       " (1973, -1),\n",
       " (1982, -1),\n",
       " (1991, -1),\n",
       " (2009, -1),\n",
       " (2077, -1),\n",
       " (2084, -1),\n",
       " (2068, -1),\n",
       " (2082, -1),\n",
       " (2132, -1),\n",
       " (2132, -1),\n",
       " (2133, -1),\n",
       " (2116, -1),\n",
       " (2069, -1),\n",
       " (2167, -1),\n",
       " (2187, -1),\n",
       " (2223, -1),\n",
       " (2227, -1),\n",
       " (2253, -1),\n",
       " (1252, -1),\n",
       " (2359, -1),\n",
       " (2363, -1),\n",
       " (2377, -1),\n",
       " (2429, -1),\n",
       " (2433, -1),\n",
       " (2440, -1),\n",
       " (869, -1),\n",
       " (2344, -1),\n",
       " (2477, -1),\n",
       " (2524, -1),\n",
       " (2525, -1),\n",
       " (2543, -1),\n",
       " (2466, -1),\n",
       " (2553, -1),\n",
       " (2580, -1),\n",
       " (2601, -1),\n",
       " (2604, -1),\n",
       " (2615, -1),\n",
       " (2635, -1),\n",
       " (2681, -1),\n",
       " (2682, -1),\n",
       " (2695, -1),\n",
       " (2727, -1),\n",
       " (2731, -1),\n",
       " (2726, -1),\n",
       " (2732, -1),\n",
       " (2737, -1),\n",
       " (2748, -1),\n",
       " (2752, -1),\n",
       " (2773, -1),\n",
       " (2813, -1),\n",
       " (2809, -1),\n",
       " (2861, -1),\n",
       " (2859, -1),\n",
       " (2902, -1),\n",
       " (2874, -1),\n",
       " (2910, -1),\n",
       " (2926, -1),\n",
       " (2964, -1),\n",
       " (2971, -1),\n",
       " (3004, -1),\n",
       " (3079, -1),\n",
       " (3033, -1),\n",
       " (3123, -1),\n",
       " (3126, -1),\n",
       " (3145, -1),\n",
       " (3164, -1),\n",
       " (3225, -1),\n",
       " (3231, -1),\n",
       " (3293, -1),\n",
       " (3193, -1),\n",
       " (3305, -1),\n",
       " (3307, -1),\n",
       " (3308, -1),\n",
       " (3317, -1),\n",
       " (3391, -1),\n",
       " (3407, -1),\n",
       " (3417, -1),\n",
       " (3439, -1),\n",
       " (3434, -1),\n",
       " (3510, -1),\n",
       " (3557, -1),\n",
       " (3524, -1),\n",
       " (3543, -1),\n",
       " (3586, -1),\n",
       " (3587, -1),\n",
       " (3624, -1),\n",
       " (3650, -1),\n",
       " (3709, -1),\n",
       " (3741, -1),\n",
       " (3727, -1),\n",
       " (3745, -1),\n",
       " (3750, -1),\n",
       " (3648, -1),\n",
       " (3760, -1),\n",
       " (3799, -1),\n",
       " (3786, -1),\n",
       " (3823, -1),\n",
       " (3860, -1),\n",
       " (3867, -1),\n",
       " (3739, -1),\n",
       " (3893, -1),\n",
       " (3912, -1),\n",
       " (3956, -1),\n",
       " (121, -1),\n",
       " (3872, -1),\n",
       " (4104, -1),\n",
       " (4110, -1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "times_to_top_10_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "507612df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how many of the good features were discarded\n",
    "# And separately plot the times to top 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e7031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "run_experiment(\n",
    "    cfg, task, task_iterator, model, criterion, optimizer,\n",
    "    repr_optimizer, distractor_tracker,\n",
    ")\n",
    "\n",
    "finish_experiment(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb99b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667dd436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
